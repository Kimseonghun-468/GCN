{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(n_fold,X):\n",
    "    list_train_fold = []\n",
    "    list_val_fold = []\n",
    "    list_train = []\n",
    "    Number = X.shape[0]//n_fold\n",
    "    for i in range(X.shape[0]):\n",
    "        list_train.append(i)\n",
    "    \n",
    "    for i in range(n_fold)[::-1]:\n",
    "        list_val = []\n",
    "        if i==n_fold-1:\n",
    "            for j in range(Number*i,X.shape[0]):\n",
    "                list_val.append(j)\n",
    "                \n",
    "            list_train_fold.append(np.setdiff1d(list_train,list_val))\n",
    "            list_val_fold.append(list_val)\n",
    "            \n",
    "        \n",
    "        if i != n_fold-1: \n",
    "            for j in range(Number*i,Number*(i+1)):\n",
    "                list_val.append(j)\n",
    "            list_train_fold.append(np.setdiff1d(list_train,list_val))\n",
    "            list_val_fold.append(list_val)\n",
    "        \n",
    "    return list_train_fold, list_val_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "from lib import models_update, graph, coarsening, utils\n",
    "# modified\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import random\n",
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_simulation(exp, ppi,p):\n",
    "#     KEGG = 238\n",
    "    KEGG = 2\n",
    "    dataset = {}\n",
    "    all_nodes = exp.shape[1]\n",
    "    dataset['exp'] = []\n",
    "    dataset['L'] = []\n",
    "    dataset['perm'] = []\n",
    "    Pathway_Name = list(p)\n",
    "    \n",
    "        \n",
    "    for i in range(len(Pathway_Name)):\n",
    "        #node_num = random.randint(300, 400)\n",
    "        #sample_nodes = random.sample(range(all_nodes), node_num)\n",
    "        Node_Number = []\n",
    "        Sub_Pathway = p[Pathway_Name[i]]['gene'].values()\n",
    "        Numbering = list(Sub_Pathway)\n",
    "        for a in range(len(Numbering)):\n",
    "            for b in range(exp.shape[1]):\n",
    "                if Numbering[a] == exp.columns[b]:\n",
    "                    Node_Number.append(b)\n",
    "\n",
    "        sample_ppi = ppi.iloc[Node_Number, Node_Number]\n",
    "        sample_ppi = np.array(sample_ppi)\n",
    "        print(\"PPI Shape :\",len(Node_Number), sample_ppi.shape[0])\n",
    "        A = coo_matrix(sample_ppi,dtype=np.float32)\n",
    "        print(\"Number of Edge :\",A.nnz//2, \"//\",i)\n",
    "        \n",
    "        graphs, perm = coarsening.coarsen(A, levels=4, self_connections=False)\n",
    "        print(type(A))\n",
    "        L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "#         print(len(L))\n",
    "#         graph.plot_spectrum(L)\n",
    "        dataset['exp'].append(exp.iloc[:, Node_Number].values)\n",
    "        dataset['L'].append(L)\n",
    "        dataset['perm'].append(perm)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734, 7545)\n",
      "(7545, 7545)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "ppi = pd.read_csv(\"Table/PPI.csv\")\n",
    "#ppi = pd.read_csv(\"Data/L1000/groupPPI.csv\")\n",
    "ppi.drop(['string'], axis='columns', inplace=True)\n",
    "#ppi.drop(['Unnamed: 0'], axis='columns', inplace=True)\n",
    "\n",
    "exp = pd.read_csv(\"Table/EXP.csv\")\n",
    "#exp = pd.read_csv(\"Data/L1000/groupEXP.csv\")\n",
    "exp.drop(['Unnamed: 0'], axis='columns', inplace=True)\n",
    "print(exp.shape)\n",
    "print(ppi.shape)\n",
    "with open('Table/Pathway.pkl','rb')as f:\n",
    "    p = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 35 35\n",
      "Number of Edge : 479 // 0\n",
      "Layer 0: M_0 = |V| = 48 nodes (13 added),|E| = 479 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 148 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 17 17\n",
      "Number of Edge : 129 // 1\n",
      "Layer 0: M_0 = |V| = 32 nodes (15 added),|E| = 129 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (7 added),|E| = 36 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 26 26\n",
      "Number of Edge : 248 // 2\n",
      "Layer 0: M_0 = |V| = 32 nodes (6 added),|E| = 248 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 77 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 21 21\n",
      "Number of Edge : 179 // 3\n",
      "Layer 0: M_0 = |V| = 32 nodes (11 added),|E| = 179 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 55 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 24 24\n",
      "Number of Edge : 187 // 4\n",
      "Layer 0: M_0 = |V| = 32 nodes (8 added),|E| = 187 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 58 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 68 68\n",
      "Number of Edge : 1190 // 5\n",
      "Layer 0: M_0 = |V| = 80 nodes (12 added),|E| = 1190 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (5 added),|E| = 519 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (2 added),|E| = 151 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 30 30\n",
      "Number of Edge : 333 // 6\n",
      "Layer 0: M_0 = |V| = 32 nodes (2 added),|E| = 333 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 102 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 21 21\n",
      "Number of Edge : 138 // 7\n",
      "Layer 0: M_0 = |V| = 32 nodes (11 added),|E| = 138 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 48 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 20 20\n",
      "Number of Edge : 122 // 8\n",
      "Layer 0: M_0 = |V| = 32 nodes (12 added),|E| = 122 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 40 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 30 30\n",
      "Number of Edge : 238 // 9\n",
      "Layer 0: M_0 = |V| = 32 nodes (2 added),|E| = 238 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 89 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 27 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 31 31\n",
      "Number of Edge : 374 // 10\n",
      "Layer 0: M_0 = |V| = 32 nodes (1 added),|E| = 374 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 119 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 31 31\n",
      "Number of Edge : 175 // 11\n",
      "Layer 0: M_0 = |V| = 32 nodes (1 added),|E| = 175 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 59 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 18 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 29 29\n",
      "Number of Edge : 243 // 12\n",
      "Layer 0: M_0 = |V| = 32 nodes (3 added),|E| = 243 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 90 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 24 24\n",
      "Number of Edge : 158 // 13\n",
      "Layer 0: M_0 = |V| = 32 nodes (8 added),|E| = 158 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 57 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 24 24\n",
      "Number of Edge : 183 // 14\n",
      "Layer 0: M_0 = |V| = 32 nodes (8 added),|E| = 183 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 61 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 17 17\n",
      "Number of Edge : 92 // 15\n",
      "Layer 0: M_0 = |V| = 32 nodes (15 added),|E| = 92 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (7 added),|E| = 34 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 34 34\n",
      "Number of Edge : 326 // 16\n",
      "Layer 0: M_0 = |V| = 48 nodes (14 added),|E| = 326 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 121 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 34 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 24 24\n",
      "Number of Edge : 174 // 17\n",
      "Layer 0: M_0 = |V| = 32 nodes (8 added),|E| = 174 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 56 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 17 17\n",
      "Number of Edge : 98 // 18\n",
      "Layer 0: M_0 = |V| = 32 nodes (15 added),|E| = 98 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (7 added),|E| = 35 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 20 20\n",
      "Number of Edge : 100 // 19\n",
      "Layer 0: M_0 = |V| = 32 nodes (12 added),|E| = 100 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 34 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 26 26\n",
      "Number of Edge : 142 // 20\n",
      "Layer 0: M_0 = |V| = 32 nodes (6 added),|E| = 142 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 57 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 31 31\n",
      "Number of Edge : 305 // 21\n",
      "Layer 0: M_0 = |V| = 32 nodes (1 added),|E| = 305 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 118 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 25 25\n",
      "Number of Edge : 160 // 22\n",
      "Layer 0: M_0 = |V| = 32 nodes (7 added),|E| = 160 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 55 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 20 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 37 37\n",
      "Number of Edge : 480 // 23\n",
      "Layer 0: M_0 = |V| = 48 nodes (11 added),|E| = 480 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 166 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 45 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 40 40\n",
      "Number of Edge : 482 // 24\n",
      "Layer 0: M_0 = |V| = 48 nodes (8 added),|E| = 482 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (4 added),|E| = 162 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 44 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 19 19\n",
      "Number of Edge : 114 // 25\n",
      "Layer 0: M_0 = |V| = 32 nodes (13 added),|E| = 114 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 36 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 11 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 24 24\n",
      "Number of Edge : 186 // 26\n",
      "Layer 0: M_0 = |V| = 32 nodes (8 added),|E| = 186 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 61 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 316 // 27\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 316 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (2 added),|E| = 89 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 24 24\n",
      "Number of Edge : 224 // 28\n",
      "Layer 0: M_0 = |V| = 32 nodes (8 added),|E| = 224 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 66 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 19 19\n",
      "Number of Edge : 145 // 29\n",
      "Layer 0: M_0 = |V| = 32 nodes (13 added),|E| = 145 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 45 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 169 // 30\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 169 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 55 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 167 // 31\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 167 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 50 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 17 17\n",
      "Number of Edge : 68 // 32\n",
      "Layer 0: M_0 = |V| = 32 nodes (15 added),|E| = 68 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (7 added),|E| = 26 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 30 30\n",
      "Number of Edge : 334 // 33\n",
      "Layer 0: M_0 = |V| = 32 nodes (2 added),|E| = 334 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 103 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 186 // 34\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 186 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 54 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 34 34\n",
      "Number of Edge : 237 // 35\n",
      "Layer 0: M_0 = |V| = 48 nodes (14 added),|E| = 237 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (7 added),|E| = 92 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 32 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 712 712\n",
      "Number of Edge : 37835 // 36\n",
      "Layer 0: M_0 = |V| = 720 nodes (8 added),|E| = 37835 edges\n",
      "Layer 1: M_1 = |V| = 360 nodes (3 added),|E| = 23263 edges\n",
      "Layer 2: M_2 = |V| = 180 nodes (1 added),|E| = 10528 edges\n",
      "Layer 3: M_3 = |V| = 90 nodes (0 added),|E| = 3635 edges\n",
      "Layer 4: M_4 = |V| = 45 nodes (0 added),|E| = 988 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 61 61\n",
      "Number of Edge : 1361 // 37\n",
      "Layer 0: M_0 = |V| = 64 nodes (3 added),|E| = 1361 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (1 added),|E| = 454 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (0 added),|E| = 120 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 33 33\n",
      "Number of Edge : 434 // 38\n",
      "Layer 0: M_0 = |V| = 48 nodes (15 added),|E| = 434 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (7 added),|E| = 135 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 38 38\n",
      "Number of Edge : 594 // 39\n",
      "Layer 0: M_0 = |V| = 48 nodes (10 added),|E| = 594 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 171 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 45 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 82 82\n",
      "Number of Edge : 989 // 40\n",
      "Layer 0: M_0 = |V| = 96 nodes (14 added),|E| = 989 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (5 added),|E| = 533 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 218 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 53 53\n",
      "Number of Edge : 1123 // 41\n",
      "Layer 0: M_0 = |V| = 64 nodes (11 added),|E| = 1123 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (5 added),|E| = 348 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 55 55\n",
      "Number of Edge : 992 // 42\n",
      "Layer 0: M_0 = |V| = 64 nodes (9 added),|E| = 992 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (4 added),|E| = 339 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 47 47\n",
      "Number of Edge : 512 // 43\n",
      "Layer 0: M_0 = |V| = 48 nodes (1 added),|E| = 512 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (0 added),|E| = 185 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 58 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 23 23\n",
      "Number of Edge : 219 // 44\n",
      "Layer 0: M_0 = |V| = 32 nodes (9 added),|E| = 219 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 66 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 27 27\n",
      "Number of Edge : 227 // 45\n",
      "Layer 0: M_0 = |V| = 32 nodes (5 added),|E| = 227 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (2 added),|E| = 79 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 25 25\n",
      "Number of Edge : 277 // 46\n",
      "Layer 0: M_0 = |V| = 32 nodes (7 added),|E| = 277 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 57 57\n",
      "Number of Edge : 589 // 47\n",
      "Layer 0: M_0 = |V| = 64 nodes (7 added),|E| = 589 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (3 added),|E| = 259 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 96 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 32 32\n",
      "Number of Edge : 226 // 48\n",
      "Layer 0: M_0 = |V| = 32 nodes (0 added),|E| = 226 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 84 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 23 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 31 31\n",
      "Number of Edge : 209 // 49\n",
      "Layer 0: M_0 = |V| = 32 nodes (1 added),|E| = 209 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 81 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 24 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 26 26\n",
      "Number of Edge : 321 // 50\n",
      "Layer 0: M_0 = |V| = 32 nodes (6 added),|E| = 321 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 297 // 51\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 297 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 84 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 24 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 690 // 52\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 690 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 211 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 20 20\n",
      "Number of Edge : 149 // 53\n",
      "Layer 0: M_0 = |V| = 32 nodes (12 added),|E| = 149 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 43 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 23 23\n",
      "Number of Edge : 236 // 54\n",
      "Layer 0: M_0 = |V| = 32 nodes (9 added),|E| = 236 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 64 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 18 18\n",
      "Number of Edge : 144 // 55\n",
      "Layer 0: M_0 = |V| = 32 nodes (14 added),|E| = 144 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (7 added),|E| = 36 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 204 // 56\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 204 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 55 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 154 154\n",
      "Number of Edge : 5148 // 57\n",
      "Layer 0: M_0 = |V| = 160 nodes (6 added),|E| = 5148 edges\n",
      "Layer 1: M_1 = |V| = 80 nodes (3 added),|E| = 2260 edges\n",
      "Layer 2: M_2 = |V| = 40 nodes (1 added),|E| = 708 edges\n",
      "Layer 3: M_3 = |V| = 20 nodes (0 added),|E| = 190 edges\n",
      "Layer 4: M_4 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 49 49\n",
      "Number of Edge : 870 // 58\n",
      "Layer 0: M_0 = |V| = 64 nodes (15 added),|E| = 870 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 292 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 117 117\n",
      "Number of Edge : 3043 // 59\n",
      "Layer 0: M_0 = |V| = 128 nodes (11 added),|E| = 3043 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (5 added),|E| = 1320 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (2 added),|E| = 409 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (1 added),|E| = 104 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 121 121\n",
      "Number of Edge : 3284 // 60\n",
      "Layer 0: M_0 = |V| = 128 nodes (7 added),|E| = 3284 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (3 added),|E| = 1400 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (1 added),|E| = 447 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (0 added),|E| = 120 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 109 109\n",
      "Number of Edge : 2179 // 61\n",
      "Layer 0: M_0 = |V| = 112 nodes (3 added),|E| = 2179 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (1 added),|E| = 986 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (0 added),|E| = 351 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 91 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 66 66\n",
      "Number of Edge : 869 // 62\n",
      "Layer 0: M_0 = |V| = 80 nodes (14 added),|E| = 869 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (6 added),|E| = 389 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 127 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 81 81\n",
      "Number of Edge : 1228 // 63\n",
      "Layer 0: M_0 = |V| = 96 nodes (15 added),|E| = 1228 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (7 added),|E| = 564 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (3 added),|E| = 198 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 123 123\n",
      "Number of Edge : 3773 // 64\n",
      "Layer 0: M_0 = |V| = 128 nodes (5 added),|E| = 3773 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (2 added),|E| = 1505 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (1 added),|E| = 435 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (0 added),|E| = 120 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 685 // 65\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 685 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 215 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 92 92\n",
      "Number of Edge : 2376 // 66\n",
      "Layer 0: M_0 = |V| = 96 nodes (4 added),|E| = 2376 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (2 added),|E| = 902 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (1 added),|E| = 246 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 60 60\n",
      "Number of Edge : 1288 // 67\n",
      "Layer 0: M_0 = |V| = 64 nodes (4 added),|E| = 1288 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (2 added),|E| = 415 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 105 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 54 54\n",
      "Number of Edge : 764 // 68\n",
      "Layer 0: M_0 = |V| = 64 nodes (10 added),|E| = 764 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (4 added),|E| = 326 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 89 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 72 72\n",
      "Number of Edge : 1528 // 69\n",
      "Layer 0: M_0 = |V| = 80 nodes (8 added),|E| = 1528 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (4 added),|E| = 545 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (2 added),|E| = 152 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 46 46\n",
      "Number of Edge : 607 // 70\n",
      "Layer 0: M_0 = |V| = 48 nodes (2 added),|E| = 607 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 223 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 65 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 66 66\n",
      "Number of Edge : 899 // 71\n",
      "Layer 0: M_0 = |V| = 80 nodes (14 added),|E| = 899 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (6 added),|E| = 368 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 119 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 35 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 69 69\n",
      "Number of Edge : 1154 // 72\n",
      "Layer 0: M_0 = |V| = 80 nodes (11 added),|E| = 1154 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (5 added),|E| = 461 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (2 added),|E| = 146 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 101 101\n",
      "Number of Edge : 2310 // 73\n",
      "Layer 0: M_0 = |V| = 112 nodes (11 added),|E| = 2310 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (5 added),|E| = 1009 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (2 added),|E| = 317 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (1 added),|E| = 78 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 78 78\n",
      "Number of Edge : 2383 // 74\n",
      "Layer 0: M_0 = |V| = 80 nodes (2 added),|E| = 2383 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (1 added),|E| = 713 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (0 added),|E| = 190 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 628 // 75\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 628 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 235 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 75 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 55 55\n",
      "Number of Edge : 1000 // 76\n",
      "Layer 0: M_0 = |V| = 64 nodes (9 added),|E| = 1000 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (4 added),|E| = 352 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 54 54\n",
      "Number of Edge : 922 // 77\n",
      "Layer 0: M_0 = |V| = 64 nodes (10 added),|E| = 922 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (5 added),|E| = 329 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 89 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 25 25\n",
      "Number of Edge : 187 // 78\n",
      "Layer 0: M_0 = |V| = 32 nodes (7 added),|E| = 187 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 71 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 59 59\n",
      "Number of Edge : 847 // 79\n",
      "Layer 0: M_0 = |V| = 64 nodes (5 added),|E| = 847 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (2 added),|E| = 356 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 104 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 81 81\n",
      "Number of Edge : 1395 // 80\n",
      "Layer 0: M_0 = |V| = 96 nodes (15 added),|E| = 1395 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (7 added),|E| = 604 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (3 added),|E| = 196 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 86 86\n",
      "Number of Edge : 1368 // 81\n",
      "Layer 0: M_0 = |V| = 96 nodes (10 added),|E| = 1368 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (5 added),|E| = 643 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 214 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 115 115\n",
      "Number of Edge : 2037 // 82\n",
      "Layer 0: M_0 = |V| = 128 nodes (13 added),|E| = 2037 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (5 added),|E| = 1032 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (2 added),|E| = 362 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (1 added),|E| = 103 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 83 83\n",
      "Number of Edge : 1035 // 83\n",
      "Layer 0: M_0 = |V| = 96 nodes (13 added),|E| = 1035 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (6 added),|E| = 478 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 177 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 49 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 51 51\n",
      "Number of Edge : 933 // 84\n",
      "Layer 0: M_0 = |V| = 64 nodes (13 added),|E| = 933 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (6 added),|E| = 306 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 80 80\n",
      "Number of Edge : 1230 // 85\n",
      "Layer 0: M_0 = |V| = 96 nodes (16 added),|E| = 1230 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (7 added),|E| = 497 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (3 added),|E| = 171 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 54 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 180 180\n",
      "Number of Edge : 6933 // 86\n",
      "Layer 0: M_0 = |V| = 192 nodes (12 added),|E| = 6933 edges\n",
      "Layer 1: M_1 = |V| = 96 nodes (6 added),|E| = 2874 edges\n",
      "Layer 2: M_2 = |V| = 48 nodes (3 added),|E| = 864 edges\n",
      "Layer 3: M_3 = |V| = 24 nodes (1 added),|E| = 249 edges\n",
      "Layer 4: M_4 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 574 // 87\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 574 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 221 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 73 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 91 91\n",
      "Number of Edge : 2239 // 88\n",
      "Layer 0: M_0 = |V| = 96 nodes (5 added),|E| = 2239 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (2 added),|E| = 883 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (1 added),|E| = 246 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 43 43\n",
      "Number of Edge : 531 // 89\n",
      "Layer 0: M_0 = |V| = 48 nodes (5 added),|E| = 531 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 202 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 32 32\n",
      "Number of Edge : 326 // 90\n",
      "Layer 0: M_0 = |V| = 32 nodes (0 added),|E| = 326 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 108 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 19 19\n",
      "Number of Edge : 169 // 91\n",
      "Layer 0: M_0 = |V| = 32 nodes (13 added),|E| = 169 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 45 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 23 23\n",
      "Number of Edge : 80 // 92\n",
      "Layer 0: M_0 = |V| = 32 nodes (9 added),|E| = 80 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 33 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 11 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 71 71\n",
      "Number of Edge : 1173 // 93\n",
      "Layer 0: M_0 = |V| = 80 nodes (9 added),|E| = 1173 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (4 added),|E| = 461 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (2 added),|E| = 138 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 89 89\n",
      "Number of Edge : 2125 // 94\n",
      "Layer 0: M_0 = |V| = 96 nodes (7 added),|E| = 2125 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (3 added),|E| = 797 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (1 added),|E| = 236 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 66 // 95\n",
      "Layer 0: M_0 = |V| = 48 nodes (26 added),|E| = 66 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (12 added),|E| = 28 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (5 added),|E| = 13 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (2 added),|E| = 5 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 2 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 59 59\n",
      "Number of Edge : 679 // 96\n",
      "Layer 0: M_0 = |V| = 64 nodes (5 added),|E| = 679 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (2 added),|E| = 287 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 95 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 57 57\n",
      "Number of Edge : 724 // 97\n",
      "Layer 0: M_0 = |V| = 64 nodes (7 added),|E| = 724 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (3 added),|E| = 304 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 101 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 88 88\n",
      "Number of Edge : 2067 // 98\n",
      "Layer 0: M_0 = |V| = 96 nodes (8 added),|E| = 2067 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (4 added),|E| = 823 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 227 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 257 // 99\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 257 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (2 added),|E| = 82 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 21 21\n",
      "Number of Edge : 98 // 100\n",
      "Layer 0: M_0 = |V| = 32 nodes (11 added),|E| = 98 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 40 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 52 52\n",
      "Number of Edge : 970 // 101\n",
      "Layer 0: M_0 = |V| = 64 nodes (12 added),|E| = 970 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (6 added),|E| = 320 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 105 105\n",
      "Number of Edge : 2756 // 102\n",
      "Layer 0: M_0 = |V| = 112 nodes (7 added),|E| = 2756 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (3 added),|E| = 1123 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (1 added),|E| = 338 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 91 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 31 31\n",
      "Number of Edge : 322 // 103\n",
      "Layer 0: M_0 = |V| = 32 nodes (1 added),|E| = 322 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 112 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 66 66\n",
      "Number of Edge : 1109 // 104\n",
      "Layer 0: M_0 = |V| = 80 nodes (14 added),|E| = 1109 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (6 added),|E| = 428 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 128 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 64 64\n",
      "Number of Edge : 1241 // 105\n",
      "Layer 0: M_0 = |V| = 64 nodes (0 added),|E| = 1241 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (0 added),|E| = 453 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (0 added),|E| = 120 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 92 92\n",
      "Number of Edge : 2013 // 106\n",
      "Layer 0: M_0 = |V| = 96 nodes (4 added),|E| = 2013 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (1 added),|E| = 806 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (0 added),|E| = 261 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 20 20\n",
      "Number of Edge : 133 // 107\n",
      "Layer 0: M_0 = |V| = 32 nodes (12 added),|E| = 133 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 42 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 122 122\n",
      "Number of Edge : 4106 // 108\n",
      "Layer 0: M_0 = |V| = 128 nodes (6 added),|E| = 4106 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (3 added),|E| = 1571 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (1 added),|E| = 456 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (0 added),|E| = 120 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 56 56\n",
      "Number of Edge : 1220 // 109\n",
      "Layer 0: M_0 = |V| = 64 nodes (8 added),|E| = 1220 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (4 added),|E| = 369 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 80 80\n",
      "Number of Edge : 1176 // 110\n",
      "Layer 0: M_0 = |V| = 80 nodes (0 added),|E| = 1176 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (0 added),|E| = 501 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (0 added),|E| = 155 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 44 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 38 38\n",
      "Number of Edge : 466 // 111\n",
      "Layer 0: M_0 = |V| = 48 nodes (10 added),|E| = 466 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (4 added),|E| = 173 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 45 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 100 100\n",
      "Number of Edge : 1678 // 112\n",
      "Layer 0: M_0 = |V| = 112 nodes (12 added),|E| = 1678 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (5 added),|E| = 817 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (2 added),|E| = 287 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (1 added),|E| = 78 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 46 46\n",
      "Number of Edge : 599 // 113\n",
      "Layer 0: M_0 = |V| = 48 nodes (2 added),|E| = 599 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 208 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 84 84\n",
      "Number of Edge : 2134 // 114\n",
      "Layer 0: M_0 = |V| = 96 nodes (12 added),|E| = 2134 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (6 added),|E| = 813 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (3 added),|E| = 209 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 742 // 115\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 742 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 227 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 64 64\n",
      "Number of Edge : 1040 // 116\n",
      "Layer 0: M_0 = |V| = 64 nodes (0 added),|E| = 1040 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (0 added),|E| = 409 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (0 added),|E| = 117 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 35 35\n",
      "Number of Edge : 399 // 117\n",
      "Layer 0: M_0 = |V| = 48 nodes (13 added),|E| = 399 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 144 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 542 // 118\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 542 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 216 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 74 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 46 46\n",
      "Number of Edge : 837 // 119\n",
      "Layer 0: M_0 = |V| = 48 nodes (2 added),|E| = 837 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 243 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 80 80\n",
      "Number of Edge : 1545 // 120\n",
      "Layer 0: M_0 = |V| = 80 nodes (0 added),|E| = 1545 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (0 added),|E| = 610 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (0 added),|E| = 181 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 25 25\n",
      "Number of Edge : 208 // 121\n",
      "Layer 0: M_0 = |V| = 32 nodes (7 added),|E| = 208 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 71 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 111 // 122\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 111 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 35 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 12 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 54 54\n",
      "Number of Edge : 855 // 123\n",
      "Layer 0: M_0 = |V| = 64 nodes (10 added),|E| = 855 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (4 added),|E| = 330 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 77 77\n",
      "Number of Edge : 1933 // 124\n",
      "Layer 0: M_0 = |V| = 80 nodes (3 added),|E| = 1933 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (1 added),|E| = 687 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (0 added),|E| = 189 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 54 54\n",
      "Number of Edge : 947 // 125\n",
      "Layer 0: M_0 = |V| = 64 nodes (10 added),|E| = 947 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (5 added),|E| = 335 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 60 60\n",
      "Number of Edge : 1050 // 126\n",
      "Layer 0: M_0 = |V| = 64 nodes (4 added),|E| = 1050 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (2 added),|E| = 395 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 104 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 49 49\n",
      "Number of Edge : 858 // 127\n",
      "Layer 0: M_0 = |V| = 64 nodes (15 added),|E| = 858 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 278 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 54 54\n",
      "Number of Edge : 821 // 128\n",
      "Layer 0: M_0 = |V| = 64 nodes (10 added),|E| = 821 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (5 added),|E| = 301 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 88 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 65 65\n",
      "Number of Edge : 1282 // 129\n",
      "Layer 0: M_0 = |V| = 80 nodes (15 added),|E| = 1282 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (7 added),|E| = 466 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 131 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 48 48\n",
      "Number of Edge : 839 // 130\n",
      "Layer 0: M_0 = |V| = 48 nodes (0 added),|E| = 839 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (0 added),|E| = 269 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 691 // 131\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 691 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 224 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 36 36\n",
      "Number of Edge : 468 // 132\n",
      "Layer 0: M_0 = |V| = 48 nodes (12 added),|E| = 468 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 145 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 52 52\n",
      "Number of Edge : 623 // 133\n",
      "Layer 0: M_0 = |V| = 64 nodes (12 added),|E| = 623 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (5 added),|E| = 255 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 83 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 68 68\n",
      "Number of Edge : 1452 // 134\n",
      "Layer 0: M_0 = |V| = 80 nodes (12 added),|E| = 1452 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (6 added),|E| = 501 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 134 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 62 62\n",
      "Number of Edge : 863 // 135\n",
      "Layer 0: M_0 = |V| = 64 nodes (2 added),|E| = 863 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (0 added),|E| = 371 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (0 added),|E| = 113 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 238 // 136\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 238 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (2 added),|E| = 82 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 38 38\n",
      "Number of Edge : 512 // 137\n",
      "Layer 0: M_0 = |V| = 48 nodes (10 added),|E| = 512 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 158 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 45 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 70 70\n",
      "Number of Edge : 614 // 138\n",
      "Layer 0: M_0 = |V| = 80 nodes (10 added),|E| = 614 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (5 added),|E| = 275 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (2 added),|E| = 106 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 33 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 31 31\n",
      "Number of Edge : 339 // 139\n",
      "Layer 0: M_0 = |V| = 32 nodes (1 added),|E| = 339 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 111 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 36 36\n",
      "Number of Edge : 275 // 140\n",
      "Layer 0: M_0 = |V| = 48 nodes (12 added),|E| = 275 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 110 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 38 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 65 65\n",
      "Number of Edge : 1212 // 141\n",
      "Layer 0: M_0 = |V| = 80 nodes (15 added),|E| = 1212 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (7 added),|E| = 473 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 136 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 41 41\n",
      "Number of Edge : 516 // 142\n",
      "Layer 0: M_0 = |V| = 48 nodes (7 added),|E| = 516 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (3 added),|E| = 179 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 51 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 46 46\n",
      "Number of Edge : 599 // 143\n",
      "Layer 0: M_0 = |V| = 48 nodes (2 added),|E| = 599 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 213 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 65 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 52 52\n",
      "Number of Edge : 900 // 144\n",
      "Layer 0: M_0 = |V| = 64 nodes (12 added),|E| = 900 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (6 added),|E| = 303 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 635 // 145\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 635 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 235 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 76 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 34 34\n",
      "Number of Edge : 281 // 146\n",
      "Layer 0: M_0 = |V| = 48 nodes (14 added),|E| = 281 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 108 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 33 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 61 61\n",
      "Number of Edge : 891 // 147\n",
      "Layer 0: M_0 = |V| = 64 nodes (3 added),|E| = 891 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (1 added),|E| = 352 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (0 added),|E| = 112 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 290 // 148\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 290 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 18 18\n",
      "Number of Edge : 72 // 149\n",
      "Layer 0: M_0 = |V| = 32 nodes (14 added),|E| = 72 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 35 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 17 17\n",
      "Number of Edge : 74 // 150\n",
      "Layer 0: M_0 = |V| = 32 nodes (15 added),|E| = 74 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (7 added),|E| = 31 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 51 51\n",
      "Number of Edge : 695 // 151\n",
      "Layer 0: M_0 = |V| = 64 nodes (13 added),|E| = 695 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (6 added),|E| = 261 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 110 110\n",
      "Number of Edge : 2825 // 152\n",
      "Layer 0: M_0 = |V| = 112 nodes (2 added),|E| = 2825 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (1 added),|E| = 1162 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (0 added),|E| = 365 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 91 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 63 63\n",
      "Number of Edge : 915 // 153\n",
      "Layer 0: M_0 = |V| = 80 nodes (17 added),|E| = 915 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (7 added),|E| = 381 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 128 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 36 36\n",
      "Number of Edge : 287 // 154\n",
      "Layer 0: M_0 = |V| = 48 nodes (12 added),|E| = 287 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 131 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 42 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 48 48\n",
      "Number of Edge : 740 // 155\n",
      "Layer 0: M_0 = |V| = 48 nodes (0 added),|E| = 740 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (0 added),|E| = 251 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 24 24\n",
      "Number of Edge : 134 // 156\n",
      "Layer 0: M_0 = |V| = 32 nodes (8 added),|E| = 134 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 59 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 19 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lib/coarsening.py:152: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  tval = vv[rs+jj] * (1.0/weights[tid] + 1.0/weights[nid])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 43 43\n",
      "Number of Edge : 526 // 157\n",
      "Layer 0: M_0 = |V| = 48 nodes (5 added),|E| = 526 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 193 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 54 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 65 65\n",
      "Number of Edge : 909 // 158\n",
      "Layer 0: M_0 = |V| = 80 nodes (15 added),|E| = 909 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (7 added),|E| = 374 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 122 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 35 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 710 // 159\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 710 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 243 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 73 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 39 39\n",
      "Number of Edge : 538 // 160\n",
      "Layer 0: M_0 = |V| = 48 nodes (9 added),|E| = 538 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (4 added),|E| = 182 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 45 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 36 36\n",
      "Number of Edge : 221 // 161\n",
      "Layer 0: M_0 = |V| = 48 nodes (12 added),|E| = 221 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 95 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 40 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 58 58\n",
      "Number of Edge : 701 // 162\n",
      "Layer 0: M_0 = |V| = 64 nodes (6 added),|E| = 701 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (2 added),|E| = 291 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 90 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 32 32\n",
      "Number of Edge : 266 // 163\n",
      "Layer 0: M_0 = |V| = 32 nodes (0 added),|E| = 266 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 110 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 67 67\n",
      "Number of Edge : 1209 // 164\n",
      "Layer 0: M_0 = |V| = 80 nodes (13 added),|E| = 1209 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (6 added),|E| = 491 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 134 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 46 46\n",
      "Number of Edge : 471 // 165\n",
      "Layer 0: M_0 = |V| = 48 nodes (2 added),|E| = 471 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 178 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 62 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 205 // 166\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 205 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 84 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 35 35\n",
      "Number of Edge : 318 // 167\n",
      "Layer 0: M_0 = |V| = 48 nodes (13 added),|E| = 318 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 124 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 35 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 45 45\n",
      "Number of Edge : 469 // 168\n",
      "Layer 0: M_0 = |V| = 48 nodes (3 added),|E| = 469 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 197 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 64 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 76 76\n",
      "Number of Edge : 1554 // 169\n",
      "Layer 0: M_0 = |V| = 80 nodes (4 added),|E| = 1554 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (2 added),|E| = 575 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (1 added),|E| = 161 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 30 30\n",
      "Number of Edge : 176 // 170\n",
      "Layer 0: M_0 = |V| = 32 nodes (2 added),|E| = 176 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 66 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 26 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 52 52\n",
      "Number of Edge : 601 // 171\n",
      "Layer 0: M_0 = |V| = 64 nodes (12 added),|E| = 601 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (6 added),|E| = 246 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 75 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 32 32\n",
      "Number of Edge : 298 // 172\n",
      "Layer 0: M_0 = |V| = 32 nodes (0 added),|E| = 298 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 104 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 26 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 20 20\n",
      "Number of Edge : 124 // 173\n",
      "Layer 0: M_0 = |V| = 32 nodes (12 added),|E| = 124 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (6 added),|E| = 41 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 55 55\n",
      "Number of Edge : 702 // 174\n",
      "Layer 0: M_0 = |V| = 64 nodes (9 added),|E| = 702 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (4 added),|E| = 283 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 86 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 45 45\n",
      "Number of Edge : 492 // 175\n",
      "Layer 0: M_0 = |V| = 48 nodes (3 added),|E| = 492 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 173 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 56 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 75 75\n",
      "Number of Edge : 1802 // 176\n",
      "Layer 0: M_0 = |V| = 80 nodes (5 added),|E| = 1802 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (2 added),|E| = 626 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (1 added),|E| = 166 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 77 77\n",
      "Number of Edge : 1099 // 177\n",
      "Layer 0: M_0 = |V| = 80 nodes (3 added),|E| = 1099 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (1 added),|E| = 530 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (0 added),|E| = 182 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 62 62\n",
      "Number of Edge : 1128 // 178\n",
      "Layer 0: M_0 = |V| = 64 nodes (2 added),|E| = 1128 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (1 added),|E| = 422 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (0 added),|E| = 120 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 264 // 179\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 264 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 91 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 26 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 18 18\n",
      "Number of Edge : 93 // 180\n",
      "Layer 0: M_0 = |V| = 32 nodes (14 added),|E| = 93 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (7 added),|E| = 32 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (3 added),|E| = 10 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 23 23\n",
      "Number of Edge : 91 // 181\n",
      "Layer 0: M_0 = |V| = 32 nodes (9 added),|E| = 91 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 33 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 36 36\n",
      "Number of Edge : 289 // 182\n",
      "Layer 0: M_0 = |V| = 48 nodes (12 added),|E| = 289 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 99 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 31 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 35 35\n",
      "Number of Edge : 346 // 183\n",
      "Layer 0: M_0 = |V| = 48 nodes (13 added),|E| = 346 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 133 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 41 41\n",
      "Number of Edge : 280 // 184\n",
      "Layer 0: M_0 = |V| = 48 nodes (7 added),|E| = 280 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (3 added),|E| = 125 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 51 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 20 20\n",
      "Number of Edge : 78 // 185\n",
      "Layer 0: M_0 = |V| = 32 nodes (12 added),|E| = 78 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 36 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 51 51\n",
      "Number of Edge : 435 // 186\n",
      "Layer 0: M_0 = |V| = 64 nodes (13 added),|E| = 435 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (5 added),|E| = 135 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 47 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 16 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 33 33\n",
      "Number of Edge : 212 // 187\n",
      "Layer 0: M_0 = |V| = 48 nodes (15 added),|E| = 212 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 93 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 32 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 31 31\n",
      "Number of Edge : 175 // 188\n",
      "Layer 0: M_0 = |V| = 48 nodes (17 added),|E| = 175 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (7 added),|E| = 74 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 27 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 26 26\n",
      "Number of Edge : 223 // 189\n",
      "Layer 0: M_0 = |V| = 32 nodes (6 added),|E| = 223 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 71 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 126 126\n",
      "Number of Edge : 2576 // 190\n",
      "Layer 0: M_0 = |V| = 128 nodes (2 added),|E| = 2576 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (0 added),|E| = 1162 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (0 added),|E| = 395 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (0 added),|E| = 114 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 73 73\n",
      "Number of Edge : 915 // 191\n",
      "Layer 0: M_0 = |V| = 80 nodes (7 added),|E| = 915 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (3 added),|E| = 424 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (1 added),|E| = 151 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 44 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 106 106\n",
      "Number of Edge : 1274 // 192\n",
      "Layer 0: M_0 = |V| = 112 nodes (6 added),|E| = 1274 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (1 added),|E| = 637 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (0 added),|E| = 261 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 90 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 74 74\n",
      "Number of Edge : 620 // 193\n",
      "Layer 0: M_0 = |V| = 80 nodes (6 added),|E| = 620 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (2 added),|E| = 280 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (1 added),|E| = 118 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 39 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 41 41\n",
      "Number of Edge : 258 // 194\n",
      "Layer 0: M_0 = |V| = 48 nodes (7 added),|E| = 258 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 131 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 48 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 71 71\n",
      "Number of Edge : 789 // 195\n",
      "Layer 0: M_0 = |V| = 80 nodes (9 added),|E| = 789 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (4 added),|E| = 339 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (1 added),|E| = 139 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 43 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 167 167\n",
      "Number of Edge : 4035 // 196\n",
      "Layer 0: M_0 = |V| = 176 nodes (9 added),|E| = 4035 edges\n",
      "Layer 1: M_1 = |V| = 88 nodes (4 added),|E| = 1968 edges\n",
      "Layer 2: M_2 = |V| = 44 nodes (2 added),|E| = 687 edges\n",
      "Layer 3: M_3 = |V| = 22 nodes (1 added),|E| = 200 edges\n",
      "Layer 4: M_4 = |V| = 11 nodes (0 added),|E| = 55 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 20 20\n",
      "Number of Edge : 88 // 197\n",
      "Layer 0: M_0 = |V| = 32 nodes (12 added),|E| = 88 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 36 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 28 28\n",
      "Number of Edge : 200 // 198\n",
      "Layer 0: M_0 = |V| = 32 nodes (4 added),|E| = 200 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 83 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 36 36\n",
      "Number of Edge : 393 // 199\n",
      "Layer 0: M_0 = |V| = 48 nodes (12 added),|E| = 393 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 138 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 404 // 200\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 404 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 164 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 51 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 486 // 201\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 486 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 201 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 32 32\n",
      "Number of Edge : 208 // 202\n",
      "Layer 0: M_0 = |V| = 32 nodes (0 added),|E| = 208 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 70 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 25 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 43 43\n",
      "Number of Edge : 368 // 203\n",
      "Layer 0: M_0 = |V| = 48 nodes (5 added),|E| = 368 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 135 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 46 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 14 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 109 109\n",
      "Number of Edge : 2140 // 204\n",
      "Layer 0: M_0 = |V| = 112 nodes (3 added),|E| = 2140 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (0 added),|E| = 931 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (0 added),|E| = 312 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 91 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 113 113\n",
      "Number of Edge : 2396 // 205\n",
      "Layer 0: M_0 = |V| = 128 nodes (15 added),|E| = 2396 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (7 added),|E| = 1092 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (3 added),|E| = 362 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (1 added),|E| = 105 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 131 131\n",
      "Number of Edge : 3071 // 206\n",
      "Layer 0: M_0 = |V| = 144 nodes (13 added),|E| = 3071 edges\n",
      "Layer 1: M_1 = |V| = 72 nodes (6 added),|E| = 1352 edges\n",
      "Layer 2: M_2 = |V| = 36 nodes (3 added),|E| = 457 edges\n",
      "Layer 3: M_3 = |V| = 18 nodes (1 added),|E| = 134 edges\n",
      "Layer 4: M_4 = |V| = 9 nodes (0 added),|E| = 36 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 45 45\n",
      "Number of Edge : 532 // 207\n",
      "Layer 0: M_0 = |V| = 48 nodes (3 added),|E| = 532 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 206 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 65 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 30 30\n",
      "Number of Edge : 268 // 208\n",
      "Layer 0: M_0 = |V| = 32 nodes (2 added),|E| = 268 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 100 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lib/coarsening.py:152: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  tval = vv[rs+jj] * (1.0/weights[tid] + 1.0/weights[nid])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 66 66\n",
      "Number of Edge : 1260 // 209\n",
      "Layer 0: M_0 = |V| = 80 nodes (14 added),|E| = 1260 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (7 added),|E| = 461 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 132 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 45 45\n",
      "Number of Edge : 607 // 210\n",
      "Layer 0: M_0 = |V| = 48 nodes (3 added),|E| = 607 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 228 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 58 58\n",
      "Number of Edge : 921 // 211\n",
      "Layer 0: M_0 = |V| = 64 nodes (6 added),|E| = 921 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (3 added),|E| = 337 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 101 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 99 // 212\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 99 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 50 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 27 27\n",
      "Number of Edge : 263 // 213\n",
      "Layer 0: M_0 = |V| = 32 nodes (5 added),|E| = 263 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (2 added),|E| = 89 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 67 67\n",
      "Number of Edge : 1007 // 214\n",
      "Layer 0: M_0 = |V| = 80 nodes (13 added),|E| = 1007 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (6 added),|E| = 396 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 125 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 68 68\n",
      "Number of Edge : 899 // 215\n",
      "Layer 0: M_0 = |V| = 80 nodes (12 added),|E| = 899 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (5 added),|E| = 382 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (2 added),|E| = 133 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 37 37\n",
      "Number of Edge : 263 // 216\n",
      "Layer 0: M_0 = |V| = 48 nodes (11 added),|E| = 263 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 94 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 33 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 9 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 92 92\n",
      "Number of Edge : 1700 // 217\n",
      "Layer 0: M_0 = |V| = 96 nodes (4 added),|E| = 1700 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (2 added),|E| = 734 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (1 added),|E| = 226 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (0 added),|E| = 64 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 80 80\n",
      "Number of Edge : 1623 // 218\n",
      "Layer 0: M_0 = |V| = 96 nodes (16 added),|E| = 1623 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (7 added),|E| = 621 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (3 added),|E| = 184 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 52 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 83 83\n",
      "Number of Edge : 2273 // 219\n",
      "Layer 0: M_0 = |V| = 96 nodes (13 added),|E| = 2273 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (6 added),|E| = 780 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (3 added),|E| = 208 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 75 75\n",
      "Number of Edge : 1641 // 220\n",
      "Layer 0: M_0 = |V| = 80 nodes (5 added),|E| = 1641 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (2 added),|E| = 615 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (1 added),|E| = 170 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 114 114\n",
      "Number of Edge : 3285 // 221\n",
      "Layer 0: M_0 = |V| = 128 nodes (14 added),|E| = 3285 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (7 added),|E| = 1312 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (3 added),|E| = 389 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (1 added),|E| = 105 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 88 88\n",
      "Number of Edge : 1877 // 222\n",
      "Layer 0: M_0 = |V| = 96 nodes (8 added),|E| = 1877 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (4 added),|E| = 733 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 220 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 191 191\n",
      "Number of Edge : 5665 // 223\n",
      "Layer 0: M_0 = |V| = 208 nodes (17 added),|E| = 5665 edges\n",
      "Layer 1: M_1 = |V| = 104 nodes (7 added),|E| = 2611 edges\n",
      "Layer 2: M_2 = |V| = 52 nodes (3 added),|E| = 935 edges\n",
      "Layer 3: M_3 = |V| = 26 nodes (1 added),|E| = 283 edges\n",
      "Layer 4: M_4 = |V| = 13 nodes (0 added),|E| = 78 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 125 125\n",
      "Number of Edge : 3442 // 224\n",
      "Layer 0: M_0 = |V| = 128 nodes (3 added),|E| = 3442 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (1 added),|E| = 1415 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (0 added),|E| = 454 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (0 added),|E| = 119 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 104 104\n",
      "Number of Edge : 2884 // 225\n",
      "Layer 0: M_0 = |V| = 112 nodes (8 added),|E| = 2884 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (4 added),|E| = 1146 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (2 added),|E| = 311 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (1 added),|E| = 77 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 181 181\n",
      "Number of Edge : 2107 // 226\n",
      "Layer 0: M_0 = |V| = 784 nodes (603 added),|E| = 2107 edges\n",
      "Layer 1: M_1 = |V| = 392 nodes (274 added),|E| = 1087 edges\n",
      "Layer 2: M_2 = |V| = 196 nodes (114 added),|E| = 470 edges\n",
      "Layer 3: M_3 = |V| = 98 nodes (37 added),|E| = 190 edges\n",
      "Layer 4: M_4 = |V| = 49 nodes (0 added),|E| = 88 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 113 113\n",
      "Number of Edge : 3198 // 227\n",
      "Layer 0: M_0 = |V| = 128 nodes (15 added),|E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (7 added),|E| = 1267 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (3 added),|E| = 360 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (1 added),|E| = 105 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 108 108\n",
      "Number of Edge : 2329 // 228\n",
      "Layer 0: M_0 = |V| = 112 nodes (4 added),|E| = 2329 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (1 added),|E| = 982 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (0 added),|E| = 340 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 91 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 85 85\n",
      "Number of Edge : 1690 // 229\n",
      "Layer 0: M_0 = |V| = 96 nodes (11 added),|E| = 1690 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (5 added),|E| = 615 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 192 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 52 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 307 307\n",
      "Number of Edge : 18133 // 230\n",
      "Layer 0: M_0 = |V| = 320 nodes (13 added),|E| = 18133 edges\n",
      "Layer 1: M_1 = |V| = 160 nodes (6 added),|E| = 8442 edges\n",
      "Layer 2: M_2 = |V| = 80 nodes (3 added),|E| = 2663 edges\n",
      "Layer 3: M_3 = |V| = 40 nodes (1 added),|E| = 737 edges\n",
      "Layer 4: M_4 = |V| = 20 nodes (0 added),|E| = 190 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 111 111\n",
      "Number of Edge : 2501 // 231\n",
      "Layer 0: M_0 = |V| = 112 nodes (1 added),|E| = 2501 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (0 added),|E| = 1224 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (0 added),|E| = 368 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 91 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 86 86\n",
      "Number of Edge : 1652 // 232\n",
      "Layer 0: M_0 = |V| = 96 nodes (10 added),|E| = 1652 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (5 added),|E| = 746 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 223 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 27 27\n",
      "Number of Edge : 277 // 233\n",
      "Layer 0: M_0 = |V| = 32 nodes (5 added),|E| = 277 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (2 added),|E| = 90 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 119 119\n",
      "Number of Edge : 4118 // 234\n",
      "Layer 0: M_0 = |V| = 128 nodes (9 added),|E| = 4118 edges\n",
      "Layer 1: M_1 = |V| = 64 nodes (4 added),|E| = 1513 edges\n",
      "Layer 2: M_2 = |V| = 32 nodes (2 added),|E| = 420 edges\n",
      "Layer 3: M_3 = |V| = 16 nodes (1 added),|E| = 105 edges\n",
      "Layer 4: M_4 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 109 109\n",
      "Number of Edge : 3351 // 235\n",
      "Layer 0: M_0 = |V| = 112 nodes (3 added),|E| = 3351 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (1 added),|E| = 1347 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (0 added),|E| = 377 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (0 added),|E| = 91 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 834 // 236\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 834 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (6 added),|E| = 303 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 32 32\n",
      "Number of Edge : 322 // 237\n",
      "Layer 0: M_0 = |V| = 32 nodes (0 added),|E| = 322 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (0 added),|E| = 105 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 27 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 49 49\n",
      "Number of Edge : 822 // 238\n",
      "Layer 0: M_0 = |V| = 64 nodes (15 added),|E| = 822 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 280 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 34 34\n",
      "Number of Edge : 404 // 239\n",
      "Layer 0: M_0 = |V| = 48 nodes (14 added),|E| = 404 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (7 added),|E| = 124 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 758 // 240\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 758 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 278 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 56 56\n",
      "Number of Edge : 1072 // 241\n",
      "Layer 0: M_0 = |V| = 64 nodes (8 added),|E| = 1072 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (4 added),|E| = 351 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (2 added),|E| = 91 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 23 23\n",
      "Number of Edge : 170 // 242\n",
      "Layer 0: M_0 = |V| = 32 nodes (9 added),|E| = 170 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 58 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 33 33\n",
      "Number of Edge : 343 // 243\n",
      "Layer 0: M_0 = |V| = 48 nodes (15 added),|E| = 343 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (7 added),|E| = 103 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 33 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 47 47\n",
      "Number of Edge : 801 // 244\n",
      "Layer 0: M_0 = |V| = 48 nodes (1 added),|E| = 801 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (0 added),|E| = 254 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 30 30\n",
      "Number of Edge : 376 // 245\n",
      "Layer 0: M_0 = |V| = 32 nodes (2 added),|E| = 376 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (1 added),|E| = 105 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (0 added),|E| = 28 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 50 50\n",
      "Number of Edge : 847 // 246\n",
      "Layer 0: M_0 = |V| = 64 nodes (14 added),|E| = 847 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (7 added),|E| = 284 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 35 35\n",
      "Number of Edge : 438 // 247\n",
      "Layer 0: M_0 = |V| = 48 nodes (13 added),|E| = 438 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (6 added),|E| = 149 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (3 added),|E| = 36 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 67 67\n",
      "Number of Edge : 1096 // 248\n",
      "Layer 0: M_0 = |V| = 80 nodes (13 added),|E| = 1096 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (6 added),|E| = 415 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (3 added),|E| = 122 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (1 added),|E| = 36 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 51 51\n",
      "Number of Edge : 852 // 249\n",
      "Layer 0: M_0 = |V| = 64 nodes (13 added),|E| = 852 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (6 added),|E| = 299 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (3 added),|E| = 78 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (1 added),|E| = 21 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 80 80\n",
      "Number of Edge : 1996 // 250\n",
      "Layer 0: M_0 = |V| = 80 nodes (0 added),|E| = 1996 edges\n",
      "Layer 1: M_1 = |V| = 40 nodes (0 added),|E| = 705 edges\n",
      "Layer 2: M_2 = |V| = 20 nodes (0 added),|E| = 189 edges\n",
      "Layer 3: M_3 = |V| = 10 nodes (0 added),|E| = 45 edges\n",
      "Layer 4: M_4 = |V| = 5 nodes (0 added),|E| = 10 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 101 101\n",
      "Number of Edge : 2339 // 251\n",
      "Layer 0: M_0 = |V| = 112 nodes (11 added),|E| = 2339 edges\n",
      "Layer 1: M_1 = |V| = 56 nodes (5 added),|E| = 934 edges\n",
      "Layer 2: M_2 = |V| = 28 nodes (2 added),|E| = 292 edges\n",
      "Layer 3: M_3 = |V| = 14 nodes (1 added),|E| = 77 edges\n",
      "Layer 4: M_4 = |V| = 7 nodes (0 added),|E| = 21 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 87 87\n",
      "Number of Edge : 2230 // 252\n",
      "Layer 0: M_0 = |V| = 96 nodes (9 added),|E| = 2230 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (4 added),|E| = 835 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (2 added),|E| = 230 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 41 41\n",
      "Number of Edge : 555 // 253\n",
      "Layer 0: M_0 = |V| = 48 nodes (7 added),|E| = 555 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (3 added),|E| = 187 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 53 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 404 // 254\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 404 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 165 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (1 added),|E| = 52 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPI Shape : 46 46\n",
      "Number of Edge : 749 // 255\n",
      "Layer 0: M_0 = |V| = 48 nodes (2 added),|E| = 749 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 241 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 66 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 21 21\n",
      "Number of Edge : 184 // 256\n",
      "Layer 0: M_0 = |V| = 32 nodes (11 added),|E| = 184 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 54 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 37 37\n",
      "Number of Edge : 512 // 257\n",
      "Layer 0: M_0 = |V| = 48 nodes (11 added),|E| = 512 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 161 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 44 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 25 25\n",
      "Number of Edge : 162 // 258\n",
      "Layer 0: M_0 = |V| = 32 nodes (7 added),|E| = 162 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (3 added),|E| = 50 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (1 added),|E| = 19 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 63 63\n",
      "Number of Edge : 994 // 259\n",
      "Layer 0: M_0 = |V| = 64 nodes (1 added),|E| = 994 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (0 added),|E| = 321 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (0 added),|E| = 95 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 24 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 207 // 260\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 207 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 55 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 23 23\n",
      "Number of Edge : 236 // 261\n",
      "Layer 0: M_0 = |V| = 32 nodes (9 added),|E| = 236 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (4 added),|E| = 66 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 22 22\n",
      "Number of Edge : 142 // 262\n",
      "Layer 0: M_0 = |V| = 32 nodes (10 added),|E| = 142 edges\n",
      "Layer 1: M_1 = |V| = 16 nodes (5 added),|E| = 46 edges\n",
      "Layer 2: M_2 = |V| = 8 nodes (2 added),|E| = 15 edges\n",
      "Layer 3: M_3 = |V| = 4 nodes (1 added),|E| = 3 edges\n",
      "Layer 4: M_4 = |V| = 2 nodes (0 added),|E| = 1 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 44 44\n",
      "Number of Edge : 363 // 263\n",
      "Layer 0: M_0 = |V| = 48 nodes (4 added),|E| = 363 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (2 added),|E| = 137 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 46 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 14 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 40 40\n",
      "Number of Edge : 407 // 264\n",
      "Layer 0: M_0 = |V| = 48 nodes (8 added),|E| = 407 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (4 added),|E| = 133 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 39 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 10 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 45 45\n",
      "Number of Edge : 386 // 265\n",
      "Layer 0: M_0 = |V| = 48 nodes (3 added),|E| = 386 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (1 added),|E| = 154 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (0 added),|E| = 54 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 59 59\n",
      "Number of Edge : 634 // 266\n",
      "Layer 0: M_0 = |V| = 64 nodes (5 added),|E| = 634 edges\n",
      "Layer 1: M_1 = |V| = 32 nodes (2 added),|E| = 246 edges\n",
      "Layer 2: M_2 = |V| = 16 nodes (1 added),|E| = 81 edges\n",
      "Layer 3: M_3 = |V| = 8 nodes (0 added),|E| = 25 edges\n",
      "Layer 4: M_4 = |V| = 4 nodes (0 added),|E| = 6 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 38 38\n",
      "Number of Edge : 361 // 267\n",
      "Layer 0: M_0 = |V| = 48 nodes (10 added),|E| = 361 edges\n",
      "Layer 1: M_1 = |V| = 24 nodes (5 added),|E| = 119 edges\n",
      "Layer 2: M_2 = |V| = 12 nodes (2 added),|E| = 40 edges\n",
      "Layer 3: M_3 = |V| = 6 nodes (1 added),|E| = 9 edges\n",
      "Layer 4: M_4 = |V| = 3 nodes (0 added),|E| = 3 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "PPI Shape : 80 80\n",
      "Number of Edge : 1545 // 268\n",
      "Layer 0: M_0 = |V| = 96 nodes (16 added),|E| = 1545 edges\n",
      "Layer 1: M_1 = |V| = 48 nodes (7 added),|E| = 647 edges\n",
      "Layer 2: M_2 = |V| = 24 nodes (3 added),|E| = 200 edges\n",
      "Layer 3: M_3 = |V| = 12 nodes (1 added),|E| = 55 edges\n",
      "Layer 4: M_4 = |V| = 6 nodes (0 added),|E| = 15 edges\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    }
   ],
   "source": [
    "dataset = data_simulation(exp, ppi,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734, 201)\n"
     ]
    }
   ],
   "source": [
    "data_IC50 = pd.read_csv(\"Data/Table_S6_GDSC_Drug_response_IC50.csv\")\n",
    "data_IC50.drop(['Unnamed: 0'], axis='columns', inplace=True)\n",
    "print(data_IC50.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp': [array([[1.89289049, 1.67111535, 1.95871214, ..., 2.30112996, 1.99677152,\n",
       "          2.11636866],\n",
       "         [1.61266879, 1.65550187, 1.33247929, ..., 2.13777303, 1.84218341,\n",
       "          2.0458107 ],\n",
       "         [1.50915218, 1.78275987, 1.75343845, ..., 2.13428864, 1.65890547,\n",
       "          2.23108801],\n",
       "         ...,\n",
       "         [1.6762205 , 1.86097231, 1.62675013, ..., 2.31916313, 1.94578817,\n",
       "          2.13534807],\n",
       "         [1.84295475, 1.84448402, 1.66741383, ..., 2.18200086, 2.03992909,\n",
       "          1.90214131],\n",
       "         [2.2233432 , 2.08044262, 1.36647275, ..., 1.68375478, 1.26557088,\n",
       "          1.76192828]]),\n",
       "  array([[1.59184464, 2.26276306, 1.03763924, ..., 1.57154316, 2.28150069,\n",
       "          2.25919521],\n",
       "         [1.29027572, 2.24025676, 1.15131768, ..., 1.407846  , 2.40356286,\n",
       "          2.04253636],\n",
       "         [1.78337258, 2.16315904, 1.00229431, ..., 1.3326172 , 2.13915876,\n",
       "          2.0852583 ],\n",
       "         ...,\n",
       "         [1.19548372, 2.33903717, 1.15100046, ..., 1.50165517, 2.20060132,\n",
       "          2.19901819],\n",
       "         [1.34701788, 2.22529702, 1.15366701, ..., 1.32525836, 1.99341918,\n",
       "          1.94964903],\n",
       "         [1.59062497, 2.2464367 , 1.13775569, ..., 1.30411952, 2.21442307,\n",
       "          1.14623717]]),\n",
       "  array([[1.67111535, 1.42215891, 1.01715023, ..., 1.3759011 , 1.49634981,\n",
       "          1.83325358],\n",
       "         [1.65550187, 1.82557244, 0.99107101, ..., 1.28225274, 1.15081784,\n",
       "          1.89029038],\n",
       "         [1.78275987, 1.59558244, 0.95565293, ..., 1.22122473, 1.12599499,\n",
       "          1.12736784],\n",
       "         ...,\n",
       "         [1.86097231, 1.39429549, 1.05067402, ..., 1.38279889, 1.33319748,\n",
       "          1.10130385],\n",
       "         [1.84448402, 1.34018495, 1.03234483, ..., 1.35487866, 1.29303003,\n",
       "          1.15497121],\n",
       "         [2.08044262, 1.57550191, 1.67732851, ..., 1.09015309, 1.2917917 ,\n",
       "          1.87689646]]),\n",
       "  array([[1.09578765, 1.20386905, 1.77162641, ..., 1.7804918 , 1.1014007 ,\n",
       "          1.20619925],\n",
       "         [1.14992405, 1.17646017, 2.07011289, ..., 1.471917  , 1.07112091,\n",
       "          1.15404668],\n",
       "         [1.18803012, 1.4870857 , 1.99872142, ..., 1.49795082, 1.09762171,\n",
       "          2.38318036],\n",
       "         ...,\n",
       "         [1.13351719, 1.44444238, 1.90250831, ..., 1.44832201, 1.11060258,\n",
       "          1.79422663],\n",
       "         [1.10469642, 1.6654677 , 1.87023215, ..., 1.49891989, 1.87569476,\n",
       "          1.24178352],\n",
       "         [1.16921996, 1.38636338, 2.06987971, ..., 1.39577422, 1.0767419 ,\n",
       "          1.08808791]]),\n",
       "  array([[1.3681394 , 1.82035548, 2.11477873, ..., 1.08403577, 2.00739809,\n",
       "          1.38212077],\n",
       "         [1.75875647, 1.99570181, 2.08005224, ..., 1.12232168, 2.01773587,\n",
       "          1.60008364],\n",
       "         [1.38834456, 1.62510335, 1.96791832, ..., 1.17062326, 1.90533954,\n",
       "          1.76791006],\n",
       "         ...,\n",
       "         [1.1392131 , 1.80182572, 1.97726677, ..., 1.35224051, 2.20607203,\n",
       "          1.55127084],\n",
       "         [1.23662433, 1.76148764, 1.95278046, ..., 1.77043307, 1.81117512,\n",
       "          1.5577182 ],\n",
       "         [1.61694958, 1.36287659, 1.99424621, ..., 1.14693586, 2.07971162,\n",
       "          1.60967029]]),\n",
       "  array([[1.79982741, 1.40902395, 2.19251145, ..., 1.43952253, 1.19459356,\n",
       "          1.75980937],\n",
       "         [2.00033172, 1.55286994, 2.03961731, ..., 1.69539872, 1.3450805 ,\n",
       "          1.741701  ],\n",
       "         [1.99605933, 1.61710779, 2.07190205, ..., 1.42965907, 1.45791797,\n",
       "          1.73027318],\n",
       "         ...,\n",
       "         [1.94811687, 1.58026862, 1.80074882, ..., 1.53499661, 1.51860873,\n",
       "          1.94382593],\n",
       "         [1.83075702, 1.5307183 , 2.0467531 , ..., 1.33643527, 1.49380526,\n",
       "          1.53959426],\n",
       "         [2.15711477, 1.63306335, 1.92107373, ..., 1.31848865, 1.27581726,\n",
       "          1.810881  ]]),\n",
       "  array([[1.81905812, 1.83319025, 2.27560278, ..., 1.17904334, 1.98367686,\n",
       "          1.98584433],\n",
       "         [1.38647009, 1.17112931, 2.25172794, ..., 1.19021759, 1.05256709,\n",
       "          1.99235007],\n",
       "         [1.87336638, 1.77366865, 2.2667073 , ..., 1.14020038, 1.08260579,\n",
       "          1.95740767],\n",
       "         ...,\n",
       "         [1.8998494 , 1.50350552, 2.36079992, ..., 1.15199232, 1.25560524,\n",
       "          2.04587164],\n",
       "         [1.89470689, 1.4076138 , 2.19315246, ..., 1.1444431 , 1.88358717,\n",
       "          1.78446699],\n",
       "         [1.55454266, 1.14913704, 2.24697859, ..., 1.13272649, 1.11473355,\n",
       "          2.07592294]]),\n",
       "  array([[1.68974475, 2.04744918, 2.32907749, ..., 1.35766602, 1.80708656,\n",
       "          2.33880658],\n",
       "         [1.75974474, 1.92257954, 2.25345916, ..., 1.63553576, 1.80721934,\n",
       "          2.33161362],\n",
       "         [1.32240641, 2.04111663, 2.32475552, ..., 1.16800568, 1.92872459,\n",
       "          2.45781234],\n",
       "         ...,\n",
       "         [1.4670868 , 2.00830477, 2.22895635, ..., 1.62506928, 1.89733417,\n",
       "          2.2223773 ],\n",
       "         [1.90465447, 2.0274686 , 2.12745303, ..., 1.46600966, 2.02490502,\n",
       "          2.32631759],\n",
       "         [1.27428454, 1.87619057, 2.11876276, ..., 2.11654979, 1.36922698,\n",
       "          2.1273351 ]]),\n",
       "  array([[1.715264  , 1.83822738, 1.79056449, ..., 1.88655309, 1.20124722,\n",
       "          2.23763492],\n",
       "         [1.60593216, 2.04489004, 1.76563299, ..., 2.00987698, 1.3664026 ,\n",
       "          2.21578902],\n",
       "         [1.70318369, 1.99615941, 2.19492537, ..., 2.10444056, 1.14519895,\n",
       "          2.28579834],\n",
       "         ...,\n",
       "         [1.85798552, 1.65241578, 1.51849268, ..., 1.83859579, 1.3468245 ,\n",
       "          2.28572462],\n",
       "         [1.5981097 , 1.52523177, 1.35568755, ..., 2.06178528, 1.17161513,\n",
       "          2.20080782],\n",
       "         [1.23010505, 1.99919707, 0.85413044, ..., 1.84013981, 2.17392286,\n",
       "          2.15687618]]),\n",
       "  array([[1.50948065, 1.27937372, 2.19549769, ..., 1.6553893 , 2.10234339,\n",
       "          2.25072994],\n",
       "         [1.61836629, 1.55447018, 2.19945997, ..., 1.75945383, 1.91737498,\n",
       "          2.1065531 ],\n",
       "         [1.55918883, 1.32003   , 2.23414812, ..., 1.69135719, 2.15182321,\n",
       "          2.26110807],\n",
       "         ...,\n",
       "         [1.97520767, 1.45707088, 2.34995028, ..., 1.54433414, 1.96331123,\n",
       "          2.22131037],\n",
       "         [1.50703607, 1.6628802 , 2.17525584, ..., 1.73055963, 2.1527445 ,\n",
       "          2.07732268],\n",
       "         [1.50756762, 1.44752241, 2.2095785 , ..., 1.76374676, 1.1979248 ,\n",
       "          2.06547419]]),\n",
       "  array([[1.67111535, 2.12043736, 1.42215891, ..., 2.30723998, 1.99512367,\n",
       "          2.18973667],\n",
       "         [1.65550187, 2.17565734, 1.82557244, ..., 2.18258909, 2.01869089,\n",
       "          2.09525407],\n",
       "         [1.78275987, 2.04225176, 1.59558244, ..., 2.14002713, 1.89941008,\n",
       "          1.89567207],\n",
       "         ...,\n",
       "         [1.86097231, 2.05468329, 1.39429549, ..., 1.78509653, 2.07229956,\n",
       "          1.95528127],\n",
       "         [1.84448402, 2.03603957, 1.34018495, ..., 2.10968503, 1.96497735,\n",
       "          2.16077417],\n",
       "         [2.08044262, 1.90222534, 1.57550191, ..., 2.00439238, 1.91482585,\n",
       "          1.98111975]]),\n",
       "  array([[1.63604807, 1.67111535, 1.66940522, ..., 1.26168778, 0.95786176,\n",
       "          1.00169179],\n",
       "         [1.84305139, 1.65550187, 1.45708812, ..., 1.4068948 , 0.97748509,\n",
       "          1.08230368],\n",
       "         [1.62999656, 1.78275987, 1.66353832, ..., 1.51896819, 1.05810184,\n",
       "          1.10860664],\n",
       "         ...,\n",
       "         [1.63347941, 1.86097231, 1.36814778, ..., 1.0630906 , 1.00798507,\n",
       "          1.03806297],\n",
       "         [1.63903231, 1.84448402, 1.68265447, ..., 1.50078734, 1.00411979,\n",
       "          1.07391641],\n",
       "         [1.69243512, 2.08044262, 1.35581525, ..., 1.37584768, 1.14013782,\n",
       "          0.98166187]]),\n",
       "  array([[1.67111535, 1.20910889, 1.81982876, ..., 1.24398006, 1.25417101,\n",
       "          2.07738337],\n",
       "         [1.65550187, 1.16628648, 1.51379325, ..., 1.15918408, 1.23234599,\n",
       "          2.1439026 ],\n",
       "         [1.78275987, 1.16805334, 1.89832369, ..., 1.37835102, 1.46182138,\n",
       "          2.11566591],\n",
       "         ...,\n",
       "         [1.86097231, 1.19860673, 1.4585675 , ..., 1.19503304, 1.28112893,\n",
       "          1.39479837],\n",
       "         [1.84448402, 1.23443609, 1.83093439, ..., 1.38522748, 1.19646085,\n",
       "          1.80968818],\n",
       "         [2.08044262, 1.94411625, 2.02180156, ..., 1.05274677, 1.11654081,\n",
       "          2.3122355 ]]),\n",
       "  array([[1.17783431, 1.95871214, 1.10959647, ..., 1.9974625 , 1.12526225,\n",
       "          2.01806269],\n",
       "         [1.28991984, 1.33247929, 1.02972892, ..., 1.78761909, 1.11808438,\n",
       "          2.13130816],\n",
       "         [1.18315206, 1.75343845, 1.08372887, ..., 2.04265192, 1.46013017,\n",
       "          2.13197018],\n",
       "         ...,\n",
       "         [1.10423895, 1.62675013, 1.17458366, ..., 1.96761995, 1.08328009,\n",
       "          2.10239564],\n",
       "         [1.15875132, 1.66741383, 1.05443027, ..., 2.19033154, 1.11578666,\n",
       "          2.1754751 ],\n",
       "         [1.69190964, 1.36647275, 1.18874778, ..., 2.013651  , 1.26741483,\n",
       "          2.17414689]]),\n",
       "  array([[1.67111535, 1.20910889, 1.05893073, ..., 1.3759011 , 1.11762943,\n",
       "          1.67216422],\n",
       "         [1.65550187, 1.16628648, 1.23918172, ..., 1.28225274, 1.11493824,\n",
       "          1.67773699],\n",
       "         [1.78275987, 1.16805334, 1.07274482, ..., 1.22122473, 1.31083748,\n",
       "          1.57914345],\n",
       "         ...,\n",
       "         [1.86097231, 1.19860673, 0.99382343, ..., 1.38279889, 1.08652954,\n",
       "          1.66814678],\n",
       "         [1.84448402, 1.23443609, 1.25842798, ..., 1.35487866, 1.33837173,\n",
       "          1.65235896],\n",
       "         [2.08044262, 1.94411625, 1.7356933 , ..., 1.09015309, 1.10160846,\n",
       "          1.31614575]]),\n",
       "  array([[1.60479822, 1.67111535, 1.74360854, ..., 1.90997113, 1.26631341,\n",
       "          1.47025722],\n",
       "         [1.90626029, 1.65550187, 1.84578763, ..., 1.76154194, 1.74972941,\n",
       "          1.11014648],\n",
       "         [1.69573721, 1.78275987, 1.87922625, ..., 1.94047753, 2.11764197,\n",
       "          1.09886392],\n",
       "         ...,\n",
       "         [1.66579807, 1.86097231, 1.67248397, ..., 1.61371055, 1.61249252,\n",
       "          1.57352979],\n",
       "         [1.56932078, 1.84448402, 1.86894707, ..., 1.73496644, 2.0313382 ,\n",
       "          1.39777617],\n",
       "         [1.78853934, 2.08044262, 1.65487909, ..., 1.44799387, 2.00917663,\n",
       "          1.44477957]]),\n",
       "  array([[1.10938801, 1.50948065, 1.86711279, ..., 1.29096037, 2.25824844,\n",
       "          2.45441246],\n",
       "         [1.17109744, 1.61836629, 1.37003818, ..., 1.6224672 , 2.17397342,\n",
       "          2.41942682],\n",
       "         [1.23674017, 1.55918883, 2.30940618, ..., 1.98783395, 2.24295486,\n",
       "          2.22201138],\n",
       "         ...,\n",
       "         [1.11741033, 1.97520767, 2.13690018, ..., 1.14265046, 2.26118539,\n",
       "          2.39038711],\n",
       "         [1.33189579, 1.50703607, 2.27277597, ..., 1.18489264, 2.16463703,\n",
       "          2.22908602],\n",
       "         [2.2472187 , 1.50756762, 2.39663628, ..., 1.41418453, 2.23699328,\n",
       "          2.23055028]]),\n",
       "  array([[1.77162641, 2.0933625 , 1.50377841, ..., 1.00472966, 1.05839497,\n",
       "          1.00298922],\n",
       "         [2.07011289, 1.94517023, 1.560034  , ..., 1.02907662, 1.21103105,\n",
       "          1.08169835],\n",
       "         [1.99872142, 2.08762592, 1.41320811, ..., 0.98899226, 1.28345737,\n",
       "          1.03011871],\n",
       "         ...,\n",
       "         [1.90250831, 2.06964242, 1.60456691, ..., 1.21477218, 1.51052395,\n",
       "          1.27314305],\n",
       "         [1.87023215, 2.12676511, 1.84231403, ..., 1.02121523, 1.40123425,\n",
       "          1.38529857],\n",
       "         [2.06987971, 1.75981112, 1.43670746, ..., 1.77595338, 1.37687134,\n",
       "          1.29703725]]),\n",
       "  array([[1.18903658, 1.66947029, 0.97131973, ..., 1.21536192, 1.27916783,\n",
       "          1.50055324],\n",
       "         [1.73417222, 1.81521995, 0.97816748, ..., 1.08182768, 1.16257288,\n",
       "          1.54060715],\n",
       "         [1.29788673, 1.68043783, 1.8089034 , ..., 1.42524514, 1.04346653,\n",
       "          1.59063613],\n",
       "         ...,\n",
       "         [1.58565709, 1.70417353, 1.04353463, ..., 1.45848937, 1.08270409,\n",
       "          1.46143179],\n",
       "         [1.96819404, 1.66205007, 2.02812033, ..., 1.36533476, 1.56787541,\n",
       "          1.61435312],\n",
       "         [1.54346977, 1.73312905, 0.95755779, ..., 1.50959258, 1.13630472,\n",
       "          1.72921292]]),\n",
       "  array([[2.0933625 , 1.085535  , 2.3226232 , ..., 1.05839497, 1.04574811,\n",
       "          1.00298922],\n",
       "         [1.94517023, 1.42319402, 1.7993397 , ..., 1.21103105, 0.99993834,\n",
       "          1.08169835],\n",
       "         [2.08762592, 1.14302279, 2.04026847, ..., 1.28345737, 1.82042502,\n",
       "          1.03011871],\n",
       "         ...,\n",
       "         [2.06964242, 1.10780045, 2.19336299, ..., 1.51052395, 1.04089283,\n",
       "          1.27314305],\n",
       "         [2.12676511, 1.08081453, 2.25276889, ..., 1.40123425, 1.1255898 ,\n",
       "          1.38529857],\n",
       "         [1.75981112, 1.80138882, 1.86336171, ..., 1.37687134, 1.12970192,\n",
       "          1.29703725]]),\n",
       "  array([[1.8929414 , 1.734724  , 2.14818888, ..., 1.00472966, 2.08209871,\n",
       "          1.27916783],\n",
       "         [1.72737714, 1.36205192, 1.7468021 , ..., 1.02907662, 2.18280988,\n",
       "          1.16257288],\n",
       "         [1.76427723, 1.56955049, 1.89229073, ..., 0.98899226, 2.18561923,\n",
       "          1.04346653],\n",
       "         ...,\n",
       "         [1.76396567, 1.90158642, 1.78207452, ..., 1.21477218, 2.15025765,\n",
       "          1.08270409],\n",
       "         [2.05450061, 1.60701496, 1.86708001, ..., 1.02121523, 2.10530384,\n",
       "          1.56787541],\n",
       "         [1.71743179, 2.21074222, 2.14530491, ..., 1.77595338, 2.24374133,\n",
       "          1.13630472]]),\n",
       "  array([[1.95122161, 1.81869585, 1.77250374, ..., 1.38920619, 1.99677152,\n",
       "          2.11636866],\n",
       "         [1.59247363, 1.82425528, 1.81920957, ..., 1.54966017, 1.84218341,\n",
       "          2.0458107 ],\n",
       "         [1.81089456, 1.89180582, 1.88916777, ..., 1.55055102, 1.65890547,\n",
       "          2.23108801],\n",
       "         ...,\n",
       "         [1.80390114, 1.71881445, 2.08492681, ..., 1.19587819, 1.94578817,\n",
       "          2.13534807],\n",
       "         [1.80253932, 1.60108964, 1.95525656, ..., 1.29947617, 2.03992909,\n",
       "          1.90214131],\n",
       "         [1.89638147, 1.35654497, 2.01020708, ..., 1.47690725, 1.26557088,\n",
       "          1.76192828]]),\n",
       "  array([[1.67111535, 1.6352402 , 1.34578834, ..., 1.92130046, 1.60968916,\n",
       "          1.92783192],\n",
       "         [1.65550187, 1.99507157, 1.49622128, ..., 1.1691627 , 1.90632689,\n",
       "          1.96017309],\n",
       "         [1.78275987, 1.84752806, 1.17971966, ..., 1.75335675, 1.63270662,\n",
       "          1.88987477],\n",
       "         ...,\n",
       "         [1.86097231, 1.89706264, 1.27969963, ..., 2.03451502, 1.52076386,\n",
       "          1.93812421],\n",
       "         [1.84448402, 1.97510978, 1.55719286, ..., 1.96402303, 2.24548499,\n",
       "          1.87854535],\n",
       "         [2.08044262, 1.95233611, 1.89129414, ..., 1.50830269, 1.32339245,\n",
       "          1.7846225 ]]),\n",
       "  array([[1.91767623, 1.50115227, 1.44645512, ..., 1.90997113, 1.27299853,\n",
       "          1.78901639],\n",
       "         [2.04331151, 1.85401787, 1.79473981, ..., 1.76154194, 1.61536378,\n",
       "          1.55205892],\n",
       "         [1.92128971, 1.70684758, 2.08775608, ..., 1.94047753, 1.38237962,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [2.03920919, 1.73517632, 1.21739284, ..., 1.61371055, 1.78323908,\n",
       "          1.71962776],\n",
       "         [1.8834902 , 1.85811364, 1.25509756, ..., 1.73496644, 1.52637877,\n",
       "          1.54759422],\n",
       "         [1.66953805, 1.88056571, 1.68658346, ..., 1.44799387, 1.60477037,\n",
       "          1.48490784]]),\n",
       "  array([[1.62458207, 1.59089921, 0.9794307 , ..., 1.63966398, 1.78325111,\n",
       "          1.92783192],\n",
       "         [1.63481574, 1.54335575, 1.08110873, ..., 1.13014756, 1.86546913,\n",
       "          1.96017309],\n",
       "         [1.42707913, 1.56509672, 1.06248278, ..., 1.47575757, 1.7311304 ,\n",
       "          1.88987477],\n",
       "         ...,\n",
       "         [1.74019614, 1.63235997, 1.23793578, ..., 1.67544105, 1.53448234,\n",
       "          1.93812421],\n",
       "         [1.94387395, 1.69145745, 1.3463227 , ..., 1.64787521, 1.75460723,\n",
       "          1.87854535],\n",
       "         [1.63487119, 1.52388941, 1.02969744, ..., 1.292799  , 1.87194034,\n",
       "          1.7846225 ]]),\n",
       "  array([[1.04647961, 1.62458207, 0.9794307 , ..., 2.07099654, 1.54848148,\n",
       "          2.1372528 ],\n",
       "         [1.05783806, 1.63481574, 1.08110873, ..., 2.0480489 , 1.21978405,\n",
       "          2.334724  ],\n",
       "         [1.07408024, 1.42707913, 1.06248278, ..., 2.03723641, 1.21651673,\n",
       "          2.18115322],\n",
       "         ...,\n",
       "         [1.08433523, 1.74019614, 1.23793578, ..., 1.84977055, 1.6546485 ,\n",
       "          2.07376214],\n",
       "         [1.08115946, 1.94387395, 1.3463227 , ..., 2.05565587, 1.85607503,\n",
       "          2.11076239],\n",
       "         [1.21377801, 1.63487119, 1.02969744, ..., 1.75282767, 1.23596267,\n",
       "          2.29477683]]),\n",
       "  array([[1.10938801, 0.9794307 , 1.99986835, ..., 1.28779286, 1.03959776,\n",
       "          2.45441246],\n",
       "         [1.17109744, 1.08110873, 1.99418962, ..., 1.30848485, 1.08212856,\n",
       "          2.41942682],\n",
       "         [1.23674017, 1.06248278, 1.80880641, ..., 1.38660518, 0.9640454 ,\n",
       "          2.22201138],\n",
       "         ...,\n",
       "         [1.11741033, 1.23793578, 1.66164122, ..., 1.35737924, 1.74570499,\n",
       "          2.39038711],\n",
       "         [1.33189579, 1.3463227 , 2.034761  , ..., 1.52844881, 1.10080245,\n",
       "          2.22908602],\n",
       "         [2.2472187 , 1.02969744, 2.07146593, ..., 1.35795805, 1.02440553,\n",
       "          2.23055028]]),\n",
       "  array([[1.74401142, 1.91388802, 1.74887243, ..., 1.77070807, 1.54848148,\n",
       "          2.07641438],\n",
       "         [1.79794308, 1.795934  , 1.6023614 , ..., 1.91394002, 1.21978405,\n",
       "          2.21178879],\n",
       "         [1.84908457, 2.10703917, 1.60531622, ..., 1.61263429, 1.21651673,\n",
       "          2.20844802],\n",
       "         ...,\n",
       "         [1.67572787, 1.98229035, 1.39498739, ..., 1.56594296, 1.6546485 ,\n",
       "          1.80088515],\n",
       "         [1.67984827, 2.17441873, 1.64129655, ..., 1.6362167 , 1.85607503,\n",
       "          1.31721898],\n",
       "         [1.76674445, 2.07289567, 1.96131039, ..., 1.90919938, 1.23596267,\n",
       "          2.15823676]]),\n",
       "  array([[1.67111535, 1.72936543, 1.08959013, ..., 1.90070265, 2.10448178,\n",
       "          2.23763492],\n",
       "         [1.65550187, 1.48181959, 1.05650574, ..., 1.79762103, 2.22491141,\n",
       "          2.21578902],\n",
       "         [1.78275987, 2.00790036, 1.09209224, ..., 1.85004737, 2.13831139,\n",
       "          2.28579834],\n",
       "         ...,\n",
       "         [1.86097231, 1.6323121 , 1.11544843, ..., 1.93996391, 2.08897333,\n",
       "          2.28572462],\n",
       "         [1.84448402, 1.91610957, 1.19337565, ..., 1.78750028, 1.90178724,\n",
       "          2.20080782],\n",
       "         [2.08044262, 1.73266341, 1.06415731, ..., 1.81542628, 2.1484155 ,\n",
       "          2.15687618]]),\n",
       "  array([[2.12043736, 1.92638496, 1.28803816, ..., 1.3759011 , 1.90997113,\n",
       "          1.93451553],\n",
       "         [2.17565734, 2.06391505, 1.60077621, ..., 1.28225274, 1.76154194,\n",
       "          1.94261839],\n",
       "         [2.04225176, 1.84888727, 1.60682611, ..., 1.22122473, 1.94047753,\n",
       "          2.17048052],\n",
       "         ...,\n",
       "         [2.05468329, 1.78607617, 1.57414831, ..., 1.38279889, 1.61371055,\n",
       "          1.89411891],\n",
       "         [2.03603957, 2.03055034, 1.73278728, ..., 1.35487866, 1.73496644,\n",
       "          2.05680943],\n",
       "         [1.90222534, 1.85921463, 1.58052528, ..., 1.09015309, 1.44799387,\n",
       "          2.09375273]]),\n",
       "  array([[1.80660781, 1.61517414, 1.33551578, ..., 2.23786438, 1.7287405 ,\n",
       "          1.28040709],\n",
       "         [1.70621391, 1.49495394, 1.73585154, ..., 2.10723163, 2.01288967,\n",
       "          1.13825117],\n",
       "         [1.73981171, 1.68830583, 1.32833996, ..., 1.5959664 , 2.07281901,\n",
       "          1.66744807],\n",
       "         ...,\n",
       "         [1.55087294, 1.66572979, 1.26136617, ..., 2.17636746, 1.82598653,\n",
       "          1.72142454],\n",
       "         [1.70186823, 1.79850741, 1.14847399, ..., 1.68530199, 1.6294921 ,\n",
       "          2.20781341],\n",
       "         [1.20740824, 1.46395202, 1.25130262, ..., 2.14480547, 2.18904325,\n",
       "          1.39883226]]),\n",
       "  array([[1.6352402 , 1.55834506, 1.76832382, ..., 1.00781899, 1.20619925,\n",
       "          1.16454514],\n",
       "         [1.99507157, 1.49794651, 1.66334324, ..., 1.08078814, 1.15404668,\n",
       "          1.22060785],\n",
       "         [1.84752806, 1.76213073, 1.70211502, ..., 0.99158803, 2.38318036,\n",
       "          2.31271835],\n",
       "         ...,\n",
       "         [1.89706264, 1.4288128 , 1.67301722, ..., 1.03565247, 1.79422663,\n",
       "          1.23583685],\n",
       "         [1.97510978, 1.47245438, 1.73203182, ..., 2.29618151, 1.24178352,\n",
       "          1.28264433],\n",
       "         [1.95233611, 1.667898  , 1.55079029, ..., 1.05305787, 1.08808791,\n",
       "          1.24816197]]),\n",
       "  array([[1.54160008, 1.06503339, 2.13228528, ..., 2.04441395, 1.71297242,\n",
       "          0.94131541],\n",
       "         [1.71282864, 1.01435491, 2.14985923, ..., 2.13838311, 1.74893531,\n",
       "          0.94077084],\n",
       "         [1.68130984, 1.41564965, 1.99028797, ..., 2.04857574, 1.75548524,\n",
       "          0.93994064],\n",
       "         ...,\n",
       "         [1.68052115, 1.10324354, 2.08376795, ..., 1.94213451, 1.77664094,\n",
       "          0.97489619],\n",
       "         [1.32940088, 1.31990618, 1.82640337, ..., 2.14820024, 1.80791671,\n",
       "          0.95871699],\n",
       "         [1.62729098, 1.04269297, 1.83703356, ..., 1.96532918, 1.89698667,\n",
       "          0.91917828]]),\n",
       "  array([[1.86711279, 1.95871214, 1.99986835, ..., 2.43758714, 1.29096037,\n",
       "          2.25824844],\n",
       "         [1.37003818, 1.33247929, 1.99418962, ..., 2.54976349, 1.6224672 ,\n",
       "          2.17397342],\n",
       "         [2.30940618, 1.75343845, 1.80880641, ..., 2.58208098, 1.98783395,\n",
       "          2.24295486],\n",
       "         ...,\n",
       "         [2.13690018, 1.62675013, 1.66164122, ..., 2.51090187, 1.14265046,\n",
       "          2.26118539],\n",
       "         [2.27277597, 1.66741383, 2.034761  , ..., 2.46364351, 1.18489264,\n",
       "          2.16463703],\n",
       "         [2.39663628, 1.36647275, 2.07146593, ..., 2.10598371, 1.41418453,\n",
       "          2.23699328]]),\n",
       "  array([[1.86711279, 1.95871214, 1.08959013, ..., 2.43758714, 1.29096037,\n",
       "          2.25824844],\n",
       "         [1.37003818, 1.33247929, 1.05650574, ..., 2.54976349, 1.6224672 ,\n",
       "          2.17397342],\n",
       "         [2.30940618, 1.75343845, 1.09209224, ..., 2.58208098, 1.98783395,\n",
       "          2.24295486],\n",
       "         ...,\n",
       "         [2.13690018, 1.62675013, 1.11544843, ..., 2.51090187, 1.14265046,\n",
       "          2.26118539],\n",
       "         [2.27277597, 1.66741383, 1.19337565, ..., 2.46364351, 1.18489264,\n",
       "          2.16463703],\n",
       "         [2.39663628, 1.36647275, 1.06415731, ..., 2.10598371, 1.41418453,\n",
       "          2.23699328]]),\n",
       "  array([[1.86711279, 1.81905812, 1.83319025, ..., 1.98367686, 2.25824844,\n",
       "          1.75980937],\n",
       "         [1.37003818, 1.38647009, 1.17112931, ..., 1.05256709, 2.17397342,\n",
       "          1.741701  ],\n",
       "         [2.30940618, 1.87336638, 1.77366865, ..., 1.08260579, 2.24295486,\n",
       "          1.73027318],\n",
       "         ...,\n",
       "         [2.13690018, 1.8998494 , 1.50350552, ..., 1.25560524, 2.26118539,\n",
       "          1.94382593],\n",
       "         [2.27277597, 1.89470689, 1.4076138 , ..., 1.88358717, 2.16463703,\n",
       "          1.53959426],\n",
       "         [2.39663628, 1.55454266, 1.14913704, ..., 1.11473355, 2.23699328,\n",
       "          1.810881  ]]),\n",
       "  array([[1.80660781, 1.27647212, 1.77833617, ..., 1.05839497, 1.00473118,\n",
       "          2.45441246],\n",
       "         [1.70621391, 1.38226277, 1.84706241, ..., 1.21103105, 1.00266546,\n",
       "          2.41942682],\n",
       "         [1.73981171, 1.95263667, 1.93889573, ..., 1.28345737, 1.29598722,\n",
       "          2.22201138],\n",
       "         ...,\n",
       "         [1.55087294, 1.41892283, 1.79949192, ..., 1.51052395, 1.01753415,\n",
       "          2.39038711],\n",
       "         [1.70186823, 1.30103524, 1.76906383, ..., 1.40123425, 0.99087291,\n",
       "          2.22908602],\n",
       "         [1.20740824, 1.47251986, 1.74812003, ..., 1.37687134, 1.05023936,\n",
       "          2.23055028]]),\n",
       "  array([[1.89289049, 2.12043736, 1.93614144, ..., 1.93451553, 1.80708656,\n",
       "          1.48909008],\n",
       "         [1.61266879, 2.17565734, 1.96590575, ..., 1.94261839, 1.80721934,\n",
       "          1.95755929],\n",
       "         [1.50915218, 2.04225176, 2.03962583, ..., 2.17048052, 1.92872459,\n",
       "          1.67137697],\n",
       "         ...,\n",
       "         [1.6762205 , 2.05468329, 1.91360358, ..., 1.89411891, 1.89733417,\n",
       "          1.52560323],\n",
       "         [1.84295475, 2.03603957, 1.68166436, ..., 2.05680943, 2.02490502,\n",
       "          1.67471315],\n",
       "         [2.2233432 , 1.90222534, 2.11484431, ..., 2.09375273, 1.36922698,\n",
       "          1.6320936 ]]),\n",
       "  array([[2.22119891, 1.01715023, 1.42215891, ..., 2.12996876, 1.49634981,\n",
       "          1.83325358],\n",
       "         [2.22566287, 0.99107101, 1.82557244, ..., 2.06599485, 1.15081784,\n",
       "          1.89029038],\n",
       "         [2.06615733, 0.95565293, 1.59558244, ..., 2.07174771, 1.12599499,\n",
       "          1.12736784],\n",
       "         ...,\n",
       "         [2.09762562, 1.05067402, 1.39429549, ..., 2.16813835, 1.33319748,\n",
       "          1.10130385],\n",
       "         [2.16653504, 1.03234483, 1.34018495, ..., 2.00983539, 1.29303003,\n",
       "          1.15497121],\n",
       "         [2.15797888, 1.67732851, 1.57550191, ..., 1.86577307, 1.2917917 ,\n",
       "          1.87689646]]),\n",
       "  array([[1.89289049, 1.93614144, 1.81982876, ..., 1.8952085 , 1.80708656,\n",
       "          2.33880658],\n",
       "         [1.61266879, 1.96590575, 1.51379325, ..., 1.91074837, 1.80721934,\n",
       "          2.33161362],\n",
       "         [1.50915218, 2.03962583, 1.89832369, ..., 1.85634917, 1.92872459,\n",
       "          2.45781234],\n",
       "         ...,\n",
       "         [1.6762205 , 1.91360358, 1.4585675 , ..., 2.11233917, 1.89733417,\n",
       "          2.2223773 ],\n",
       "         [1.84295475, 1.68166436, 1.83093439, ..., 1.88770447, 2.02490502,\n",
       "          2.32631759],\n",
       "         [2.2233432 , 2.11484431, 2.02180156, ..., 1.82360336, 1.36922698,\n",
       "          2.1273351 ]]),\n",
       "  array([[1.80660781, 1.91333826, 1.27647212, ..., 1.7287405 , 1.11762943,\n",
       "          1.8952085 ],\n",
       "         [1.70621391, 1.92266703, 1.38226277, ..., 2.01288967, 1.11493824,\n",
       "          1.91074837],\n",
       "         [1.73981171, 2.14363424, 1.95263667, ..., 2.07281901, 1.31083748,\n",
       "          1.85634917],\n",
       "         ...,\n",
       "         [1.55087294, 2.19413513, 1.41892283, ..., 1.82598653, 1.08652954,\n",
       "          2.11233917],\n",
       "         [1.70186823, 1.91901482, 1.30103524, ..., 1.6294921 , 1.33837173,\n",
       "          1.88770447],\n",
       "         [1.20740824, 2.10016534, 1.47251986, ..., 2.18904325, 1.10160846,\n",
       "          1.82360336]]),\n",
       "  array([[1.75229496, 1.24687   , 1.08195741, ..., 1.5741009 , 1.82557653,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.11213476, 1.06325793, ..., 1.15108094, 1.42277476,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.29013839, 1.81169324, ..., 1.25651027, 1.70676048,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.14842401, 1.43157496, ..., 1.4854386 , 1.54225427,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.18381704, 1.18184826, ..., 1.69679409, 1.80142249,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.50308824, 1.05159775, ..., 1.18713617, 1.51188483,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.51062881, 1.24687   , ..., 1.5741009 , 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.57174699, 1.11213476, ..., 1.15108094, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.65440876, 1.29013839, ..., 1.25651027, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.15214069, 1.14842401, ..., 1.4854386 , 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54446902, 1.18381704, ..., 1.69679409, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.70415854, 1.50308824, ..., 1.18713617, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.10333655, 1.86711279, ..., 2.25824844, 1.5237267 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.21611905, 1.37003818, ..., 2.17397342, 1.14702919,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.1789313 , 2.30940618, ..., 2.24295486, 1.5049025 ,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.18590107, 2.13690018, ..., 2.26118539, 1.37195886,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.00183703, 2.27277597, ..., 2.16463703, 1.615447  ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.13187821, 2.39663628, ..., 2.23699328, 1.2821139 ,\n",
       "          1.48490784]]),\n",
       "  array([[1.86513411, 1.20309782, 1.0645105 , ..., 1.61804934, 1.88276957,\n",
       "          1.40611403],\n",
       "         [1.84990216, 1.01035976, 1.15660826, ..., 1.83506989, 2.09304525,\n",
       "          1.60303555],\n",
       "         [1.9822202 , 1.00392658, 1.05886395, ..., 1.60706473, 1.84526448,\n",
       "          1.74527114],\n",
       "         ...,\n",
       "         [1.8850391 , 1.0721154 , 1.15426189, ..., 1.77763499, 1.74480533,\n",
       "          1.24443411],\n",
       "         [1.77485161, 1.03150448, 1.92758515, ..., 1.70615553, 2.12826469,\n",
       "          1.60773321],\n",
       "         [1.76942897, 0.98043212, 1.78166263, ..., 1.8708759 , 1.73804677,\n",
       "          1.13614796]]),\n",
       "  array([[1.90053686, 1.79207016, 2.18469965, ..., 2.24238528, 2.03381827,\n",
       "          2.2410517 ],\n",
       "         [2.06303588, 1.90450416, 2.1972118 , ..., 2.26539435, 2.04004973,\n",
       "          2.25940005],\n",
       "         [1.87885601, 1.71392041, 2.12732238, ..., 2.28391197, 2.04044105,\n",
       "          2.13358804],\n",
       "         ...,\n",
       "         [2.14739896, 1.83217927, 2.19260245, ..., 2.19201515, 2.04191018,\n",
       "          2.22072501],\n",
       "         [1.99329116, 1.73907503, 2.17347385, ..., 2.20755838, 2.01972133,\n",
       "          2.04511999],\n",
       "         [1.8630176 , 1.50651741, 1.97183716, ..., 2.16757421, 1.99043499,\n",
       "          2.10745009]]),\n",
       "  array([[2.23801341, 1.54663043, 1.65450098, ..., 2.22512116, 2.03777724,\n",
       "          1.57078364],\n",
       "         [2.15840333, 1.54635192, 1.71589102, ..., 2.02707026, 1.84842247,\n",
       "          1.71805626],\n",
       "         [2.16376856, 1.46324523, 1.58055466, ..., 2.0545292 , 1.79985321,\n",
       "          1.50878047],\n",
       "         ...,\n",
       "         [2.27669739, 1.31042352, 1.84149535, ..., 1.887567  , 1.84843606,\n",
       "          1.44326934],\n",
       "         [2.1840684 , 1.44489179, 1.48361379, ..., 2.12069244, 1.73911557,\n",
       "          1.58590261],\n",
       "         [2.21541077, 1.36847313, 1.55387525, ..., 2.0162038 , 1.86974754,\n",
       "          1.4091467 ]]),\n",
       "  array([[2.18001341, 1.89304616, 2.04002578, ..., 2.3415631 , 2.12022134,\n",
       "          2.21833692],\n",
       "         [2.33326014, 1.89716973, 1.99950298, ..., 2.29842493, 2.07898913,\n",
       "          2.23967245],\n",
       "         [2.10829681, 1.95238235, 1.98515934, ..., 2.24531045, 1.85963803,\n",
       "          2.2341939 ],\n",
       "         ...,\n",
       "         [2.13953767, 2.02553544, 1.99910875, ..., 2.33695535, 1.90725036,\n",
       "          2.26337341],\n",
       "         [2.03659731, 1.99091417, 1.85663003, ..., 2.26674206, 1.93851873,\n",
       "          2.38833659],\n",
       "         [2.23707513, 2.1318118 , 2.07970204, ..., 2.29204755, 1.66473633,\n",
       "          2.19668447]]),\n",
       "  array([[1.25307349, 1.42242015, 1.79019164, ..., 1.32641115, 2.3415631 ,\n",
       "          2.21833692],\n",
       "         [1.80478644, 1.80430466, 2.05119314, ..., 1.12950246, 2.29842493,\n",
       "          2.23967245],\n",
       "         [1.34397588, 1.6593813 , 1.8604156 , ..., 1.06045605, 2.24531045,\n",
       "          2.2341939 ],\n",
       "         ...,\n",
       "         [2.16205518, 1.74035051, 1.75571263, ..., 1.239142  , 2.33695535,\n",
       "          2.26337341],\n",
       "         [1.51954481, 1.67889334, 1.83242667, ..., 1.11277916, 2.26674206,\n",
       "          2.38833659],\n",
       "         [2.00835726, 1.55327097, 1.99620358, ..., 1.4850472 , 2.29204755,\n",
       "          2.19668447]]),\n",
       "  array([[1.89289049, 2.11569104, 1.89782695, ..., 1.57050859, 2.30112996,\n",
       "          2.23141267],\n",
       "         [1.61266879, 2.12980302, 2.03098094, ..., 1.67907135, 2.13777303,\n",
       "          2.07966372],\n",
       "         [1.50915218, 2.04624679, 1.9836047 , ..., 1.70580728, 2.13428864,\n",
       "          2.14715361],\n",
       "         ...,\n",
       "         [1.6762205 , 2.0343909 , 1.89335849, ..., 1.85476641, 2.31916313,\n",
       "          2.12593535],\n",
       "         [1.84295475, 2.0330797 , 1.79177244, ..., 1.65467982, 2.18200086,\n",
       "          1.90527037],\n",
       "         [2.2233432 , 1.94536146, 2.02354836, ..., 1.83994741, 1.68375478,\n",
       "          2.0332904 ]]),\n",
       "  array([[2.21013643, 1.6906174 , 1.74925316, ..., 1.95148796, 2.27117254,\n",
       "          2.22003912],\n",
       "         [2.20387251, 1.64542301, 1.8041966 , ..., 2.05209567, 2.35708658,\n",
       "          2.23926527],\n",
       "         [2.10057243, 1.70259844, 1.82383451, ..., 2.06651294, 2.3625285 ,\n",
       "          2.15975582],\n",
       "         ...,\n",
       "         [2.08053121, 1.88481908, 1.81508798, ..., 2.03931791, 2.42417066,\n",
       "          2.24459862],\n",
       "         [2.01565593, 1.65804243, 1.73030652, ..., 1.79161293, 2.33968531,\n",
       "          1.94075695],\n",
       "         [2.11053312, 1.52219155, 1.88203187, ..., 1.89600702, 2.35671368,\n",
       "          2.14498908]]),\n",
       "  array([[1.67277506, 2.22721045, 1.89782695, ..., 2.04506594, 2.3415631 ,\n",
       "          1.87122417],\n",
       "         [1.89405081, 2.24013176, 2.03098094, ..., 2.15214472, 2.29842493,\n",
       "          1.81820386],\n",
       "         [1.72323476, 2.12176072, 1.9836047 , ..., 2.12464675, 2.24531045,\n",
       "          1.7315033 ],\n",
       "         ...,\n",
       "         [1.84331238, 2.20241253, 1.89335849, ..., 1.83564163, 2.33695535,\n",
       "          1.9255818 ],\n",
       "         [1.7899426 , 2.05912731, 1.79177244, ..., 2.07500914, 2.26674206,\n",
       "          1.8854606 ],\n",
       "         [1.67574201, 2.21092616, 2.02354836, ..., 2.06968903, 2.29204755,\n",
       "          1.88178335]]),\n",
       "  array([[2.35484337, 1.1097168 , 1.01715023, ..., 1.49634981, 1.83325358,\n",
       "          2.18973667],\n",
       "         [2.20498862, 1.2694202 , 0.99107101, ..., 1.15081784, 1.89029038,\n",
       "          2.09525407],\n",
       "         [2.30529609, 1.53331514, 0.95565293, ..., 1.12599499, 1.12736784,\n",
       "          1.89567207],\n",
       "         ...,\n",
       "         [2.34021934, 1.25026306, 1.05067402, ..., 1.33319748, 1.10130385,\n",
       "          1.95528127],\n",
       "         [2.27176719, 1.59784379, 1.03234483, ..., 1.29303003, 1.15497121,\n",
       "          2.16077417],\n",
       "         [2.27486964, 2.16821002, 1.67732851, ..., 1.2917917 , 1.87689646,\n",
       "          1.98111975]]),\n",
       "  array([[1.32670141, 1.43567047, 1.6906174 , ..., 1.96224268, 2.27117254,\n",
       "          1.71087526],\n",
       "         [1.54566324, 1.66370701, 1.64542301, ..., 2.06138133, 2.35708658,\n",
       "          1.56821642],\n",
       "         [1.58296244, 1.73690226, 1.70259844, ..., 1.94773376, 2.3625285 ,\n",
       "          1.5963024 ],\n",
       "         ...,\n",
       "         [1.4083743 , 1.55377049, 1.88481908, ..., 1.77411744, 2.42417066,\n",
       "          1.4948428 ],\n",
       "         [1.80189036, 1.53637342, 1.65804243, ..., 1.73460085, 2.33968531,\n",
       "          1.60590269],\n",
       "         [1.79399037, 1.61009976, 1.52219155, ..., 2.05799662, 2.35671368,\n",
       "          1.57802555]]),\n",
       "  array([[2.21013643, 1.6906174 , 2.21120318, ..., 2.23894553, 1.95148796,\n",
       "          2.27117254],\n",
       "         [2.20387251, 1.64542301, 1.86797138, ..., 2.14892124, 2.05209567,\n",
       "          2.35708658],\n",
       "         [2.10057243, 1.70259844, 1.81474935, ..., 2.18591359, 2.06651294,\n",
       "          2.3625285 ],\n",
       "         ...,\n",
       "         [2.08053121, 1.88481908, 1.96849602, ..., 2.08677559, 2.03931791,\n",
       "          2.42417066],\n",
       "         [2.01565593, 1.65804243, 1.82281061, ..., 2.08441629, 1.79161293,\n",
       "          2.33968531],\n",
       "         [2.11053312, 1.52219155, 1.79460691, ..., 2.08001775, 1.89600702,\n",
       "          2.35671368]]),\n",
       "  array([[2.01339139, 1.75512579, 2.21013643, ..., 1.88896936, 2.18559912,\n",
       "          1.80290841],\n",
       "         [2.03625623, 1.92363285, 2.20387251, ..., 1.92288265, 2.15369169,\n",
       "          1.67584968],\n",
       "         [2.2280348 , 1.79760961, 2.10057243, ..., 1.85501759, 2.34101046,\n",
       "          1.78427959],\n",
       "         ...,\n",
       "         [2.07083089, 1.86437332, 2.08053121, ..., 1.97579868, 2.44254437,\n",
       "          2.00034996],\n",
       "         [1.70976507, 1.68731299, 2.01565593, ..., 1.90686808, 2.16362648,\n",
       "          1.81315809],\n",
       "         [1.99875977, 1.67442339, 2.11053312, ..., 1.85032553, 2.20991366,\n",
       "          1.82160214]]),\n",
       "  array([[2.21013643, 1.43813072, 1.98052375, ..., 1.91120496, 2.23894553,\n",
       "          2.31588812],\n",
       "         [2.20387251, 1.30945917, 2.16891875, ..., 1.75437401, 2.14892124,\n",
       "          2.33970723],\n",
       "         [2.10057243, 1.80261338, 2.04259932, ..., 1.7653034 , 2.18591359,\n",
       "          2.21033788],\n",
       "         ...,\n",
       "         [2.08053121, 1.46388648, 2.10936553, ..., 1.95088665, 2.08677559,\n",
       "          2.30121362],\n",
       "         [2.01565593, 1.67962659, 2.0290778 , ..., 1.89705198, 2.08441629,\n",
       "          2.25114824],\n",
       "         [2.11053312, 1.45940559, 1.95685874, ..., 1.8293194 , 2.08001775,\n",
       "          2.37909564]]),\n",
       "  array([[1.93786522, 2.07023958, 1.08195741, ..., 1.23061716, 1.5741009 ,\n",
       "          2.04385511],\n",
       "         [1.87043739, 2.07630483, 1.06325793, ..., 1.18392996, 1.15108094,\n",
       "          1.74018722],\n",
       "         [1.87518156, 2.1176614 , 1.81169324, ..., 1.55320612, 1.25651027,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [2.07327999, 1.81160523, 1.43157496, ..., 1.6300149 , 1.4854386 ,\n",
       "          1.8359737 ],\n",
       "         [1.7650156 , 1.54072683, 1.18184826, ..., 2.0998435 , 1.69679409,\n",
       "          1.9253121 ],\n",
       "         [1.79206338, 1.69227599, 1.05159775, ..., 1.34326663, 1.18713617,\n",
       "          1.31214762]]),\n",
       "  array([[1.75229496, 2.06072626, 2.15522925, ..., 2.04790568, 1.5741009 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.86050822, 2.17515621, ..., 1.38759913, 1.15108094,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.83855123, 2.08048709, ..., 1.78390168, 1.25651027,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.88597731, 2.21205177, ..., 1.84280718, 1.4854386 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.09805395, 2.04663546, ..., 1.52390521, 1.69679409,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.24973533, 2.07725923, ..., 1.42122684, 1.18713617,\n",
       "          1.48490784]]),\n",
       "  array([[2.07023958, 1.80841525, 1.08195741, ..., 1.16948981, 1.96432369,\n",
       "          1.5741009 ],\n",
       "         [2.07630483, 1.93518253, 1.06325793, ..., 1.18556595, 1.04625723,\n",
       "          1.15108094],\n",
       "         [2.1176614 , 1.784662  , 1.81169324, ..., 1.26988052, 1.05699407,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.81160523, 1.65355129, 1.43157496, ..., 1.21930252, 1.24578735,\n",
       "          1.4854386 ],\n",
       "         [1.54072683, 1.94294503, 1.18184826, ..., 1.29064115, 1.07372318,\n",
       "          1.69679409],\n",
       "         [1.69227599, 1.92727057, 1.05159775, ..., 2.08865288, 1.10060965,\n",
       "          1.18713617]]),\n",
       "  array([[2.07023958, 1.08195741, 1.1628622 , ..., 1.96432369, 1.98387903,\n",
       "          1.5741009 ],\n",
       "         [2.07630483, 1.06325793, 1.10182692, ..., 1.04625723, 1.96185081,\n",
       "          1.15108094],\n",
       "         [2.1176614 , 1.81169324, 1.19032066, ..., 1.05699407, 2.04250882,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.81160523, 1.43157496, 1.22200866, ..., 1.24578735, 2.19016088,\n",
       "          1.4854386 ],\n",
       "         [1.54072683, 1.18184826, 1.43748627, ..., 1.07372318, 2.0618537 ,\n",
       "          1.69679409],\n",
       "         [1.69227599, 1.05159775, 1.67017029, ..., 1.10060965, 1.88687232,\n",
       "          1.18713617]]),\n",
       "  array([[1.33116285, 1.86088903, 1.08195741, ..., 2.23046527, 1.98387903,\n",
       "          1.5741009 ],\n",
       "         [1.43360517, 1.17765548, 1.06325793, ..., 1.94276135, 1.96185081,\n",
       "          1.15108094],\n",
       "         [1.33114885, 1.03528999, 1.81169324, ..., 1.81333753, 2.04250882,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.48626333, 1.03280899, 1.43157496, ..., 2.03792332, 2.19016088,\n",
       "          1.4854386 ],\n",
       "         [1.41460305, 1.11352635, 1.18184826, ..., 1.8995943 , 2.0618537 ,\n",
       "          1.69679409],\n",
       "         [1.38565246, 1.00194326, 1.05159775, ..., 1.88380671, 1.88687232,\n",
       "          1.18713617]]),\n",
       "  array([[1.75229496, 1.86088903, 1.07100682, ..., 1.24601416, 1.8083045 ,\n",
       "          1.19459356],\n",
       "         [1.3757134 , 1.17765548, 1.15483606, ..., 1.14152476, 1.91529173,\n",
       "          1.3450805 ],\n",
       "         [1.04714897, 1.03528999, 1.22908147, ..., 1.0958254 , 1.76202332,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.77179139, 1.03280899, 1.0934602 , ..., 1.23600913, 1.73356331,\n",
       "          1.51860873],\n",
       "         [1.24833316, 1.11352635, 1.15639617, ..., 1.22232164, 1.54590377,\n",
       "          1.49380526],\n",
       "         [1.19976203, 1.00194326, 1.5656282 , ..., 1.1703797 , 1.67241869,\n",
       "          1.27581726]]),\n",
       "  array([[2.07023958, 1.03751956, 1.3539028 , ..., 1.70074383, 1.98387903,\n",
       "          1.12340946],\n",
       "         [2.07630483, 1.03486113, 1.44310783, ..., 1.90931277, 1.96185081,\n",
       "          1.18290296],\n",
       "         [2.1176614 , 1.04095056, 1.36078803, ..., 1.69213135, 2.04250882,\n",
       "          1.14675705],\n",
       "         ...,\n",
       "         [1.81160523, 1.04551916, 1.31213334, ..., 1.7683221 , 2.19016088,\n",
       "          1.14776652],\n",
       "         [1.54072683, 1.04349839, 1.38688744, ..., 1.81045573, 2.0618537 ,\n",
       "          1.21602675],\n",
       "         [1.69227599, 1.05776826, 1.47166422, ..., 1.91030132, 1.88687232,\n",
       "          1.12324267]]),\n",
       "  array([[1.72594368, 1.16024544, 1.59923172, ..., 1.29391417, 1.23061716,\n",
       "          1.29564013],\n",
       "         [1.05362179, 1.12055417, 1.52877416, ..., 1.42737429, 1.18392996,\n",
       "          1.19056253],\n",
       "         [1.11047783, 1.10281393, 1.72903255, ..., 1.46119158, 1.55320612,\n",
       "          1.17893099],\n",
       "         ...,\n",
       "         [1.31626344, 1.08853938, 1.67389745, ..., 1.67175869, 1.6300149 ,\n",
       "          1.2812754 ],\n",
       "         [2.0308627 , 1.09058092, 1.69815659, ..., 1.92730342, 2.0998435 ,\n",
       "          1.47628314],\n",
       "         [1.25344766, 1.1028539 , 1.2812204 , ..., 1.34855018, 1.34326663,\n",
       "          1.28541721]]),\n",
       "  array([[1.83306696, 1.01723553, 0.97826525, ..., 1.10705695, 1.32826908,\n",
       "          1.12706946],\n",
       "         [1.54515157, 0.98797525, 1.04477012, ..., 1.11206253, 1.28824641,\n",
       "          1.13555345],\n",
       "         [1.75419084, 1.84876577, 1.00092216, ..., 1.11279532, 1.28088144,\n",
       "          1.08871089],\n",
       "         ...,\n",
       "         [1.80704448, 1.00916697, 0.95290886, ..., 1.23917292, 1.28084164,\n",
       "          1.15911111],\n",
       "         [1.79795803, 2.09600067, 1.13304869, ..., 1.2291066 , 1.52607034,\n",
       "          1.15187612],\n",
       "         [1.59813601, 1.01669741, 0.94485519, ..., 1.18170135, 1.20427945,\n",
       "          1.116293  ]]),\n",
       "  array([[2.07023958, 2.15522925, 1.034652  , ..., 1.88138698, 1.98387903,\n",
       "          1.29391417],\n",
       "         [2.07630483, 2.17515621, 0.98155422, ..., 2.01273752, 1.96185081,\n",
       "          1.42737429],\n",
       "         [2.1176614 , 2.08048709, 0.97836963, ..., 1.90999593, 2.04250882,\n",
       "          1.46119158],\n",
       "         ...,\n",
       "         [1.81160523, 2.21205177, 0.99705667, ..., 1.98642572, 2.19016088,\n",
       "          1.67175869],\n",
       "         [1.54072683, 2.04663546, 1.03344321, ..., 1.915977  , 2.0618537 ,\n",
       "          1.92730342],\n",
       "         [1.69227599, 2.07725923, 1.00689769, ..., 2.04664512, 1.88687232,\n",
       "          1.34855018]]),\n",
       "  array([[1.39996235, 1.18483994, 1.24687   , ..., 1.3514235 , 1.82557653,\n",
       "          2.04385511],\n",
       "         [1.89553111, 1.08815167, 1.11213476, ..., 1.49242114, 1.42277476,\n",
       "          1.74018722],\n",
       "         [1.54980834, 1.0221904 , 1.29013839, ..., 1.61395233, 1.70676048,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [1.55607828, 1.02853296, 1.14842401, ..., 1.84484785, 1.54225427,\n",
       "          1.8359737 ],\n",
       "         [1.43051353, 1.0930099 , 1.18381704, ..., 2.14160989, 1.80142249,\n",
       "          1.9253121 ],\n",
       "         [1.44218916, 1.04165874, 1.50308824, ..., 1.29537749, 1.51188483,\n",
       "          1.31214762]]),\n",
       "  array([[1.83122672, 1.75229496, 1.89289049, ..., 1.5741009 , 1.99677152,\n",
       "          1.78901639],\n",
       "         [1.75601651, 1.3757134 , 1.61266879, ..., 1.15108094, 1.84218341,\n",
       "          1.55205892],\n",
       "         [1.86308682, 1.04714897, 1.50915218, ..., 1.25651027, 1.65890547,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.99791602, 1.77179139, 1.6762205 , ..., 1.4854386 , 1.94578817,\n",
       "          1.71962776],\n",
       "         [1.78465291, 1.24833316, 1.84295475, ..., 1.69679409, 2.03992909,\n",
       "          1.54759422],\n",
       "         [1.89545605, 1.19976203, 2.2233432 , ..., 1.18713617, 1.26557088,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.10232303, 1.88018421, ..., 1.98499197, 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.21555877, 1.59594466, ..., 1.98302685, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.39600557, 1.60810897, ..., 1.96123021, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.02200999, 1.6921999 , ..., 2.19983661, 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.85423393, 1.83000872, ..., 2.13323445, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.17729895, 1.61769339, ..., 2.02809005, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.91767623, 1.50115227, 2.02668556, ..., 1.55613208, 1.27299853,\n",
       "          1.78901639],\n",
       "         [2.04331151, 1.85401787, 2.0531079 , ..., 1.2340156 , 1.61536378,\n",
       "          1.55205892],\n",
       "         [1.92128971, 1.70684758, 2.04906555, ..., 1.47221644, 1.38237962,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [2.03920919, 1.73517632, 2.11837345, ..., 1.58732565, 1.78323908,\n",
       "          1.71962776],\n",
       "         [1.8834902 , 1.85811364, 2.26032609, ..., 1.76861528, 1.52637877,\n",
       "          1.54759422],\n",
       "         [1.66953805, 1.88056571, 2.07907756, ..., 1.71795398, 1.60477037,\n",
       "          1.48490784]]),\n",
       "  array([[1.25307349, 1.75229496, 2.07023958, ..., 2.08940659, 1.32641115,\n",
       "          1.78901639],\n",
       "         [1.80478644, 1.3757134 , 2.07630483, ..., 2.17462029, 1.12950246,\n",
       "          1.55205892],\n",
       "         [1.34397588, 1.04714897, 2.1176614 , ..., 2.06733522, 1.06045605,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [2.16205518, 1.77179139, 1.81160523, ..., 2.05902785, 1.239142  ,\n",
       "          1.71962776],\n",
       "         [1.51954481, 1.24833316, 1.54072683, ..., 1.89257828, 1.11277916,\n",
       "          1.54759422],\n",
       "         [2.00835726, 1.19976203, 1.69227599, ..., 2.05597432, 1.4850472 ,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.86088903, 1.977652  , ..., 1.5741009 , 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.17765548, 1.75518776, ..., 1.15108094, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.03528999, 1.71413889, ..., 1.25651027, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.03280899, 1.86167654, ..., 1.4854386 , 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.11352635, 1.50530405, ..., 1.69679409, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.00194326, 1.99507045, ..., 1.18713617, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.86088903, 1.80094609, 1.05789014, ..., 1.04133847, 1.12340946,\n",
       "          1.27233377],\n",
       "         [1.17765548, 1.7226584 , 1.0677121 , ..., 1.03836256, 1.18290296,\n",
       "          1.21674764],\n",
       "         [1.03528999, 1.81337079, 1.14106975, ..., 1.43838451, 1.14675705,\n",
       "          1.21704044],\n",
       "         ...,\n",
       "         [1.03280899, 1.26449848, 1.60417913, ..., 1.19276843, 1.14776652,\n",
       "          1.60963757],\n",
       "         [1.11352635, 1.46604195, 1.13562404, ..., 1.10396782, 1.21602675,\n",
       "          1.08210882],\n",
       "         [1.00194326, 1.82333154, 1.1113533 , ..., 0.96848222, 1.12324267,\n",
       "          0.95134058]]),\n",
       "  array([[2.0164485 , 2.11122884, 1.8603085 , ..., 2.06812628, 1.91405056,\n",
       "          2.04385511],\n",
       "         [2.10147044, 2.07640129, 1.70069377, ..., 2.29080201, 2.05467047,\n",
       "          1.74018722],\n",
       "         [2.02210251, 1.91512371, 1.72271164, ..., 2.14675775, 1.87148816,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [2.05108984, 2.17627526, 1.75724392, ..., 2.26667824, 1.85970597,\n",
       "          1.8359737 ],\n",
       "         [2.06003767, 1.99907342, 1.70070169, ..., 2.14545164, 1.91177727,\n",
       "          1.9253121 ],\n",
       "         [1.98849528, 2.01909049, 2.00432033, ..., 2.26898901, 1.80675301,\n",
       "          1.31214762]]),\n",
       "  array([[2.11122884, 1.34008266, 1.51062881, ..., 2.31046349, 1.8083045 ,\n",
       "          1.19459356],\n",
       "         [2.07640129, 1.33861511, 1.57174699, ..., 2.25997568, 1.91529173,\n",
       "          1.3450805 ],\n",
       "         [1.91512371, 1.44155811, 1.65440876, ..., 2.19317734, 1.76202332,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [2.17627526, 2.20797634, 1.15214069, ..., 2.32994071, 1.73356331,\n",
       "          1.51860873],\n",
       "         [1.99907342, 2.11522492, 1.54446902, ..., 2.21726404, 1.54590377,\n",
       "          1.49380526],\n",
       "         [2.01909049, 2.25226839, 1.70415854, ..., 2.14824811, 1.67241869,\n",
       "          1.27581726]]),\n",
       "  array([[1.34562014, 1.24687   , 2.10167375, ..., 1.87246964, 1.5237267 ,\n",
       "          2.04385511],\n",
       "         [1.56281772, 1.11213476, 1.67264539, ..., 2.05258824, 1.14702919,\n",
       "          1.74018722],\n",
       "         [1.44850143, 1.29013839, 1.92539513, ..., 2.12673846, 1.5049025 ,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [1.56132512, 1.14842401, 2.13822407, ..., 1.62620206, 1.37195886,\n",
       "          1.8359737 ],\n",
       "         [2.10628474, 1.18381704, 2.21036352, ..., 1.83123431, 1.615447  ,\n",
       "          1.9253121 ],\n",
       "         [1.66443043, 1.50308824, 1.36009102, ..., 1.4206291 , 1.2821139 ,\n",
       "          1.31214762]]),\n",
       "  array([[1.39996235, 1.75297328, 2.23699836, ..., 1.72472204, 1.82436062,\n",
       "          1.98282715],\n",
       "         [1.89553111, 1.88667011, 2.15092647, ..., 1.40280265, 2.05787319,\n",
       "          1.98860674],\n",
       "         [1.54980834, 1.98896293, 2.20004411, ..., 1.51098905, 1.90693077,\n",
       "          1.93438043],\n",
       "         ...,\n",
       "         [1.55607828, 1.67644037, 2.22154727, ..., 1.29195975, 1.62035261,\n",
       "          1.99381314],\n",
       "         [1.43051353, 1.85418909, 2.1769318 , ..., 1.24988412, 1.62030327,\n",
       "          2.01312788],\n",
       "         [1.44218916, 2.16559666, 2.22639299, ..., 1.34324229, 1.81533924,\n",
       "          2.04619953]]),\n",
       "  array([[1.71005402, 1.99563637, 2.38739371, ..., 2.07400695, 2.03348599,\n",
       "          1.82557653],\n",
       "         [1.70280803, 2.00481166, 2.38521073, ..., 2.13557445, 2.01439139,\n",
       "          1.42277476],\n",
       "         [1.72806999, 1.88667891, 2.31375543, ..., 1.94810722, 1.79797796,\n",
       "          1.70676048],\n",
       "         ...,\n",
       "         [1.80336951, 1.84889806, 2.14316596, ..., 2.07522002, 1.75422262,\n",
       "          1.54225427],\n",
       "         [1.62721211, 1.89649052, 2.28895697, ..., 1.972218  , 1.70052742,\n",
       "          1.80142249],\n",
       "         [1.78123099, 1.94789828, 2.23214936, ..., 1.9030316 , 1.66292863,\n",
       "          1.51188483]]),\n",
       "  array([[1.75229496, 1.82784352, 1.39996235, ..., 1.96100764, 1.82557653,\n",
       "          1.83741695],\n",
       "         [1.3757134 , 2.08284867, 1.89553111, ..., 1.48751004, 1.42277476,\n",
       "          1.66553463],\n",
       "         [1.04714897, 1.97835175, 1.54980834, ..., 1.64690799, 1.70676048,\n",
       "          1.72920306],\n",
       "         ...,\n",
       "         [1.77179139, 2.05528472, 1.55607828, ..., 1.52760336, 1.54225427,\n",
       "          1.9720211 ],\n",
       "         [1.24833316, 1.88078411, 1.43051353, ..., 1.80614854, 1.80142249,\n",
       "          1.88005041],\n",
       "         [1.19976203, 1.91076932, 1.44218916, ..., 1.34941361, 1.51188483,\n",
       "          1.86498024]]),\n",
       "  array([[2.34901756, 1.39939656, 2.15086682, ..., 1.83791051, 1.45973193,\n",
       "          1.00298922],\n",
       "         [2.16465008, 1.27567439, 2.09126753, ..., 1.67375866, 1.67142463,\n",
       "          1.08169835],\n",
       "         [2.23439873, 1.54128606, 2.10698587, ..., 1.66685942, 1.66076718,\n",
       "          1.03011871],\n",
       "         ...,\n",
       "         [2.28464748, 1.34175776, 2.06309133, ..., 1.72055571, 1.42827598,\n",
       "          1.27314305],\n",
       "         [2.28021865, 1.51999062, 2.07939853, ..., 1.70909415, 1.82233971,\n",
       "          1.38529857],\n",
       "         [2.28583315, 1.4774952 , 2.15783974, ..., 1.74025424, 1.91587439,\n",
       "          1.29703725]]),\n",
       "  array([[2.01407309, 1.71198293, 1.07012829, ..., 1.77070807, 2.07641438,\n",
       "          1.71763761],\n",
       "         [2.1079194 , 1.69597825, 1.0156788 , ..., 1.91394002, 2.21178879,\n",
       "          1.7413519 ],\n",
       "         [2.07937825, 1.74070717, 1.2465584 , ..., 1.61263429, 2.20844802,\n",
       "          1.92002782],\n",
       "         ...,\n",
       "         [1.95421047, 1.56151906, 1.02659789, ..., 1.56594296, 1.80088515,\n",
       "          1.65001591],\n",
       "         [2.36823024, 1.75463588, 1.22889178, ..., 1.6362167 , 1.31721898,\n",
       "          1.93060308],\n",
       "         [2.25410299, 1.79308991, 1.05490739, ..., 1.90919938, 2.15823676,\n",
       "          1.79538801]]),\n",
       "  array([[2.08489635, 1.08195741, 1.55903525, ..., 1.23061716, 1.5741009 ,\n",
       "          2.34767687],\n",
       "         [2.15181918, 1.06325793, 1.6770274 , ..., 1.18392996, 1.15108094,\n",
       "          2.30312485],\n",
       "         [1.98054803, 1.81169324, 1.50519785, ..., 1.55320612, 1.25651027,\n",
       "          2.27531667],\n",
       "         ...,\n",
       "         [2.23286486, 1.43157496, 1.48354009, ..., 1.6300149 , 1.4854386 ,\n",
       "          2.24468292],\n",
       "         [2.49247844, 1.18184826, 1.72649815, ..., 2.0998435 , 1.69679409,\n",
       "          2.20390288],\n",
       "         [1.85103322, 1.05159775, 1.32457787, ..., 1.34326663, 1.18713617,\n",
       "          2.31283583]]),\n",
       "  array([[2.08489635, 1.73381853, 1.18981441, ..., 0.98105782, 1.08403577,\n",
       "          0.99326476],\n",
       "         [2.15181918, 1.77260702, 1.07015243, ..., 1.02536348, 1.12232168,\n",
       "          1.04183067],\n",
       "         [1.98054803, 2.04948639, 1.27549194, ..., 0.97739391, 1.17062326,\n",
       "          1.11117826],\n",
       "         ...,\n",
       "         [2.23286486, 2.32484027, 1.62273201, ..., 1.0521073 , 1.35224051,\n",
       "          1.06054999],\n",
       "         [2.49247844, 2.08011137, 1.69682546, ..., 0.98838998, 1.77043307,\n",
       "          1.55622024],\n",
       "         [1.85103322, 1.87786189, 1.17960756, ..., 1.60966326, 1.14693586,\n",
       "          1.30538952]]),\n",
       "  array([[1.91376914, 1.72953027, 1.75752249, ..., 1.49634981, 2.1451795 ,\n",
       "          1.63386067],\n",
       "         [1.99224525, 1.89555511, 1.8698548 , ..., 1.15081784, 2.32620949,\n",
       "          1.65172422],\n",
       "         [1.91507158, 1.58147772, 1.80737767, ..., 1.12599499, 2.32086157,\n",
       "          1.67384381],\n",
       "         ...,\n",
       "         [1.94702779, 1.87663679, 1.82381931, ..., 1.33319748, 2.10948364,\n",
       "          1.79288075],\n",
       "         [1.79022015, 1.59357074, 1.87943686, ..., 1.29303003, 2.35679838,\n",
       "          1.49304492],\n",
       "         [1.90892454, 1.77203609, 1.72167505, ..., 1.2917917 , 2.3506765 ,\n",
       "          1.68387859]]),\n",
       "  array([[1.85530073, 2.15522925, 2.26307905, ..., 2.04988054, 1.08403577,\n",
       "          1.9001432 ],\n",
       "         [1.69618465, 2.17515621, 2.08614175, ..., 2.08020804, 1.12232168,\n",
       "          2.08694116],\n",
       "         [1.48570318, 2.08048709, 2.11737687, ..., 2.02733891, 1.17062326,\n",
       "          1.92820833],\n",
       "         ...,\n",
       "         [1.84502877, 2.21205177, 1.96585764, ..., 2.06109775, 1.35224051,\n",
       "          1.84081307],\n",
       "         [2.02042593, 2.04663546, 1.88999768, ..., 2.09224227, 1.77043307,\n",
       "          1.87193626],\n",
       "         [2.05576829, 2.07725923, 1.01021402, ..., 1.91768297, 1.14693586,\n",
       "          1.90958341]]),\n",
       "  array([[1.08195741, 2.15522925, 1.08974358, ..., 0.99326476, 1.61177011,\n",
       "          1.5741009 ],\n",
       "         [1.06325793, 2.17515621, 1.51074991, ..., 1.04183067, 1.2126352 ,\n",
       "          1.15108094],\n",
       "         [1.81169324, 2.08048709, 1.53020264, ..., 1.11117826, 1.23892683,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.43157496, 2.21205177, 1.44092493, ..., 1.06054999, 1.66982811,\n",
       "          1.4854386 ],\n",
       "         [1.18184826, 2.04663546, 1.16093991, ..., 1.55622024, 1.64448561,\n",
       "          1.69679409],\n",
       "         [1.05159775, 2.07725923, 1.14836264, ..., 1.30538952, 1.22556603,\n",
       "          1.18713617]]),\n",
       "  array([[1.25307349, 1.75229496, 1.89680031, ..., 1.32641115, 1.83325358,\n",
       "          1.08694212],\n",
       "         [1.80478644, 1.3757134 , 1.7747679 , ..., 1.12950246, 1.89029038,\n",
       "          1.04285192],\n",
       "         [1.34397588, 1.04714897, 1.80218561, ..., 1.06045605, 1.12736784,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [2.16205518, 1.77179139, 2.00472197, ..., 1.239142  , 1.10130385,\n",
       "          1.05734956],\n",
       "         [1.51954481, 1.24833316, 1.91780238, ..., 1.11277916, 1.15497121,\n",
       "          1.23543696],\n",
       "         [2.00835726, 1.19976203, 1.79334066, ..., 1.4850472 , 1.87689646,\n",
       "          1.32472077]]),\n",
       "  array([[1.71198293, 1.58170917, 1.72594368, ..., 1.83791051, 1.71087526,\n",
       "          2.04385511],\n",
       "         [1.69597825, 1.47537767, 1.05362179, ..., 1.67375866, 1.56821642,\n",
       "          1.74018722],\n",
       "         [1.74070717, 1.99040416, 1.11047783, ..., 1.66685942, 1.5963024 ,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [1.56151906, 1.7003191 , 1.31626344, ..., 1.72055571, 1.4948428 ,\n",
       "          1.8359737 ],\n",
       "         [1.75463588, 1.74789275, 2.0308627 , ..., 1.70909415, 1.60590269,\n",
       "          1.9253121 ],\n",
       "         [1.79308991, 2.04005321, 1.25344766, ..., 1.74025424, 1.57802555,\n",
       "          1.31214762]]),\n",
       "  array([[1.75229496, 1.51062881, 1.78901639, ..., 1.98387903, 1.19459356,\n",
       "          1.08694212],\n",
       "         [1.3757134 , 1.57174699, 1.55205892, ..., 1.96185081, 1.3450805 ,\n",
       "          1.04285192],\n",
       "         [1.04714897, 1.65440876, 1.64322633, ..., 2.04250882, 1.45791797,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [1.77179139, 1.15214069, 1.71962776, ..., 2.19016088, 1.51860873,\n",
       "          1.05734956],\n",
       "         [1.24833316, 1.54446902, 1.54759422, ..., 2.0618537 , 1.49380526,\n",
       "          1.23543696],\n",
       "         [1.19976203, 1.70415854, 1.48490784, ..., 1.88687232, 1.27581726,\n",
       "          1.32472077]]),\n",
       "  array([[1.75229496, 1.51062881, 1.10989361, ..., 1.98499197, 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.57174699, 1.09137139, ..., 1.98302685, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.65440876, 1.06849566, ..., 1.96123021, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.15214069, 1.14183599, ..., 2.19983661, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54446902, 1.13532906, ..., 2.13323445, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.70415854, 1.10867497, ..., 2.02809005, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.24687   , 2.10167375, 2.04740901, ..., 2.11088923, 1.5237267 ,\n",
       "          1.82557653],\n",
       "         [1.11213476, 1.67264539, 2.33877017, ..., 2.01260578, 1.14702919,\n",
       "          1.42277476],\n",
       "         [1.29013839, 1.92539513, 1.92971246, ..., 1.94838023, 1.5049025 ,\n",
       "          1.70676048],\n",
       "         ...,\n",
       "         [1.14842401, 2.13822407, 2.15705914, ..., 2.16554079, 1.37195886,\n",
       "          1.54225427],\n",
       "         [1.18381704, 2.21036352, 2.06621865, ..., 1.82147228, 1.615447  ,\n",
       "          1.80142249],\n",
       "         [1.50308824, 1.36009102, 2.03544867, ..., 1.96596476, 1.2821139 ,\n",
       "          1.51188483]]),\n",
       "  array([[2.07509862, 1.50948065, 2.20365359, ..., 1.14793199, 1.49634981,\n",
       "          2.21800728],\n",
       "         [1.88027155, 1.61836629, 2.19110078, ..., 1.15223675, 1.15081784,\n",
       "          1.43408537],\n",
       "         [2.19589534, 1.55918883, 2.06343991, ..., 1.09211877, 1.12599499,\n",
       "          2.03421238],\n",
       "         ...,\n",
       "         [2.07740665, 1.97520767, 1.90075079, ..., 1.12298879, 1.33319748,\n",
       "          2.17857692],\n",
       "         [2.22638705, 1.50703607, 2.00976197, ..., 1.165544  , 1.29303003,\n",
       "          2.3283029 ],\n",
       "         [2.0065958 , 1.50756762, 2.03292409, ..., 1.12759566, 1.2917917 ,\n",
       "          1.22255985]]),\n",
       "  array([[1.77093112, 1.24687   , 2.0025032 , ..., 2.34767687, 1.5237267 ,\n",
       "          1.26738841],\n",
       "         [1.71760955, 1.11213476, 2.00394777, ..., 2.30312485, 1.14702919,\n",
       "          1.4240069 ],\n",
       "         [1.7338964 , 1.29013839, 2.00384658, ..., 2.27531667, 1.5049025 ,\n",
       "          1.72632829],\n",
       "         ...,\n",
       "         [1.56420932, 1.14842401, 2.05602952, ..., 2.24468292, 1.37195886,\n",
       "          1.40738742],\n",
       "         [1.59930237, 1.18381704, 2.0055233 , ..., 2.20390288, 1.615447  ,\n",
       "          1.42122894],\n",
       "         [1.89512047, 1.50308824, 1.92355718, ..., 2.31283583, 1.2821139 ,\n",
       "          1.39889214]]),\n",
       "  array([[2.08489635, 1.65184439, 1.46676461, ..., 1.91405056, 1.23061716,\n",
       "          2.04385511],\n",
       "         [2.15181918, 1.5361943 , 1.01618022, ..., 2.05467047, 1.18392996,\n",
       "          1.74018722],\n",
       "         [1.98054803, 1.66926676, 1.04424044, ..., 1.87148816, 1.55320612,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [2.23286486, 1.81375171, 1.9235316 , ..., 1.85970597, 1.6300149 ,\n",
       "          1.8359737 ],\n",
       "         [2.49247844, 1.85668244, 1.02532121, ..., 1.91177727, 2.0998435 ,\n",
       "          1.9253121 ],\n",
       "         [1.85103322, 1.36043676, 2.07108281, ..., 1.80675301, 1.34326663,\n",
       "          1.31214762]]),\n",
       "  array([[1.1931415 , 1.04225067, 1.33795157, ..., 2.13139336, 2.10545736,\n",
       "          1.37362353],\n",
       "         [1.12476291, 1.07727265, 1.33513128, ..., 2.11463635, 2.09465638,\n",
       "          1.93124476],\n",
       "         [1.46583361, 1.02049032, 1.22103023, ..., 2.03206588, 2.01723807,\n",
       "          1.81574941],\n",
       "         ...,\n",
       "         [1.16791012, 1.06192175, 1.47487346, ..., 2.1689488 , 2.37649634,\n",
       "          1.27282486],\n",
       "         [1.18229419, 1.07116564, 1.27355749, ..., 2.14787689, 2.22154072,\n",
       "          1.18346705],\n",
       "         [1.0939519 , 1.53527255, 1.18265642, ..., 1.98081827, 2.06757307,\n",
       "          1.41524096]]),\n",
       "  array([[1.25307349, 1.75229496, 1.51062881, ..., 2.10545736, 1.37362353,\n",
       "          1.19459356],\n",
       "         [1.80478644, 1.3757134 , 1.57174699, ..., 2.09465638, 1.93124476,\n",
       "          1.3450805 ],\n",
       "         [1.34397588, 1.04714897, 1.65440876, ..., 2.01723807, 1.81574941,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [2.16205518, 1.77179139, 1.15214069, ..., 2.37649634, 1.27282486,\n",
       "          1.51860873],\n",
       "         [1.51954481, 1.24833316, 1.54446902, ..., 2.22154072, 1.18346705,\n",
       "          1.49380526],\n",
       "         [2.00835726, 1.19976203, 1.70415854, ..., 2.06757307, 1.41524096,\n",
       "          1.27581726]]),\n",
       "  array([[1.13444977, 1.51062881, 1.86088903, ..., 1.98387903, 1.04133847,\n",
       "          1.19459356],\n",
       "         [1.1925719 , 1.57174699, 1.17765548, ..., 1.96185081, 1.03836256,\n",
       "          1.3450805 ],\n",
       "         [1.33296486, 1.65440876, 1.03528999, ..., 2.04250882, 1.43838451,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.18887969, 1.15214069, 1.03280899, ..., 2.19016088, 1.19276843,\n",
       "          1.51860873],\n",
       "         [1.14884023, 1.54446902, 1.11352635, ..., 2.0618537 , 1.10396782,\n",
       "          1.49380526],\n",
       "         [2.1648543 , 1.70415854, 1.00194326, ..., 1.88687232, 0.96848222,\n",
       "          1.27581726]]),\n",
       "  array([[2.07023958, 1.06057352, 2.26307905, ..., 1.58254849, 2.52193557,\n",
       "          1.26255727],\n",
       "         [2.07630483, 1.07447914, 2.08614175, ..., 1.81242892, 1.68414645,\n",
       "          1.49385951],\n",
       "         [2.1176614 , 1.02663847, 2.11737687, ..., 1.72002167, 1.09664211,\n",
       "          1.99610538],\n",
       "         ...,\n",
       "         [1.81160523, 1.074177  , 1.96585764, ..., 1.63839724, 2.12012668,\n",
       "          1.26372045],\n",
       "         [1.54072683, 1.02748035, 1.88999768, ..., 1.47901442, 1.09800949,\n",
       "          1.10301403],\n",
       "         [1.69227599, 1.02684632, 1.01021402, ..., 1.36969138, 1.75973444,\n",
       "          1.8149094 ]]),\n",
       "  array([[1.43813072, 1.47179179, 1.14838982, ..., 2.2671277 , 2.01608469,\n",
       "          1.33989328],\n",
       "         [1.30945917, 1.57282717, 1.40351354, ..., 2.25797807, 2.06398644,\n",
       "          1.37146885],\n",
       "         [1.80261338, 2.27978342, 1.22274443, ..., 2.19869978, 2.0821042 ,\n",
       "          1.27441984],\n",
       "         ...,\n",
       "         [1.46388648, 2.01072942, 1.13158075, ..., 2.18812136, 2.07245647,\n",
       "          1.42963919],\n",
       "         [1.67962659, 2.22887495, 1.1513273 , ..., 2.17607145, 2.0631491 ,\n",
       "          1.64716575],\n",
       "         [1.45940559, 1.29489474, 1.26234496, ..., 2.02621582, 2.13329692,\n",
       "          1.2043238 ]]),\n",
       "  array([[1.51062881, 1.24687   , 1.916748  , ..., 1.06779124, 1.32517832,\n",
       "          2.0456013 ],\n",
       "         [1.57174699, 1.11213476, 1.61192965, ..., 1.2554284 , 1.79760736,\n",
       "          2.02269644],\n",
       "         [1.65440876, 1.29013839, 1.76266709, ..., 1.2628121 , 1.89329483,\n",
       "          1.90325211],\n",
       "         ...,\n",
       "         [1.15214069, 1.14842401, 1.90421006, ..., 1.08171167, 1.55821406,\n",
       "          1.72395554],\n",
       "         [1.54446902, 1.18381704, 1.86717489, ..., 1.17897114, 1.48921961,\n",
       "          1.96108982],\n",
       "         [1.70415854, 1.50308824, 1.6229841 , ..., 1.04028208, 1.35230296,\n",
       "          1.96014473]]),\n",
       "  array([[2.17435502, 1.21077952, 1.68504511, ..., 1.23061716, 1.63321219,\n",
       "          1.06227466],\n",
       "         [1.2952327 , 1.68313904, 1.34909177, ..., 1.18392996, 1.35723391,\n",
       "          1.07781197],\n",
       "         [2.12248882, 1.52697874, 1.88863295, ..., 1.55320612, 1.12373842,\n",
       "          1.03888112],\n",
       "         ...,\n",
       "         [1.55681602, 1.38887435, 1.62386308, ..., 1.6300149 , 1.27775057,\n",
       "          1.12056441],\n",
       "         [1.74273618, 1.51798142, 1.98236101, ..., 2.0998435 , 1.29072827,\n",
       "          1.10193779],\n",
       "         [1.05375208, 1.15899983, 1.32203773, ..., 1.34326663, 1.75975777,\n",
       "          1.51723071]]),\n",
       "  array([[2.07023958, 1.37498166, 1.45261416, ..., 1.96784839, 1.107412  ,\n",
       "          1.9258579 ],\n",
       "         [2.07630483, 1.34133453, 1.12549876, ..., 1.34677255, 1.19049781,\n",
       "          1.7524926 ],\n",
       "         [2.1176614 , 1.57367971, 1.16183796, ..., 1.88168451, 1.07385924,\n",
       "          2.02136065],\n",
       "         ...,\n",
       "         [1.81160523, 1.66894004, 1.18250544, ..., 1.64520854, 1.20287823,\n",
       "          1.93153985],\n",
       "         [1.54072683, 1.77465402, 1.32671684, ..., 1.61632899, 1.24350036,\n",
       "          1.79108766],\n",
       "         [1.69227599, 1.39099706, 1.28372241, ..., 1.26614176, 1.12196292,\n",
       "          1.95840684]]),\n",
       "  array([[1.75229496, 2.07023958, 2.00934217, ..., 1.96158339, 1.8083045 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.07630483, 2.01214278, ..., 1.70536096, 1.91529173,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.1176614 , 2.00413486, ..., 1.92713834, 1.76202332,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.81160523, 2.01826492, ..., 2.05569654, 1.73356331,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54072683, 1.8027581 , ..., 1.76966979, 1.54590377,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.69227599, 2.10242147, ..., 1.58597554, 1.67241869,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.41336423, 1.51062881, ..., 1.96100764, 1.19459356,\n",
       "          1.08694212],\n",
       "         [1.3757134 , 1.68426866, 1.57174699, ..., 1.48751004, 1.3450805 ,\n",
       "          1.04285192],\n",
       "         [1.04714897, 2.42950053, 1.65440876, ..., 1.64690799, 1.45791797,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [1.77179139, 1.92345199, 1.15214069, ..., 1.52760336, 1.51860873,\n",
       "          1.05734956],\n",
       "         [1.24833316, 1.49501566, 1.54446902, ..., 1.80614854, 1.49380526,\n",
       "          1.23543696],\n",
       "         [1.19976203, 1.14389835, 1.70415854, ..., 1.34941361, 1.27581726,\n",
       "          1.32472077]]),\n",
       "  array([[1.75229496, 1.39996235, 2.37747944, ..., 1.23061716, 1.8083045 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89553111, 1.93373219, ..., 1.18392996, 1.91529173,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.54980834, 2.36122218, ..., 1.55320612, 1.76202332,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 2.37456741, ..., 1.6300149 , 1.73356331,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.43051353, 2.47449087, ..., 2.0998435 , 1.54590377,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.44218916, 2.37103696, ..., 1.34326663, 1.67241869,\n",
       "          1.48490784]]),\n",
       "  array([[1.93786522, 2.26307905, 1.18981441, ..., 1.96432369, 1.32641115,\n",
       "          1.23061716],\n",
       "         [1.87043739, 2.08614175, 1.07015243, ..., 1.04625723, 1.12950246,\n",
       "          1.18392996],\n",
       "         [1.87518156, 2.11737687, 1.27549194, ..., 1.05699407, 1.06045605,\n",
       "          1.55320612],\n",
       "         ...,\n",
       "         [2.07327999, 1.96585764, 1.62273201, ..., 1.24578735, 1.239142  ,\n",
       "          1.6300149 ],\n",
       "         [1.7650156 , 1.88999768, 1.69682546, ..., 1.07372318, 1.11277916,\n",
       "          2.0998435 ],\n",
       "         [1.79206338, 1.01021402, 1.17960756, ..., 1.10060965, 1.4850472 ,\n",
       "          1.34326663]]),\n",
       "  array([[1.93786522, 1.05668736, 1.79434963, ..., 1.64326462, 1.76113621,\n",
       "          1.9585173 ],\n",
       "         [1.87043739, 1.26187784, 2.02769993, ..., 1.71138484, 1.9033869 ,\n",
       "          1.72264461],\n",
       "         [1.87518156, 1.02906291, 1.85346864, ..., 1.61452843, 1.85916693,\n",
       "          1.58177802],\n",
       "         ...,\n",
       "         [2.07327999, 1.08847392, 1.7324119 , ..., 1.68883055, 1.78232411,\n",
       "          1.60647979],\n",
       "         [1.7650156 , 1.16489977, 1.74763124, ..., 1.55877823, 1.69923431,\n",
       "          1.59710946],\n",
       "         [1.79206338, 1.24968903, 0.99587557, ..., 1.7938021 , 1.39861847,\n",
       "          1.43491601]]),\n",
       "  array([[2.07023958, 2.15522925, 1.9723396 , ..., 1.65076199, 1.61177011,\n",
       "          1.5741009 ],\n",
       "         [2.07630483, 2.17515621, 1.16353949, ..., 1.13402999, 1.2126352 ,\n",
       "          1.15108094],\n",
       "         [2.1176614 , 2.08048709, 1.64727099, ..., 1.09393777, 1.23892683,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.81160523, 2.21205177, 1.79735342, ..., 1.19710844, 1.66982811,\n",
       "          1.4854386 ],\n",
       "         [1.54072683, 2.04663546, 1.63533023, ..., 1.27105338, 1.64448561,\n",
       "          1.69679409],\n",
       "         [1.69227599, 2.07725923, 1.59638146, ..., 1.19619956, 1.22556603,\n",
       "          1.18713617]]),\n",
       "  array([[1.60847291, 1.1095598 , 1.52132349, ..., 1.40958767, 2.20439914,\n",
       "          1.46054516],\n",
       "         [1.35935946, 1.10471154, 1.24583779, ..., 1.53621583, 2.12523809,\n",
       "          1.19282932],\n",
       "         [1.76830919, 1.16280604, 1.12818241, ..., 1.81532569, 2.07647221,\n",
       "          1.15939498],\n",
       "         ...,\n",
       "         [1.55294879, 1.06735871, 1.12005351, ..., 1.30157956, 2.20350442,\n",
       "          1.27896888],\n",
       "         [1.85960462, 1.14506446, 1.1110386 , ..., 1.73049738, 2.29747139,\n",
       "          1.56930235],\n",
       "         [1.27624195, 1.15366698, 1.07273014, ..., 1.34349737, 1.91581996,\n",
       "          1.09548947]]),\n",
       "  array([[2.08489635, 1.18981441, 1.04165623, ..., 1.90984496, 1.52571342,\n",
       "          1.02804087],\n",
       "         [2.15181918, 1.07015243, 1.06749727, ..., 2.05505159, 1.34867808,\n",
       "          1.06139975],\n",
       "         [1.98054803, 1.27549194, 1.07442078, ..., 2.08965552, 1.50022444,\n",
       "          1.03740545],\n",
       "         ...,\n",
       "         [2.23286486, 1.62273201, 1.08262627, ..., 1.84803364, 1.27500181,\n",
       "          1.08405756],\n",
       "         [2.49247844, 1.69682546, 1.11883243, ..., 2.11592757, 1.29467686,\n",
       "          1.04605939],\n",
       "         [1.85103322, 1.17960756, 1.07721099, ..., 2.07773787, 1.66298139,\n",
       "          2.05174449]]),\n",
       "  array([[1.4133791 , 2.07023958, 1.977652  , ..., 1.78228799, 1.5741009 ,\n",
       "          1.85543403],\n",
       "         [1.49390561, 2.07630483, 1.75518776, ..., 1.84399206, 1.15108094,\n",
       "          2.05918342],\n",
       "         [1.38213409, 2.1176614 , 1.71413889, ..., 1.82282187, 1.25651027,\n",
       "          1.74445451],\n",
       "         ...,\n",
       "         [1.50815499, 1.81160523, 1.86167654, ..., 1.62085084, 1.4854386 ,\n",
       "          1.99830385],\n",
       "         [1.3463585 , 1.54072683, 1.50530405, ..., 1.58601251, 1.69679409,\n",
       "          1.50460286],\n",
       "         [1.28697989, 1.69227599, 1.99507045, ..., 1.54541739, 1.18713617,\n",
       "          1.72188482]]),\n",
       "  array([[1.78074092, 2.10123197, 1.63974049, ..., 1.32641115, 1.19348033,\n",
       "          1.80123634],\n",
       "         [1.4522812 , 1.9081743 , 1.55766563, ..., 1.12950246, 1.06242535,\n",
       "          1.88868763],\n",
       "         [1.73002967, 2.01388974, 2.00467731, ..., 1.06045605, 1.12527719,\n",
       "          1.92337735],\n",
       "         ...,\n",
       "         [1.79214317, 2.19525683, 1.77625144, ..., 1.239142  , 1.3172416 ,\n",
       "          1.51800671],\n",
       "         [1.79096869, 2.04376811, 1.94604928, ..., 1.11277916, 1.49010456,\n",
       "          1.87551927],\n",
       "         [1.50262334, 1.62254559, 1.40621958, ..., 1.4850472 , 1.10300218,\n",
       "          1.56040657]]),\n",
       "  array([[1.51062881, 1.29125896, 2.29640743, ..., 1.24601416, 1.5741009 ,\n",
       "          1.19459356],\n",
       "         [1.57174699, 1.19379942, 2.50619434, ..., 1.14152476, 1.15108094,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 1.44208639, 2.03070712, ..., 1.0958254 , 1.25651027,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 1.20247394, 2.28002407, ..., 1.23600913, 1.4854386 ,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.24552696, 1.60011726, ..., 1.22232164, 1.69679409,\n",
       "          1.49380526],\n",
       "         [1.70415854, 1.80370256, 2.42171736, ..., 1.1703797 , 1.18713617,\n",
       "          1.27581726]]),\n",
       "  array([[1.08195741, 2.15522925, 2.26307905, ..., 1.14000785, 2.04988054,\n",
       "          1.94799126],\n",
       "         [1.06325793, 2.17515621, 2.08614175, ..., 1.29494413, 2.08020804,\n",
       "          1.76402984],\n",
       "         [1.81169324, 2.08048709, 2.11737687, ..., 1.45709892, 2.02733891,\n",
       "          1.98001575],\n",
       "         ...,\n",
       "         [1.43157496, 2.21205177, 1.96585764, ..., 1.16987343, 2.06109775,\n",
       "          1.82370635],\n",
       "         [1.18184826, 2.04663546, 1.88999768, ..., 1.15935142, 2.09224227,\n",
       "          1.96216926],\n",
       "         [1.05159775, 2.07725923, 1.01021402, ..., 1.13667756, 1.91768297,\n",
       "          1.97591158]]),\n",
       "  array([[1.07963804, 1.51642183, 1.05534986, ..., 1.07575533, 2.21856618,\n",
       "          1.06675424],\n",
       "         [1.0683355 , 1.10954354, 1.02286742, ..., 1.1012497 , 1.79801521,\n",
       "          1.05274785],\n",
       "         [1.19370966, 1.3648734 , 0.9842662 , ..., 1.1028994 , 1.998678  ,\n",
       "          1.11069308],\n",
       "         ...,\n",
       "         [1.06638658, 1.08645211, 1.07256861, ..., 1.09105286, 1.92350979,\n",
       "          1.06035019],\n",
       "         [1.10045227, 1.09027798, 1.05922363, ..., 1.17522584, 2.29854555,\n",
       "          1.03394759],\n",
       "         [1.09881032, 1.46863427, 0.94520915, ..., 1.11463885, 2.18613665,\n",
       "          1.23327119]]),\n",
       "  array([[1.75229496, 1.32985666, 1.51062881, ..., 2.34303413, 2.20439914,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.04878836, 1.57174699, ..., 1.54646589, 2.12523809,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.07031222, 1.65440876, ..., 2.28352576, 2.07647221,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.14857632, 1.15214069, ..., 2.35512193, 2.20350442,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.14206397, 1.54446902, ..., 2.32505193, 2.29747139,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.12409293, 1.70415854, ..., 2.25277172, 1.91581996,\n",
       "          1.48490784]]),\n",
       "  array([[2.01407309, 2.32564265, 2.08489635, ..., 2.26519833, 1.88276957,\n",
       "          2.13666662],\n",
       "         [2.1079194 , 2.16755661, 2.15181918, ..., 2.23029334, 2.09304525,\n",
       "          1.946875  ],\n",
       "         [2.07937825, 2.28906239, 1.98054803, ..., 2.16958807, 1.84526448,\n",
       "          1.93136407],\n",
       "         ...,\n",
       "         [1.95421047, 2.29490032, 2.23286486, ..., 2.28543443, 1.74480533,\n",
       "          1.75403774],\n",
       "         [2.36823024, 2.36028267, 2.49247844, ..., 2.24267514, 2.12826469,\n",
       "          2.04238324],\n",
       "         [2.25410299, 2.31286256, 1.85103322, ..., 2.21367414, 1.73804677,\n",
       "          1.15769442]]),\n",
       "  array([[1.75229496, 2.37747944, 1.49989316, ..., 0.99326476, 1.14024448,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.93373219, 1.20265457, ..., 1.04183067, 1.14851083,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.36122218, 1.9087128 , ..., 1.11117826, 1.15436064,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.37456741, 1.78937118, ..., 1.06054999, 1.23414013,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.47449087, 2.09408227, ..., 1.55622024, 1.22456717,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.37103696, 1.63219627, ..., 1.30538952, 1.18236856,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.39996235, 1.71198293, ..., 1.03545622, 1.5237267 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89553111, 1.69597825, ..., 0.95066797, 1.14702919,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.54980834, 1.74070717, ..., 1.44132867, 1.5049025 ,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 1.56151906, ..., 1.06609196, 1.37195886,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.43051353, 1.75463588, ..., 1.22662794, 1.615447  ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.44218916, 1.79308991, ..., 1.01681648, 1.2821139 ,\n",
       "          1.48490784]]),\n",
       "  array([[1.45968794, 2.12168573, 1.55993134, ..., 1.26104249, 2.22547603,\n",
       "          2.04529003],\n",
       "         [1.46707993, 1.91013544, 1.17121492, ..., 1.15597637, 1.96653109,\n",
       "          2.04104433],\n",
       "         [1.46267756, 2.36975273, 1.25124655, ..., 1.16528681, 2.10089196,\n",
       "          2.01304427],\n",
       "         ...,\n",
       "         [1.45117432, 1.98989696, 1.62288298, ..., 1.23455363, 2.03075536,\n",
       "          2.02813959],\n",
       "         [1.40885123, 1.75398044, 1.31189997, ..., 1.13029309, 1.96265557,\n",
       "          1.98850201],\n",
       "         [1.4032937 , 1.52630415, 1.11214232, ..., 1.19791456, 1.93960484,\n",
       "          1.69262193]]),\n",
       "  array([[1.39996235, 2.15235087, 1.80422259, ..., 2.12047266, 1.3514235 ,\n",
       "          1.5237267 ],\n",
       "         [1.89553111, 2.29748687, 1.97417309, ..., 1.94783653, 1.49242114,\n",
       "          1.14702919],\n",
       "         [1.54980834, 1.875294  , 1.79530447, ..., 2.18840583, 1.61395233,\n",
       "          1.5049025 ],\n",
       "         ...,\n",
       "         [1.55607828, 2.08327944, 1.97286336, ..., 2.18417826, 1.84484785,\n",
       "          1.37195886],\n",
       "         [1.43051353, 2.45703142, 1.83905518, ..., 2.20985824, 2.14160989,\n",
       "          1.615447  ],\n",
       "         [1.44218916, 1.59070937, 1.70475003, ..., 2.13763903, 1.29537749,\n",
       "          1.2821139 ]]),\n",
       "  array([[1.73333346, 1.10623661, 2.01093513, ..., 2.12047266, 2.32590365,\n",
       "          1.3514235 ],\n",
       "         [1.99365484, 1.07487982, 2.10504194, ..., 1.94783653, 2.14627341,\n",
       "          1.49242114],\n",
       "         [2.0837893 , 1.08635114, 2.07547407, ..., 2.18840583, 2.2415155 ,\n",
       "          1.61395233],\n",
       "         ...,\n",
       "         [2.00537305, 1.08365046, 2.08812751, ..., 2.18417826, 2.33630074,\n",
       "          1.84484785],\n",
       "         [1.85861944, 1.29061458, 2.02895098, ..., 2.20985824, 2.20065184,\n",
       "          2.14160989],\n",
       "         [1.89998124, 1.07458931, 2.04486911, ..., 2.13763903, 2.2783909 ,\n",
       "          1.29537749]]),\n",
       "  array([[1.75229496, 2.33415201, 1.08762083, ..., 1.8083045 , 1.5237267 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.27873942, 1.16917017, ..., 1.91529173, 1.14702919,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.325405  , 1.23242878, ..., 1.76202332, 1.5049025 ,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.3940365 , 1.1884544 , ..., 1.73356331, 1.37195886,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.27260928, 1.20035822, ..., 1.54590377, 1.615447  ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.42754078, 1.07571416, ..., 1.67241869, 1.2821139 ,\n",
       "          1.48490784]]),\n",
       "  array([[2.15522925, 1.16024544, 1.08717717, ..., 1.06779124, 1.58982198,\n",
       "          1.5741009 ],\n",
       "         [2.17515621, 1.12055417, 1.09854928, ..., 1.2554284 , 1.47373381,\n",
       "          1.15108094],\n",
       "         [2.08048709, 1.10281393, 1.0826282 , ..., 1.2628121 , 1.69020372,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [2.21205177, 1.08853938, 1.11962526, ..., 1.08171167, 1.55419103,\n",
       "          1.4854386 ],\n",
       "         [2.04663546, 1.09058092, 1.16263905, ..., 1.17897114, 1.71183886,\n",
       "          1.69679409],\n",
       "         [2.07725923, 1.1028539 , 1.11087312, ..., 1.04028208, 1.59186903,\n",
       "          1.18713617]]),\n",
       "  array([[1.18495841, 1.04215422, 1.06380828, ..., 1.14605584, 1.61177011,\n",
       "          1.29564013],\n",
       "         [1.12522545, 1.14508459, 1.13090886, ..., 1.12452159, 1.2126352 ,\n",
       "          1.19056253],\n",
       "         [1.19235229, 1.0764457 , 1.09683043, ..., 1.29773726, 1.23892683,\n",
       "          1.17893099],\n",
       "         ...,\n",
       "         [1.12001796, 1.1244454 , 1.20724213, ..., 1.31144586, 1.66982811,\n",
       "          1.2812754 ],\n",
       "         [1.18791186, 1.15757453, 1.18054217, ..., 2.19442152, 1.64448561,\n",
       "          1.47628314],\n",
       "         [1.09212911, 1.16816345, 1.1509892 , ..., 1.15537141, 1.22556603,\n",
       "          1.28541721]]),\n",
       "  array([[2.07023958, 1.977652  , 1.78901639, ..., 1.29564013, 1.8083045 ,\n",
       "          2.13666662],\n",
       "         [2.07630483, 1.75518776, 1.55205892, ..., 1.19056253, 1.91529173,\n",
       "          1.946875  ],\n",
       "         [2.1176614 , 1.71413889, 1.64322633, ..., 1.17893099, 1.76202332,\n",
       "          1.93136407],\n",
       "         ...,\n",
       "         [1.81160523, 1.86167654, 1.71962776, ..., 1.2812754 , 1.73356331,\n",
       "          1.75403774],\n",
       "         [1.54072683, 1.50530405, 1.54759422, ..., 1.47628314, 1.54590377,\n",
       "          2.04238324],\n",
       "         [1.69227599, 1.99507045, 1.48490784, ..., 1.28541721, 1.67241869,\n",
       "          1.15769442]]),\n",
       "  array([[1.01723553, 1.39996235, 0.97826525, ..., 1.10705695, 1.29564013,\n",
       "          1.5237267 ],\n",
       "         [0.98797525, 1.89553111, 1.04477012, ..., 1.11206253, 1.19056253,\n",
       "          1.14702919],\n",
       "         [1.84876577, 1.54980834, 1.00092216, ..., 1.11279532, 1.17893099,\n",
       "          1.5049025 ],\n",
       "         ...,\n",
       "         [1.00916697, 1.55607828, 0.95290886, ..., 1.23917292, 1.2812754 ,\n",
       "          1.37195886],\n",
       "         [2.09600067, 1.43051353, 1.13304869, ..., 1.2291066 , 1.47628314,\n",
       "          1.615447  ],\n",
       "         [1.01669741, 1.44218916, 0.94485519, ..., 1.18170135, 1.28541721,\n",
       "          1.2821139 ]]),\n",
       "  array([[1.05028406, 1.04215422, 1.06380828, ..., 2.04790568, 1.58982198,\n",
       "          1.8083045 ],\n",
       "         [1.20993145, 1.14508459, 1.13090886, ..., 1.38759913, 1.47373381,\n",
       "          1.91529173],\n",
       "         [1.5145703 , 1.0764457 , 1.09683043, ..., 1.78390168, 1.69020372,\n",
       "          1.76202332],\n",
       "         ...,\n",
       "         [1.58652283, 1.1244454 , 1.20724213, ..., 1.84280718, 1.55419103,\n",
       "          1.73356331],\n",
       "         [1.21913485, 1.15757453, 1.18054217, ..., 1.52390521, 1.71183886,\n",
       "          1.54590377],\n",
       "         [1.05212269, 1.16816345, 1.1509892 , ..., 1.42122684, 1.59186903,\n",
       "          1.67241869]]),\n",
       "  array([[1.83306696, 1.05028406, 1.04215422, ..., 1.19348033, 1.23061716,\n",
       "          1.8083045 ],\n",
       "         [1.54515157, 1.20993145, 1.14508459, ..., 1.06242535, 1.18392996,\n",
       "          1.91529173],\n",
       "         [1.75419084, 1.5145703 , 1.0764457 , ..., 1.12527719, 1.55320612,\n",
       "          1.76202332],\n",
       "         ...,\n",
       "         [1.80704448, 1.58652283, 1.1244454 , ..., 1.3172416 , 1.6300149 ,\n",
       "          1.73356331],\n",
       "         [1.79795803, 1.21913485, 1.15757453, ..., 1.49010456, 2.0998435 ,\n",
       "          1.54590377],\n",
       "         [1.59813601, 1.05212269, 1.16816345, ..., 1.10300218, 1.34326663,\n",
       "          1.67241869]]),\n",
       "  array([[1.75229496, 1.05028406, 1.977652  , ..., 1.03545622, 1.8083045 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.20993145, 1.75518776, ..., 0.95066797, 1.91529173,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.5145703 , 1.71413889, ..., 1.44132867, 1.76202332,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.58652283, 1.86167654, ..., 1.06609196, 1.73356331,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.21913485, 1.50530405, ..., 1.22662794, 1.54590377,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.05212269, 1.99507045, ..., 1.01681648, 1.67241869,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.07023958, 2.15522925, ..., 2.04790568, 1.8083045 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.07630483, 2.17515621, ..., 1.38759913, 1.91529173,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.1176614 , 2.08048709, ..., 1.78390168, 1.76202332,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.81160523, 2.21205177, ..., 1.84280718, 1.73356331,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54072683, 2.04663546, ..., 1.52390521, 1.54590377,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.69227599, 2.07725923, ..., 1.42122684, 1.67241869,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.07023958, 1.977652  , ..., 1.53684919, 1.29564013,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.07630483, 1.75518776, ..., 1.56805445, 1.19056253,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.1176614 , 1.71413889, ..., 1.52818699, 1.17893099,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.81160523, 1.86167654, ..., 1.68177271, 1.2812754 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54072683, 1.50530405, ..., 1.82258337, 1.47628314,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.69227599, 1.99507045, ..., 1.76719724, 1.28541721,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.65627031, 2.01763004, ..., 1.08952894, 2.19371008,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.19599597, 1.86195225, ..., 1.60191483, 2.36697793,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.5639548 , 1.81472394, ..., 1.09056375, 2.29426944,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.65838808, 1.91405209, ..., 1.24003624, 2.21601682,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.5192014 , 2.0209193 , ..., 1.12711056, 2.04581025,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.37067187, 1.89884798, ..., 1.61914922, 2.2238089 ,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.01723553, 0.97826525, ..., 1.03545622, 1.5237267 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 0.98797525, 1.04477012, ..., 0.95066797, 1.14702919,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.84876577, 1.00092216, ..., 1.44132867, 1.5049025 ,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.00916697, 0.95290886, ..., 1.06609196, 1.37195886,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.09600067, 1.13304869, ..., 1.22662794, 1.615447  ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.01669741, 0.94485519, ..., 1.01681648, 1.2821139 ,\n",
       "          1.48490784]]),\n",
       "  array([[2.37747944, 1.18483994, 1.03359049, ..., 2.24712288, 2.20439914,\n",
       "          1.78901639],\n",
       "         [1.93373219, 1.08815167, 1.13648295, ..., 2.24861519, 2.12523809,\n",
       "          1.55205892],\n",
       "         [2.36122218, 1.0221904 , 1.0504743 , ..., 2.30371731, 2.07647221,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [2.37456741, 1.02853296, 1.08567966, ..., 2.09356642, 2.20350442,\n",
       "          1.71962776],\n",
       "         [2.47449087, 1.0930099 , 2.0175601 , ..., 1.93854034, 2.29747139,\n",
       "          1.54759422],\n",
       "         [2.37103696, 1.04165874, 1.34414864, ..., 2.12379041, 1.91581996,\n",
       "          1.48490784]]),\n",
       "  array([[1.06380828, 1.19022403, 1.04165623, ..., 1.07250481, 1.42125736,\n",
       "          1.01768415],\n",
       "         [1.13090886, 1.22756934, 1.06749727, ..., 1.20298206, 1.40797226,\n",
       "          1.05059697],\n",
       "         [1.09683043, 1.17588551, 1.07442078, ..., 1.04153746, 1.29282335,\n",
       "          1.00229839],\n",
       "         ...,\n",
       "         [1.20724213, 1.20362541, 1.08262627, ..., 1.07322001, 1.39333885,\n",
       "          0.96264157],\n",
       "         [1.18054217, 1.20999529, 1.11883243, ..., 1.0921432 , 1.47707839,\n",
       "          0.99464783],\n",
       "         [1.1509892 , 1.17026436, 1.07721099, ..., 1.19826109, 1.51482943,\n",
       "          0.99952414]]),\n",
       "  array([[1.41336423, 1.51062881, 1.1599679 , ..., 1.98387903, 1.55613208,\n",
       "          1.19459356],\n",
       "         [1.68426866, 1.57174699, 1.11420456, ..., 1.96185081, 1.2340156 ,\n",
       "          1.3450805 ],\n",
       "         [2.42950053, 1.65440876, 1.08862929, ..., 2.04250882, 1.47221644,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.92345199, 1.15214069, 1.14351416, ..., 2.19016088, 1.58732565,\n",
       "          1.51860873],\n",
       "         [1.49501566, 1.54446902, 1.11532477, ..., 2.0618537 , 1.76861528,\n",
       "          1.49380526],\n",
       "         [1.14389835, 1.70415854, 2.36110655, ..., 1.88687232, 1.71795398,\n",
       "          1.27581726]]),\n",
       "  array([[2.41638667, 1.51062881, 1.99649867, ..., 1.83325358, 1.19459356,\n",
       "          1.08694212],\n",
       "         [2.33133613, 1.57174699, 1.90977531, ..., 1.89029038, 1.3450805 ,\n",
       "          1.04285192],\n",
       "         [2.34599987, 1.65440876, 1.93585487, ..., 1.12736784, 1.45791797,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [2.27373943, 1.15214069, 2.05054898, ..., 1.10130385, 1.51860873,\n",
       "          1.05734956],\n",
       "         [2.24387755, 1.54446902, 2.10109398, ..., 1.15497121, 1.49380526,\n",
       "          1.23543696],\n",
       "         [2.31855632, 1.70415854, 2.13190706, ..., 1.87689646, 1.27581726,\n",
       "          1.32472077]]),\n",
       "  array([[1.51062881, 2.03072632, 1.18764292, ..., 1.55613208, 1.14849649,\n",
       "          1.8083045 ],\n",
       "         [1.57174699, 1.91924292, 1.32987571, ..., 1.2340156 , 1.30043806,\n",
       "          1.91529173],\n",
       "         [1.65440876, 2.14164865, 1.13700495, ..., 1.47221644, 1.09767988,\n",
       "          1.76202332],\n",
       "         ...,\n",
       "         [1.15214069, 2.06130454, 1.2786727 , ..., 1.58732565, 1.24554355,\n",
       "          1.73356331],\n",
       "         [1.54446902, 1.95126493, 1.37726343, ..., 1.76861528, 1.19311427,\n",
       "          1.54590377],\n",
       "         [1.70415854, 2.01616974, 1.13655219, ..., 1.71795398, 1.56185083,\n",
       "          1.67241869]]),\n",
       "  array([[1.39325972, 1.82035548, 1.17863288, ..., 1.15109416, 2.00739809,\n",
       "          1.38212077],\n",
       "         [1.53633739, 1.99570181, 1.13261297, ..., 1.25629429, 2.01773587,\n",
       "          1.60008364],\n",
       "         [1.58137553, 1.62510335, 1.16457548, ..., 1.12723582, 1.90533954,\n",
       "          1.76791006],\n",
       "         ...,\n",
       "         [1.44366747, 1.80182572, 1.25384151, ..., 1.03551089, 2.20607203,\n",
       "          1.55127084],\n",
       "         [1.45209376, 1.76148764, 1.19950297, ..., 1.1586131 , 1.81117512,\n",
       "          1.5577182 ],\n",
       "         [1.77354015, 1.36287659, 1.4654671 , ..., 1.86807723, 2.07971162,\n",
       "          1.60967029]]),\n",
       "  array([[1.75229496, 1.39996235, 1.24687   , ..., 1.98499197, 2.3907403 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89553111, 1.11213476, ..., 1.98302685, 2.43921648,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.54980834, 1.29013839, ..., 1.96123021, 2.26384321,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 1.14842401, ..., 2.19983661, 2.27510704,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.43051353, 1.18381704, ..., 2.13323445, 2.13503381,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.44218916, 1.50308824, ..., 2.02809005, 2.40942918,\n",
       "          1.48490784]]),\n",
       "  array([[1.41336423, 1.51062881, 2.03072632, ..., 1.55613208, 1.92130046,\n",
       "          1.19459356],\n",
       "         [1.68426866, 1.57174699, 1.91924292, ..., 1.2340156 , 1.1691627 ,\n",
       "          1.3450805 ],\n",
       "         [2.42950053, 1.65440876, 2.14164865, ..., 1.47221644, 1.75335675,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.92345199, 1.15214069, 2.06130454, ..., 1.58732565, 2.03451502,\n",
       "          1.51860873],\n",
       "         [1.49501566, 1.54446902, 1.95126493, ..., 1.76861528, 1.96402303,\n",
       "          1.49380526],\n",
       "         [1.14389835, 1.70415854, 2.01616974, ..., 1.71795398, 1.50830269,\n",
       "          1.27581726]]),\n",
       "  array([[1.41336423, 1.51062881, 2.03072632, ..., 1.8083045 , 1.19459356,\n",
       "          1.26738841],\n",
       "         [1.68426866, 1.57174699, 1.91924292, ..., 1.91529173, 1.3450805 ,\n",
       "          1.4240069 ],\n",
       "         [2.42950053, 1.65440876, 2.14164865, ..., 1.76202332, 1.45791797,\n",
       "          1.72632829],\n",
       "         ...,\n",
       "         [1.92345199, 1.15214069, 2.06130454, ..., 1.73356331, 1.51860873,\n",
       "          1.40738742],\n",
       "         [1.49501566, 1.54446902, 1.95126493, ..., 1.54590377, 1.49380526,\n",
       "          1.42122894],\n",
       "         [1.14389835, 1.70415854, 2.01616974, ..., 1.67241869, 1.27581726,\n",
       "          1.39889214]]),\n",
       "  array([[1.75229496, 1.41336423, 1.51062881, ..., 1.14296902, 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.68426866, 1.57174699, ..., 1.33679255, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.42950053, 1.65440876, ..., 1.09788172, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.92345199, 1.15214069, ..., 1.13776947, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.49501566, 1.54446902, ..., 1.30306169, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.14389835, 1.70415854, ..., 1.12345215, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.41336423, 1.51062881, 1.06264921, ..., 1.12526225, 1.55613208,\n",
       "          1.15159097],\n",
       "         [1.68426866, 1.57174699, 1.03022189, ..., 1.11808438, 1.2340156 ,\n",
       "          1.1048707 ],\n",
       "         [2.42950053, 1.65440876, 1.0438173 , ..., 1.46013017, 1.47221644,\n",
       "          1.05367355],\n",
       "         ...,\n",
       "         [1.92345199, 1.15214069, 1.00962336, ..., 1.08328009, 1.58732565,\n",
       "          1.11214816],\n",
       "         [1.49501566, 1.54446902, 1.22898465, ..., 1.11578666, 1.76861528,\n",
       "          0.94702493],\n",
       "         [1.14389835, 1.70415854, 1.78240577, ..., 1.26741483, 1.71795398,\n",
       "          1.05412739]]),\n",
       "  array([[1.41336423, 1.51062881, 1.74746014, ..., 1.96100764, 1.19459356,\n",
       "          1.26738841],\n",
       "         [1.68426866, 1.57174699, 1.31991667, ..., 1.48751004, 1.3450805 ,\n",
       "          1.4240069 ],\n",
       "         [2.42950053, 1.65440876, 1.19854712, ..., 1.64690799, 1.45791797,\n",
       "          1.72632829],\n",
       "         ...,\n",
       "         [1.92345199, 1.15214069, 1.73283421, ..., 1.52760336, 1.51860873,\n",
       "          1.40738742],\n",
       "         [1.49501566, 1.54446902, 1.76626355, ..., 1.80614854, 1.49380526,\n",
       "          1.42122894],\n",
       "         [1.14389835, 1.70415854, 2.19951089, ..., 1.34941361, 1.27581726,\n",
       "          1.39889214]]),\n",
       "  array([[2.26293751, 1.25307349, 1.75229496, ..., 2.20085208, 1.55613208,\n",
       "          1.32641115],\n",
       "         [2.27719867, 1.80478644, 1.3757134 , ..., 2.06536193, 1.2340156 ,\n",
       "          1.12950246],\n",
       "         [2.18916216, 1.34397588, 1.04714897, ..., 2.07049043, 1.47221644,\n",
       "          1.06045605],\n",
       "         ...,\n",
       "         [2.24016328, 2.16205518, 1.77179139, ..., 2.09243967, 1.58732565,\n",
       "          1.239142  ],\n",
       "         [2.15078532, 1.51954481, 1.24833316, ..., 2.00401224, 1.76861528,\n",
       "          1.11277916],\n",
       "         [2.20882803, 2.00835726, 1.19976203, ..., 1.83582578, 1.71795398,\n",
       "          1.4850472 ]]),\n",
       "  array([[2.03072632, 1.10989361, 1.18764292, ..., 2.20984614, 2.23730752,\n",
       "          1.55613208],\n",
       "         [1.91924292, 1.09137139, 1.32987571, ..., 2.07298514, 2.22139365,\n",
       "          1.2340156 ],\n",
       "         [2.14164865, 1.06849566, 1.13700495, ..., 2.10390786, 2.31311132,\n",
       "          1.47221644],\n",
       "         ...,\n",
       "         [2.06130454, 1.14183599, 1.2786727 , ..., 2.11464467, 2.3973848 ,\n",
       "          1.58732565],\n",
       "         [1.95126493, 1.13532906, 1.37726343, ..., 2.03583489, 2.28561993,\n",
       "          1.76861528],\n",
       "         [2.01616974, 1.10867497, 1.13655219, ..., 2.16569734, 2.28151429,\n",
       "          1.71795398]]),\n",
       "  array([[1.51062881, 1.62837833, 1.20473838, ..., 1.79074408, 1.92130562,\n",
       "          1.50203423],\n",
       "         [1.57174699, 1.75939077, 1.31855498, ..., 1.03024426, 1.25275384,\n",
       "          1.66397409],\n",
       "         [1.65440876, 1.77173745, 1.46635053, ..., 1.05724272, 2.11043837,\n",
       "          1.68141122],\n",
       "         ...,\n",
       "         [1.15214069, 1.69966224, 1.26731463, ..., 1.08217047, 1.20473398,\n",
       "          1.38984911],\n",
       "         [1.54446902, 2.28354002, 1.31079558, ..., 1.13022137, 1.6331565 ,\n",
       "          1.35224472],\n",
       "         [1.70415854, 2.08153425, 1.31339973, ..., 1.10956136, 1.87996151,\n",
       "          1.50102985]]),\n",
       "  array([[0.97927129, 0.96347193, 1.18787601, ..., 1.09708021, 1.07141427,\n",
       "          1.79074408],\n",
       "         [0.99257252, 1.27792232, 1.15815066, ..., 1.61383387, 1.29489502,\n",
       "          1.03024426],\n",
       "         [1.04698132, 1.30557762, 1.52353962, ..., 1.37949231, 1.05977403,\n",
       "          1.05724272],\n",
       "         ...,\n",
       "         [0.97351998, 1.12623223, 1.19131165, ..., 2.06372443, 1.88820745,\n",
       "          1.08217047],\n",
       "         [1.07047343, 1.12362018, 1.33589465, ..., 1.31033933, 1.11104797,\n",
       "          1.13022137],\n",
       "         [1.29188693, 1.59402852, 1.13381455, ..., 2.00705641, 1.41278799,\n",
       "          1.10956136]]),\n",
       "  array([[1.51062881, 1.05028406, 1.06863806, ..., 1.55613208, 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.57174699, 1.20993145, 1.29288865, ..., 1.2340156 , 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.65440876, 1.5145703 , 1.2823741 , ..., 1.47221644, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.15214069, 1.58652283, 1.10498174, ..., 1.58732565, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.54446902, 1.21913485, 1.39665407, ..., 1.76861528, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.70415854, 1.05212269, 1.12246283, ..., 1.71795398, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[2.07023958, 1.08195741, 1.08974358, ..., 1.96432369, 1.61177011,\n",
       "          1.5741009 ],\n",
       "         [2.07630483, 1.06325793, 1.51074991, ..., 1.04625723, 1.2126352 ,\n",
       "          1.15108094],\n",
       "         [2.1176614 , 1.81169324, 1.53020264, ..., 1.05699407, 1.23892683,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.81160523, 1.43157496, 1.44092493, ..., 1.24578735, 1.66982811,\n",
       "          1.4854386 ],\n",
       "         [1.54072683, 1.18184826, 1.16093991, ..., 1.07372318, 1.64448561,\n",
       "          1.69679409],\n",
       "         [1.69227599, 1.05159775, 1.14836264, ..., 1.10060965, 1.22556603,\n",
       "          1.18713617]]),\n",
       "  array([[1.75229496, 1.51062881, 1.78901639, ..., 2.06504365, 1.99677152,\n",
       "          1.08694212],\n",
       "         [1.3757134 , 1.57174699, 1.55205892, ..., 1.93006118, 1.84218341,\n",
       "          1.04285192],\n",
       "         [1.04714897, 1.65440876, 1.64322633, ..., 1.89327658, 1.65890547,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [1.77179139, 1.15214069, 1.71962776, ..., 1.94022386, 1.94578817,\n",
       "          1.05734956],\n",
       "         [1.24833316, 1.54446902, 1.54759422, ..., 2.00570213, 2.03992909,\n",
       "          1.23543696],\n",
       "         [1.19976203, 1.70415854, 1.48490784, ..., 1.80439427, 1.26557088,\n",
       "          1.32472077]]),\n",
       "  array([[1.51062881, 1.93468875, 1.1599679 , ..., 1.94471567, 1.04757781,\n",
       "          1.19459356],\n",
       "         [1.57174699, 2.16747765, 1.11420456, ..., 1.68773335, 1.35970496,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 1.0341381 , 1.08862929, ..., 1.82880119, 1.02714078,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 1.58847379, 1.14351416, ..., 2.09875452, 1.08221015,\n",
       "          1.51860873],\n",
       "         [1.54446902, 0.99569603, 1.11532477, ..., 2.10507291, 1.16527846,\n",
       "          1.49380526],\n",
       "         [1.70415854, 2.17487766, 2.36110655, ..., 1.560967  , 1.19797091,\n",
       "          1.27581726]]),\n",
       "  array([[1.51062881, 2.03072632, 2.15522925, ..., 1.98387903, 1.5741009 ,\n",
       "          1.19459356],\n",
       "         [1.57174699, 1.91924292, 2.17515621, ..., 1.96185081, 1.15108094,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 2.14164865, 2.08048709, ..., 2.04250882, 1.25651027,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 2.06130454, 2.21205177, ..., 2.19016088, 1.4854386 ,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.95126493, 2.04663546, ..., 2.0618537 , 1.69679409,\n",
       "          1.49380526],\n",
       "         [1.70415854, 2.01616974, 2.07725923, ..., 1.88687232, 1.18713617,\n",
       "          1.27581726]]),\n",
       "  array([[1.51062881, 1.10989361, 1.55015312, ..., 1.98387903, 1.06795591,\n",
       "          1.19459356],\n",
       "         [1.57174699, 1.09137139, 1.18963529, ..., 1.96185081, 1.28867186,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 1.06849566, 1.03264914, ..., 2.04250882, 1.49700981,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 1.14183599, 1.09606095, ..., 2.19016088, 1.41981627,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.13532906, 1.27628399, ..., 2.0618537 , 1.18034174,\n",
       "          1.49380526],\n",
       "         [1.70415854, 1.10867497, 1.24444887, ..., 1.88687232, 1.2293912 ,\n",
       "          1.27581726]]),\n",
       "  array([[2.11122884, 1.75229496, 1.51062881, ..., 2.31046349, 1.19459356,\n",
       "          1.78901639],\n",
       "         [2.07640129, 1.3757134 , 1.57174699, ..., 2.25997568, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.91512371, 1.04714897, 1.65440876, ..., 2.19317734, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [2.17627526, 1.77179139, 1.15214069, ..., 2.32994071, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.99907342, 1.24833316, 1.54446902, ..., 2.21726404, 1.49380526,\n",
       "          1.54759422],\n",
       "         [2.01909049, 1.19976203, 1.70415854, ..., 2.14824811, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.11370757, 1.51062881, ..., 1.98387903, 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89869739, 1.57174699, ..., 1.96185081, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.04484497, 1.65440876, ..., 2.04250882, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.1752741 , 1.15214069, ..., 2.19016088, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.07798912, 1.54446902, ..., 2.0618537 , 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.07220472, 1.70415854, ..., 1.88687232, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.17783431, 1.51062881, 2.03072632, ..., 1.08839286, 1.98387903,\n",
       "          1.19459356],\n",
       "         [1.28991984, 1.57174699, 1.91924292, ..., 1.04912533, 1.96185081,\n",
       "          1.3450805 ],\n",
       "         [1.18315206, 1.65440876, 2.14164865, ..., 1.09307187, 2.04250882,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.10423895, 1.15214069, 2.06130454, ..., 1.01487412, 2.19016088,\n",
       "          1.51860873],\n",
       "         [1.15875132, 1.54446902, 1.95126493, ..., 1.09551702, 2.0618537 ,\n",
       "          1.49380526],\n",
       "         [1.69190964, 1.70415854, 2.01616974, ..., 0.95828499, 1.88687232,\n",
       "          1.27581726]]),\n",
       "  array([[1.75229496, 1.03366397, 2.06072626, ..., 1.06779124, 1.98499197,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.06184392, 1.86050822, ..., 1.2554284 , 1.98302685,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.07328493, 1.83855123, ..., 1.2628121 , 1.96123021,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.03133719, 1.88597731, ..., 1.08171167, 2.19983661,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.78497359, 2.09805395, ..., 1.17897114, 2.13323445,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.08133384, 2.24973533, ..., 1.04028208, 2.02809005,\n",
       "          1.48490784]]),\n",
       "  array([[1.10938801, 1.51062881, 2.03072632, ..., 1.98387903, 1.55613208,\n",
       "          2.45441246],\n",
       "         [1.17109744, 1.57174699, 1.91924292, ..., 1.96185081, 1.2340156 ,\n",
       "          2.41942682],\n",
       "         [1.23674017, 1.65440876, 2.14164865, ..., 2.04250882, 1.47221644,\n",
       "          2.22201138],\n",
       "         ...,\n",
       "         [1.11741033, 1.15214069, 2.06130454, ..., 2.19016088, 1.58732565,\n",
       "          2.39038711],\n",
       "         [1.33189579, 1.54446902, 1.95126493, ..., 2.0618537 , 1.76861528,\n",
       "          2.22908602],\n",
       "         [2.2472187 , 1.70415854, 2.01616974, ..., 1.88687232, 1.71795398,\n",
       "          2.23055028]]),\n",
       "  array([[1.75229496, 2.17435502, 1.51062881, ..., 1.40958767, 1.27299853,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.2952327 , 1.57174699, ..., 1.53621583, 1.61536378,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.12248882, 1.65440876, ..., 1.81532569, 1.38237962,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55681602, 1.15214069, ..., 1.30157956, 1.78323908,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.74273618, 1.54446902, ..., 1.73049738, 1.52637877,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.05375208, 1.70415854, ..., 1.34349737, 1.60477037,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.05028406, 1.01715023, ..., 1.49634981, 1.83325358,\n",
       "          1.08694212],\n",
       "         [1.3757134 , 1.20993145, 0.99107101, ..., 1.15081784, 1.89029038,\n",
       "          1.04285192],\n",
       "         [1.04714897, 1.5145703 , 0.95565293, ..., 1.12599499, 1.12736784,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [1.77179139, 1.58652283, 1.05067402, ..., 1.33319748, 1.10130385,\n",
       "          1.05734956],\n",
       "         [1.24833316, 1.21913485, 1.03234483, ..., 1.29303003, 1.15497121,\n",
       "          1.23543696],\n",
       "         [1.19976203, 1.05212269, 1.67732851, ..., 1.2917917 , 1.87689646,\n",
       "          1.32472077]]),\n",
       "  array([[1.51062881, 1.97673403, 1.1440921 , ..., 1.5741009 , 1.14296902,\n",
       "          1.19459356],\n",
       "         [1.57174699, 2.10007106, 1.58528733, ..., 1.15108094, 1.33679255,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 2.05342842, 1.52369828, ..., 1.25651027, 1.09788172,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 2.03591201, 1.38768239, ..., 1.4854386 , 1.13776947,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.94561621, 1.15724596, ..., 1.69679409, 1.30306169,\n",
       "          1.49380526],\n",
       "         [1.70415854, 1.97677302, 1.33332441, ..., 1.18713617, 1.12345215,\n",
       "          1.27581726]]),\n",
       "  array([[1.75229496, 1.51062881, 2.03072632, ..., 1.8083045 , 1.83325358,\n",
       "          1.08694212],\n",
       "         [1.3757134 , 1.57174699, 1.91924292, ..., 1.91529173, 1.89029038,\n",
       "          1.04285192],\n",
       "         [1.04714897, 1.65440876, 2.14164865, ..., 1.76202332, 1.12736784,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [1.77179139, 1.15214069, 2.06130454, ..., 1.73356331, 1.10130385,\n",
       "          1.05734956],\n",
       "         [1.24833316, 1.54446902, 1.95126493, ..., 1.54590377, 1.15497121,\n",
       "          1.23543696],\n",
       "         [1.19976203, 1.70415854, 2.01616974, ..., 1.67241869, 1.87689646,\n",
       "          1.32472077]]),\n",
       "  array([[1.75229496, 1.51062881, 1.80787344, ..., 1.24601416, 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.57174699, 1.21795203, ..., 1.14152476, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.65440876, 1.19237173, ..., 1.0958254 , 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.15214069, 1.8295111 , ..., 1.23600913, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54446902, 1.86271482, ..., 1.22232164, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.70415854, 1.6612087 , ..., 1.1703797 , 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.51062881, 1.86088903, 1.1599679 , ..., 1.24601416, 1.55613208,\n",
       "          1.8083045 ],\n",
       "         [1.57174699, 1.17765548, 1.11420456, ..., 1.14152476, 1.2340156 ,\n",
       "          1.91529173],\n",
       "         [1.65440876, 1.03528999, 1.08862929, ..., 1.0958254 , 1.47221644,\n",
       "          1.76202332],\n",
       "         ...,\n",
       "         [1.15214069, 1.03280899, 1.14351416, ..., 1.23600913, 1.58732565,\n",
       "          1.73356331],\n",
       "         [1.54446902, 1.11352635, 1.11532477, ..., 1.22232164, 1.76861528,\n",
       "          1.54590377],\n",
       "         [1.70415854, 1.00194326, 2.36110655, ..., 1.1703797 , 1.71795398,\n",
       "          1.67241869]]),\n",
       "  array([[1.51062881, 1.86088903, 2.03072632, ..., 1.55613208, 1.04133847,\n",
       "          1.19459356],\n",
       "         [1.57174699, 1.17765548, 1.91924292, ..., 1.2340156 , 1.03836256,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 1.03528999, 2.14164865, ..., 1.47221644, 1.43838451,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 1.03280899, 2.06130454, ..., 1.58732565, 1.19276843,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.11352635, 1.95126493, ..., 1.76861528, 1.10396782,\n",
       "          1.49380526],\n",
       "         [1.70415854, 1.00194326, 2.01616974, ..., 1.71795398, 0.96848222,\n",
       "          1.27581726]]),\n",
       "  array([[1.75229496, 1.41336423, 1.51062881, ..., 1.48379169, 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.68426866, 1.57174699, ..., 1.28029546, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.42950053, 1.65440876, ..., 1.21390889, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.92345199, 1.15214069, ..., 1.43262181, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.49501566, 1.54446902, ..., 1.59747481, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.14389835, 1.70415854, ..., 1.23196294, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[1.51062881, 1.86088903, 2.03072632, ..., 1.55613208, 1.04133847,\n",
       "          1.19459356],\n",
       "         [1.57174699, 1.17765548, 1.91924292, ..., 1.2340156 , 1.03836256,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 1.03528999, 2.14164865, ..., 1.47221644, 1.43838451,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 1.03280899, 2.06130454, ..., 1.58732565, 1.19276843,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.11352635, 1.95126493, ..., 1.76861528, 1.10396782,\n",
       "          1.49380526],\n",
       "         [1.70415854, 1.00194326, 2.01616974, ..., 1.71795398, 0.96848222,\n",
       "          1.27581726]]),\n",
       "  array([[1.51062881, 1.24687   , 2.03072632, ..., 1.52011035, 1.1457041 ,\n",
       "          1.19459356],\n",
       "         [1.57174699, 1.11213476, 1.91924292, ..., 1.37984619, 1.09707551,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 1.29013839, 2.14164865, ..., 1.06136607, 1.09556913,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 1.14842401, 2.06130454, ..., 1.85608147, 1.36577254,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.18381704, 1.95126493, ..., 1.54690709, 1.55100081,\n",
       "          1.49380526],\n",
       "         [1.70415854, 1.50308824, 2.01616974, ..., 1.12281309, 1.73232919,\n",
       "          1.27581726]]),\n",
       "  array([[1.75229496, 2.03072632, 1.18764292, ..., 1.04757781, 1.15159097,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.91924292, 1.32987571, ..., 1.35970496, 1.1048707 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.14164865, 1.13700495, ..., 1.02714078, 1.05367355,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.06130454, 1.2786727 , ..., 1.08221015, 1.11214816,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.95126493, 1.37726343, ..., 1.16527846, 0.94702493,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.01616974, 1.13655219, ..., 1.19797091, 1.05412739,\n",
       "          1.48490784]]),\n",
       "  array([[1.52943155, 1.38679328, 1.94288851, ..., 1.539593  , 1.99677152,\n",
       "          1.78901639],\n",
       "         [1.43597731, 1.14739199, 2.02746656, ..., 1.45516453, 1.84218341,\n",
       "          1.55205892],\n",
       "         [1.35667106, 2.04588506, 1.7934004 , ..., 1.74237871, 1.65890547,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.72984076, 1.48629356, 1.97623279, ..., 1.51864113, 1.94578817,\n",
       "          1.71962776],\n",
       "         [1.60904102, 1.33536065, 1.87010299, ..., 1.58839872, 2.03992909,\n",
       "          1.54759422],\n",
       "         [1.29008272, 1.42731638, 1.8490312 , ..., 1.776608  , 1.26557088,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.68974475, 1.05028406, ..., 1.94471567, 1.04133847,\n",
       "          1.08694212],\n",
       "         [1.3757134 , 1.75974474, 1.20993145, ..., 1.68773335, 1.03836256,\n",
       "          1.04285192],\n",
       "         [1.04714897, 1.32240641, 1.5145703 , ..., 1.82880119, 1.43838451,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [1.77179139, 1.4670868 , 1.58652283, ..., 2.09875452, 1.19276843,\n",
       "          1.05734956],\n",
       "         [1.24833316, 1.90465447, 1.21913485, ..., 2.10507291, 1.10396782,\n",
       "          1.23543696],\n",
       "         [1.19976203, 1.27428454, 1.05212269, ..., 1.560967  , 0.96848222,\n",
       "          1.32472077]]),\n",
       "  array([[1.75229496, 1.52943155, 1.94288851, ..., 1.14605584, 1.5237267 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.43597731, 2.02746656, ..., 1.12452159, 1.14702919,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.35667106, 1.7934004 , ..., 1.29773726, 1.5049025 ,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.72984076, 1.97623279, ..., 1.31144586, 1.37195886,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.60904102, 1.87010299, ..., 2.19442152, 1.615447  ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.29008272, 1.8490312 , ..., 1.15537141, 1.2821139 ,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.18483994, 1.52132349, ..., 1.23061716, 1.27299853,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.08815167, 1.24583779, ..., 1.18392996, 1.61536378,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.0221904 , 1.12818241, ..., 1.55320612, 1.38237962,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.02853296, 1.12005351, ..., 1.6300149 , 1.78323908,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.0930099 , 1.1110386 , ..., 2.0998435 , 1.52637877,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.04165874, 1.07273014, ..., 1.34326663, 1.60477037,\n",
       "          1.48490784]]),\n",
       "  array([[1.86088903, 2.26307905, 2.10993236, ..., 1.91405056, 1.04133847,\n",
       "          1.5741009 ],\n",
       "         [1.17765548, 2.08614175, 2.02246393, ..., 2.05467047, 1.03836256,\n",
       "          1.15108094],\n",
       "         [1.03528999, 2.11737687, 2.14272867, ..., 1.87148816, 1.43838451,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.03280899, 1.96585764, 1.97934639, ..., 1.85970597, 1.19276843,\n",
       "          1.4854386 ],\n",
       "         [1.11352635, 1.88999768, 2.16833488, ..., 1.91177727, 1.10396782,\n",
       "          1.69679409],\n",
       "         [1.00194326, 1.01021402, 2.09540949, ..., 1.80675301, 0.96848222,\n",
       "          1.18713617]]),\n",
       "  array([[1.75229496, 1.51062881, 2.03072632, ..., 2.02624791, 1.19459356,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.57174699, 1.91924292, ..., 1.30848135, 1.3450805 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.65440876, 2.14164865, ..., 2.16057646, 1.45791797,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.15214069, 2.06130454, ..., 2.22866815, 1.51860873,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54446902, 1.95126493, ..., 2.24639164, 1.49380526,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.70415854, 2.01616974, ..., 1.28534949, 1.27581726,\n",
       "          1.48490784]]),\n",
       "  array([[2.08489635, 1.06380828, 1.37788878, ..., 1.01768415, 1.14605584,\n",
       "          2.13666662],\n",
       "         [2.15181918, 1.13090886, 1.78273871, ..., 1.05059697, 1.12452159,\n",
       "          1.946875  ],\n",
       "         [1.98054803, 1.09683043, 1.07327964, ..., 1.00229839, 1.29773726,\n",
       "          1.93136407],\n",
       "         ...,\n",
       "         [2.23286486, 1.20724213, 1.61602188, ..., 0.96264157, 1.31144586,\n",
       "          1.75403774],\n",
       "         [2.49247844, 1.18054217, 2.15728805, ..., 0.99464783, 2.19442152,\n",
       "          2.04238324],\n",
       "         [1.85103322, 1.1509892 , 2.42703326, ..., 0.99952414, 1.15537141,\n",
       "          1.15769442]]),\n",
       "  array([[1.07099287, 2.10232303, 1.90270651, ..., 1.22380612, 1.539593  ,\n",
       "          1.78901639],\n",
       "         [1.23144495, 1.21555877, 1.92911   , ..., 1.63281025, 1.45516453,\n",
       "          1.55205892],\n",
       "         [1.10537173, 1.39600557, 1.8550356 , ..., 1.33880426, 1.74237871,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.09984036, 2.02200999, 1.94864547, ..., 1.2121356 , 1.51864113,\n",
       "          1.71962776],\n",
       "         [1.14564666, 1.85423393, 2.04030421, ..., 1.22405585, 1.58839872,\n",
       "          1.54759422],\n",
       "         [1.11603741, 1.17729895, 1.86779627, ..., 1.38333943, 1.776608  ,\n",
       "          1.48490784]]),\n",
       "  array([[1.51062881, 2.03072632, 1.3074791 , ..., 1.45123111, 1.34695345,\n",
       "          1.72448766],\n",
       "         [1.57174699, 1.91924292, 1.06153661, ..., 1.27462749, 1.3987962 ,\n",
       "          2.01278609],\n",
       "         [1.65440876, 2.14164865, 2.22580808, ..., 1.45647973, 1.31753669,\n",
       "          1.92109708],\n",
       "         ...,\n",
       "         [1.15214069, 2.06130454, 1.07217011, ..., 1.26105576, 1.19636562,\n",
       "          2.00279338],\n",
       "         [1.54446902, 1.95126493, 1.32527926, ..., 1.43273563, 1.20119039,\n",
       "          1.8307111 ],\n",
       "         [1.70415854, 2.01616974, 1.31419629, ..., 1.13884269, 1.56045284,\n",
       "          1.90148515]]),\n",
       "  array([[1.51062881, 2.03072632, 1.80787344, ..., 1.24601416, 1.19459356,\n",
       "          1.06136486],\n",
       "         [1.57174699, 1.91924292, 1.21795203, ..., 1.14152476, 1.3450805 ,\n",
       "          1.05667998],\n",
       "         [1.65440876, 2.14164865, 1.19237173, ..., 1.0958254 , 1.45791797,\n",
       "          1.02757615],\n",
       "         ...,\n",
       "         [1.15214069, 2.06130454, 1.8295111 , ..., 1.23600913, 1.51860873,\n",
       "          1.02489204],\n",
       "         [1.54446902, 1.95126493, 1.86271482, ..., 1.22232164, 1.49380526,\n",
       "          1.61415986],\n",
       "         [1.70415854, 2.01616974, 1.6612087 , ..., 1.1703797 , 1.27581726,\n",
       "          1.00200648]]),\n",
       "  array([[1.51062881, 2.03072632, 1.18764292, ..., 1.08839286, 1.55613208,\n",
       "          1.19459356],\n",
       "         [1.57174699, 1.91924292, 1.32987571, ..., 1.04912533, 1.2340156 ,\n",
       "          1.3450805 ],\n",
       "         [1.65440876, 2.14164865, 1.13700495, ..., 1.09307187, 1.47221644,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.15214069, 2.06130454, 1.2786727 , ..., 1.01487412, 1.58732565,\n",
       "          1.51860873],\n",
       "         [1.54446902, 1.95126493, 1.37726343, ..., 1.09551702, 1.76861528,\n",
       "          1.49380526],\n",
       "         [1.70415854, 2.01616974, 1.13655219, ..., 0.95828499, 1.71795398,\n",
       "          1.27581726]]),\n",
       "  array([[2.03072632, 0.9794307 , 1.18764292, ..., 1.98387903, 1.55613208,\n",
       "          1.19459356],\n",
       "         [1.91924292, 1.08110873, 1.32987571, ..., 1.96185081, 1.2340156 ,\n",
       "          1.3450805 ],\n",
       "         [2.14164865, 1.06248278, 1.13700495, ..., 2.04250882, 1.47221644,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [2.06130454, 1.23793578, 1.2786727 , ..., 2.19016088, 1.58732565,\n",
       "          1.51860873],\n",
       "         [1.95126493, 1.3463227 , 1.37726343, ..., 2.0618537 , 1.76861528,\n",
       "          1.49380526],\n",
       "         [2.01616974, 1.02969744, 1.13655219, ..., 1.88687232, 1.71795398,\n",
       "          1.27581726]]),\n",
       "  array([[1.75229496, 1.17179897, 1.38679328, ..., 2.00956314, 1.99677152,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.55154051, 1.14739199, ..., 1.09408233, 1.84218341,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.17548969, 2.04588506, ..., 1.02599198, 1.65890547,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.10399698, 1.48629356, ..., 1.08645904, 1.94578817,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.20842149, 1.33536065, ..., 1.08967363, 2.03992909,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.00401835, 1.42731638, ..., 1.02760386, 1.26557088,\n",
       "          1.48490784]]),\n",
       "  array([[1.29593204, 2.03553967, 1.09750372, ..., 1.39398663, 1.24473956,\n",
       "          1.08858277],\n",
       "         [1.31486682, 1.71333408, 1.09001176, ..., 1.6435961 , 1.23814743,\n",
       "          1.5362284 ],\n",
       "         [1.22669274, 2.09501986, 1.50119313, ..., 1.651203  , 1.96907344,\n",
       "          1.03220734],\n",
       "         ...,\n",
       "         [1.60799784, 1.95060612, 1.22091468, ..., 1.63736851, 1.35776499,\n",
       "          1.04946952],\n",
       "         [1.86430948, 2.23128231, 1.14586869, ..., 1.80025557, 2.06941811,\n",
       "          1.09720001],\n",
       "         [1.39667552, 1.87647379, 1.09432435, ..., 1.82678049, 1.21459948,\n",
       "          1.04016373]]),\n",
       "  array([[1.51062881, 1.11357927, 1.04748401, ..., 1.94471567, 1.40611403,\n",
       "          1.15159097],\n",
       "         [1.57174699, 1.08125485, 1.04387036, ..., 1.68773335, 1.60303555,\n",
       "          1.1048707 ],\n",
       "         [1.65440876, 1.50183729, 1.03884566, ..., 1.82880119, 1.74527114,\n",
       "          1.05367355],\n",
       "         ...,\n",
       "         [1.15214069, 1.09780951, 1.10161756, ..., 2.09875452, 1.24443411,\n",
       "          1.11214816],\n",
       "         [1.54446902, 1.1840609 , 1.06897783, ..., 2.10507291, 1.60773321,\n",
       "          0.94702493],\n",
       "         [1.70415854, 1.86737915, 1.75877232, ..., 1.560967  , 1.13614796,\n",
       "          1.05412739]]),\n",
       "  array([[1.7406541 , 1.04225067, 1.355315  , ..., 1.1425674 , 1.14793199,\n",
       "          1.4612936 ],\n",
       "         [1.35527318, 1.07727265, 1.44095434, ..., 1.11306399, 1.15223675,\n",
       "          1.32202116],\n",
       "         [1.47314064, 1.02049032, 1.75280529, ..., 1.67420699, 1.09211877,\n",
       "          1.3907712 ],\n",
       "         ...,\n",
       "         [2.16724488, 1.06192175, 1.92660565, ..., 1.18972118, 1.12298879,\n",
       "          1.84478255],\n",
       "         [1.18823979, 1.07116564, 1.983516  , ..., 1.08319388, 1.165544  ,\n",
       "          2.04305524],\n",
       "         [1.64367783, 1.53527255, 1.13336001, ..., 1.76450809, 1.12759566,\n",
       "          1.02176726]]),\n",
       "  array([[1.9879039 , 2.32104202, 1.74089324, ..., 1.44594472, 1.55625929,\n",
       "          2.17231365],\n",
       "         [2.04251532, 2.19259622, 1.75482235, ..., 1.17087632, 1.55647848,\n",
       "          2.19242548],\n",
       "         [2.06560484, 2.24093854, 1.84643099, ..., 1.71516363, 1.5333187 ,\n",
       "          2.16245159],\n",
       "         ...,\n",
       "         [2.16512992, 2.28963881, 1.87698481, ..., 1.19055921, 1.61903307,\n",
       "          2.11104923],\n",
       "         [2.06937411, 2.25334223, 1.77401172, ..., 1.26978289, 1.67756874,\n",
       "          2.21355787],\n",
       "         [2.01468986, 1.68422518, 1.61597096, ..., 1.30376228, 1.60900528,\n",
       "          2.19443947]]),\n",
       "  array([[2.26307905, 1.00682826, 2.24043193, ..., 2.18224369, 1.83791051,\n",
       "          2.23046527],\n",
       "         [2.08614175, 1.3739963 , 2.25851867, ..., 2.27045283, 1.67375866,\n",
       "          1.94276135],\n",
       "         [2.11737687, 1.04524682, 2.28797915, ..., 2.13353149, 1.66685942,\n",
       "          1.81333753],\n",
       "         ...,\n",
       "         [1.96585764, 1.09462566, 2.09565728, ..., 2.00938673, 1.72055571,\n",
       "          2.03792332],\n",
       "         [1.88999768, 1.09273582, 2.24653744, ..., 1.87367664, 1.70909415,\n",
       "          1.8995943 ],\n",
       "         [1.01021402, 1.05862461, 2.30855631, ..., 2.15211217, 1.74025424,\n",
       "          1.88380671]]),\n",
       "  array([[2.26293751, 1.51062881, 2.29640743, ..., 2.07400695, 2.03348599,\n",
       "          1.82557653],\n",
       "         [2.27719867, 1.57174699, 2.50619434, ..., 2.13557445, 2.01439139,\n",
       "          1.42277476],\n",
       "         [2.18916216, 1.65440876, 2.03070712, ..., 1.94810722, 1.79797796,\n",
       "          1.70676048],\n",
       "         ...,\n",
       "         [2.24016328, 1.15214069, 2.28002407, ..., 2.07522002, 1.75422262,\n",
       "          1.54225427],\n",
       "         [2.15078532, 1.54446902, 1.60011726, ..., 1.972218  , 1.70052742,\n",
       "          1.80142249],\n",
       "         [2.20882803, 1.70415854, 2.42171736, ..., 1.9030316 , 1.66292863,\n",
       "          1.51188483]]),\n",
       "  array([[1.10938801, 1.82784352, 2.08947715, ..., 1.83791051, 2.45441246,\n",
       "          1.50508328],\n",
       "         [1.17109744, 2.08284867, 2.12281262, ..., 1.67375866, 2.41942682,\n",
       "          1.54378803],\n",
       "         [1.23674017, 1.97835175, 2.08403202, ..., 1.66685942, 2.22201138,\n",
       "          1.38411001],\n",
       "         ...,\n",
       "         [1.11741033, 2.05528472, 2.14899695, ..., 1.72055571, 2.39038711,\n",
       "          1.49939304],\n",
       "         [1.33189579, 1.88078411, 1.95442241, ..., 1.70909415, 2.22908602,\n",
       "          1.24664423],\n",
       "         [2.2472187 , 1.91076932, 2.10764002, ..., 1.74025424, 2.23055028,\n",
       "          1.30255996]]),\n",
       "  array([[1.10938801, 2.26293751, 2.29640743, ..., 1.04365281, 1.5237267 ,\n",
       "          1.08694212],\n",
       "         [1.17109744, 2.27719867, 2.50619434, ..., 1.1450558 , 1.14702919,\n",
       "          1.04285192],\n",
       "         [1.23674017, 2.18916216, 2.03070712, ..., 1.10094106, 1.5049025 ,\n",
       "          1.06872553],\n",
       "         ...,\n",
       "         [1.11741033, 2.24016328, 2.28002407, ..., 1.0511041 , 1.37195886,\n",
       "          1.05734956],\n",
       "         [1.33189579, 2.15078532, 1.60011726, ..., 1.05143166, 1.615447  ,\n",
       "          1.23543696],\n",
       "         [2.2472187 , 2.20882803, 2.42171736, ..., 1.05789354, 1.2821139 ,\n",
       "          1.32472077]]),\n",
       "  array([[1.75229496, 1.68064894, 2.03072632, ..., 1.55613208, 1.04365281,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.97542085, 1.91924292, ..., 1.2340156 , 1.1450558 ,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.6333003 , 2.14164865, ..., 1.47221644, 1.10094106,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.43886491, 2.06130454, ..., 1.58732565, 1.0511041 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.61759491, 1.95126493, ..., 1.76861528, 1.05143166,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.71392807, 2.01616974, ..., 1.71795398, 1.05789354,\n",
       "          1.48490784]]),\n",
       "  array([[2.26293751, 2.37747944, 1.51062881, ..., 1.8083045 , 2.21800728,\n",
       "          1.78901639],\n",
       "         [2.27719867, 1.93373219, 1.57174699, ..., 1.91529173, 1.43408537,\n",
       "          1.55205892],\n",
       "         [2.18916216, 2.36122218, 1.65440876, ..., 1.76202332, 2.03421238,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [2.24016328, 2.37456741, 1.15214069, ..., 1.73356331, 2.17857692,\n",
       "          1.71962776],\n",
       "         [2.15078532, 2.47449087, 1.54446902, ..., 1.54590377, 2.3283029 ,\n",
       "          1.54759422],\n",
       "         [2.20882803, 2.37103696, 1.70415854, ..., 1.67241869, 1.22255985,\n",
       "          1.48490784]]),\n",
       "  array([[1.10938801, 1.82784352, 2.08947715, ..., 2.23046527, 2.45441246,\n",
       "          1.50508328],\n",
       "         [1.17109744, 2.08284867, 2.12281262, ..., 1.94276135, 2.41942682,\n",
       "          1.54378803],\n",
       "         [1.23674017, 1.97835175, 2.08403202, ..., 1.81333753, 2.22201138,\n",
       "          1.38411001],\n",
       "         ...,\n",
       "         [1.11741033, 2.05528472, 2.14899695, ..., 2.03792332, 2.39038711,\n",
       "          1.49939304],\n",
       "         [1.33189579, 1.88078411, 1.95442241, ..., 1.8995943 , 2.22908602,\n",
       "          1.24664423],\n",
       "         [2.2472187 , 1.91076932, 2.10764002, ..., 1.88380671, 2.23055028,\n",
       "          1.30255996]]),\n",
       "  array([[1.51062881, 1.17863288, 1.04000575, ..., 1.15233542, 1.12526225,\n",
       "          2.04790568],\n",
       "         [1.57174699, 1.13261297, 1.06134474, ..., 1.11920716, 1.11808438,\n",
       "          1.38759913],\n",
       "         [1.65440876, 1.16457548, 1.0885975 , ..., 1.14812545, 1.46013017,\n",
       "          1.78390168],\n",
       "         ...,\n",
       "         [1.15214069, 1.25384151, 1.1658151 , ..., 1.1875692 , 1.08328009,\n",
       "          1.84280718],\n",
       "         [1.54446902, 1.19950297, 1.11674575, ..., 1.16665782, 1.11578666,\n",
       "          1.52390521],\n",
       "         [1.70415854, 1.4654671 , 1.64024084, ..., 1.15452045, 1.26741483,\n",
       "          1.42122684]]),\n",
       "  array([[1.51062881, 1.17863288, 2.33415201, ..., 2.04790568, 1.08839286,\n",
       "          1.8083045 ],\n",
       "         [1.57174699, 1.13261297, 2.27873942, ..., 1.38759913, 1.04912533,\n",
       "          1.91529173],\n",
       "         [1.65440876, 1.16457548, 2.325405  , ..., 1.78390168, 1.09307187,\n",
       "          1.76202332],\n",
       "         ...,\n",
       "         [1.15214069, 1.25384151, 2.3940365 , ..., 1.84280718, 1.01487412,\n",
       "          1.73356331],\n",
       "         [1.54446902, 1.19950297, 2.27260928, ..., 1.52390521, 1.09551702,\n",
       "          1.54590377],\n",
       "         [1.70415854, 1.4654671 , 2.42754078, ..., 1.42122684, 0.95828499,\n",
       "          1.67241869]]),\n",
       "  array([[1.41336423, 1.51062881, 1.05279817, ..., 1.98387903, 1.43952253,\n",
       "          1.19459356],\n",
       "         [1.68426866, 1.57174699, 1.10680125, ..., 1.96185081, 1.69539872,\n",
       "          1.3450805 ],\n",
       "         [2.42950053, 1.65440876, 1.07970164, ..., 2.04250882, 1.42965907,\n",
       "          1.45791797],\n",
       "         ...,\n",
       "         [1.92345199, 1.15214069, 1.09094281, ..., 2.19016088, 1.53499661,\n",
       "          1.51860873],\n",
       "         [1.49501566, 1.54446902, 1.1105129 , ..., 2.0618537 , 1.33643527,\n",
       "          1.49380526],\n",
       "         [1.14389835, 1.70415854, 1.14749517, ..., 1.88687232, 1.31848865,\n",
       "          1.27581726]]),\n",
       "  array([[1.41336423, 1.23558028, 1.51062881, ..., 2.2671277 , 1.08839286,\n",
       "          1.33654775],\n",
       "         [1.68426866, 1.50718107, 1.57174699, ..., 2.25797807, 1.04912533,\n",
       "          1.08554044],\n",
       "         [2.42950053, 1.28054133, 1.65440876, ..., 2.19869978, 1.09307187,\n",
       "          1.39652269],\n",
       "         ...,\n",
       "         [1.92345199, 1.6170603 , 1.15214069, ..., 2.18812136, 1.01487412,\n",
       "          1.37543407],\n",
       "         [1.49501566, 1.32972665, 1.54446902, ..., 2.17607145, 1.09551702,\n",
       "          1.28080554],\n",
       "         [1.14389835, 1.63321351, 1.70415854, ..., 2.02621582, 0.95828499,\n",
       "          1.63043536]]),\n",
       "  array([[1.50415834, 2.06072626, 1.33654775, ..., 1.56183897, 2.20439914,\n",
       "          1.78901639],\n",
       "         [1.03935423, 1.86050822, 1.08554044, ..., 1.15142125, 2.12523809,\n",
       "          1.55205892],\n",
       "         [1.87182879, 1.83855123, 1.39652269, ..., 1.59403996, 2.07647221,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.6910132 , 1.88597731, 1.37543407, ..., 1.72734875, 2.20350442,\n",
       "          1.71962776],\n",
       "         [2.16115247, 2.09805395, 1.28080554, ..., 1.4012455 , 2.29747139,\n",
       "          1.54759422],\n",
       "         [1.74911949, 2.24973533, 1.63043536, ..., 1.33461808, 1.91581996,\n",
       "          1.48490784]]),\n",
       "  array([[1.51062881, 1.82035548, 1.5337192 , ..., 1.98387903, 2.03572142,\n",
       "          1.38212077],\n",
       "         [1.57174699, 1.99570181, 1.69940077, ..., 1.96185081, 2.11145982,\n",
       "          1.60008364],\n",
       "         [1.65440876, 1.62510335, 1.57800324, ..., 2.04250882, 2.10039817,\n",
       "          1.76791006],\n",
       "         ...,\n",
       "         [1.15214069, 1.80182572, 1.39796949, ..., 2.19016088, 2.23418029,\n",
       "          1.55127084],\n",
       "         [1.54446902, 1.76148764, 1.45388151, ..., 2.0618537 , 2.12166989,\n",
       "          1.5577182 ],\n",
       "         [1.70415854, 1.36287659, 1.28742919, ..., 1.88687232, 1.30997242,\n",
       "          1.60967029]]),\n",
       "  array([[1.82035548, 1.18342647, 1.5337192 , ..., 1.56183897, 1.38212077,\n",
       "          1.5741009 ],\n",
       "         [1.99570181, 1.84577226, 1.69940077, ..., 1.15142125, 1.60008364,\n",
       "          1.15108094],\n",
       "         [1.62510335, 1.14791876, 1.57800324, ..., 1.59403996, 1.76791006,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.80182572, 1.4914161 , 1.39796949, ..., 1.72734875, 1.55127084,\n",
       "          1.4854386 ],\n",
       "         [1.76148764, 1.401965  , 1.45388151, ..., 1.4012455 , 1.5577182 ,\n",
       "          1.69679409],\n",
       "         [1.36287659, 1.73789256, 1.28742919, ..., 1.33461808, 1.60967029,\n",
       "          1.18713617]]),\n",
       "  array([[1.14391016, 1.72594368, 1.45968794, ..., 2.28699441, 2.00031796,\n",
       "          1.83330529],\n",
       "         [1.12735371, 1.05362179, 1.46707993, ..., 2.20564181, 1.89851966,\n",
       "          1.45584712],\n",
       "         [1.18956247, 1.11047783, 1.46267756, ..., 2.28755422, 1.85768504,\n",
       "          1.93710281],\n",
       "         ...,\n",
       "         [1.30719077, 1.31626344, 1.45117432, ..., 2.30373795, 1.75467465,\n",
       "          1.90340287],\n",
       "         [1.21169804, 2.0308627 , 1.40885123, ..., 2.21808761, 1.77727243,\n",
       "          1.90324098],\n",
       "         [1.14491311, 1.25344766, 1.4032937 , ..., 2.22543443, 1.86028122,\n",
       "          1.74628183]]),\n",
       "  array([[1.96353466, 1.65447031, 2.12168573, ..., 2.30087246, 1.29564013,\n",
       "          1.99677152],\n",
       "         [2.10780692, 1.80265827, 1.91013544, ..., 2.2574332 , 1.19056253,\n",
       "          1.84218341],\n",
       "         [1.96918569, 1.76901893, 2.36975273, ..., 2.31151678, 1.17893099,\n",
       "          1.65890547],\n",
       "         ...,\n",
       "         [2.05799263, 1.68369785, 1.98989696, ..., 2.19064421, 1.2812754 ,\n",
       "          1.94578817],\n",
       "         [2.00460441, 1.79256283, 1.75398044, ..., 2.16463703, 1.47628314,\n",
       "          2.03992909],\n",
       "         [1.77107786, 1.70785713, 1.52630415, ..., 2.17279368, 1.28541721,\n",
       "          1.26557088]]),\n",
       "  array([[2.03417348, 2.08947715, 2.2107843 , ..., 2.28699441, 1.92948383,\n",
       "          0.99326476],\n",
       "         [1.39969625, 2.12281262, 2.06391487, ..., 2.20564181, 1.87192998,\n",
       "          1.04183067],\n",
       "         [1.7458512 , 2.08403202, 2.20457369, ..., 2.28755422, 1.95343293,\n",
       "          1.11117826],\n",
       "         ...,\n",
       "         [2.1795165 , 2.14899695, 2.1286385 , ..., 2.30373795, 2.02377022,\n",
       "          1.06054999],\n",
       "         [2.07664843, 1.95442241, 2.10515487, ..., 2.21808761, 2.00041391,\n",
       "          1.55622024],\n",
       "         [1.61289589, 2.10764002, 2.0255702 , ..., 2.22543443, 1.92855705,\n",
       "          1.30538952]]),\n",
       "  array([[1.01723553, 1.39996235, 0.97826525, ..., 1.17155007, 1.14605584,\n",
       "          2.20439914],\n",
       "         [0.98797525, 1.89553111, 1.04477012, ..., 1.19735603, 1.12452159,\n",
       "          2.12523809],\n",
       "         [1.84876577, 1.54980834, 1.00092216, ..., 1.14239625, 1.29773726,\n",
       "          2.07647221],\n",
       "         ...,\n",
       "         [1.00916697, 1.55607828, 0.95290886, ..., 1.15130859, 1.31144586,\n",
       "          2.20350442],\n",
       "         [2.09600067, 1.43051353, 1.13304869, ..., 1.21595126, 2.19442152,\n",
       "          2.29747139],\n",
       "         [1.01669741, 1.44218916, 0.94485519, ..., 1.16419985, 1.15537141,\n",
       "          1.91581996]]),\n",
       "  array([[1.10623661, 1.7881791 , 1.11795947, ..., 0.99326476, 1.88890423,\n",
       "          1.5237267 ],\n",
       "         [1.07487982, 1.79536409, 1.1078577 , ..., 1.04183067, 1.61339557,\n",
       "          1.14702919],\n",
       "         [1.08635114, 1.71870081, 1.18096099, ..., 1.11117826, 1.82420013,\n",
       "          1.5049025 ],\n",
       "         ...,\n",
       "         [1.08365046, 1.90039967, 1.29487814, ..., 1.06054999, 1.44690038,\n",
       "          1.37195886],\n",
       "         [1.29061458, 1.60595427, 1.10132581, ..., 1.55622024, 1.63839166,\n",
       "          1.615447  ],\n",
       "         [1.07458931, 1.85552575, 1.14970618, ..., 1.30538952, 1.72001073,\n",
       "          1.2821139 ]]),\n",
       "  array([[1.75229496, 1.39996235, 2.07023958, ..., 2.04790568, 2.20439914,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89553111, 2.07630483, ..., 1.38759913, 2.12523809,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.54980834, 2.1176614 , ..., 1.78390168, 2.07647221,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 1.81160523, ..., 1.84280718, 2.20350442,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.43051353, 1.54072683, ..., 1.52390521, 2.29747139,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.44218916, 1.69227599, ..., 1.42122684, 1.91581996,\n",
       "          1.48490784]]),\n",
       "  array([[2.37747944, 1.39996235, 1.45968794, ..., 2.19371008, 1.88890423,\n",
       "          2.20439914],\n",
       "         [1.93373219, 1.89553111, 1.46707993, ..., 2.36697793, 1.61339557,\n",
       "          2.12523809],\n",
       "         [2.36122218, 1.54980834, 1.46267756, ..., 2.29426944, 1.82420013,\n",
       "          2.07647221],\n",
       "         ...,\n",
       "         [2.37456741, 1.55607828, 1.45117432, ..., 2.21601682, 1.44690038,\n",
       "          2.20350442],\n",
       "         [2.47449087, 1.43051353, 1.40885123, ..., 2.04581025, 1.63839166,\n",
       "          2.29747139],\n",
       "         [2.37103696, 1.44218916, 1.4032937 , ..., 2.2238089 , 1.72001073,\n",
       "          1.91581996]]),\n",
       "  array([[1.25307349, 1.39996235, 1.75229496, ..., 0.99326476, 1.5237267 ,\n",
       "          1.78901639],\n",
       "         [1.80478644, 1.89553111, 1.3757134 , ..., 1.04183067, 1.14702919,\n",
       "          1.55205892],\n",
       "         [1.34397588, 1.54980834, 1.04714897, ..., 1.11117826, 1.5049025 ,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [2.16205518, 1.55607828, 1.77179139, ..., 1.06054999, 1.37195886,\n",
       "          1.71962776],\n",
       "         [1.51954481, 1.43051353, 1.24833316, ..., 1.55622024, 1.615447  ,\n",
       "          1.54759422],\n",
       "         [2.00835726, 1.44218916, 1.19976203, ..., 1.30538952, 1.2821139 ,\n",
       "          1.48490784]]),\n",
       "  array([[1.18483994, 2.03072632, 1.13037483, ..., 1.45123111, 1.29612847,\n",
       "          1.11762943],\n",
       "         [1.08815167, 1.91924292, 1.11118008, ..., 1.27462749, 1.18082436,\n",
       "          1.11493824],\n",
       "         [1.0221904 , 2.14164865, 1.1061603 , ..., 1.45647973, 1.33497905,\n",
       "          1.31083748],\n",
       "         ...,\n",
       "         [1.02853296, 2.06130454, 1.11254957, ..., 1.26105576, 1.88342225,\n",
       "          1.08652954],\n",
       "         [1.0930099 , 1.95126493, 1.11676294, ..., 1.43273563, 1.62169683,\n",
       "          1.33837173],\n",
       "         [1.04165874, 2.01616974, 1.14890419, ..., 1.13884269, 1.02208707,\n",
       "          1.10160846]]),\n",
       "  array([[1.18483994, 1.42879853, 1.13037483, ..., 1.01357899, 0.99326476,\n",
       "          1.56183897],\n",
       "         [1.08815167, 1.19254431, 1.11118008, ..., 0.98704063, 1.04183067,\n",
       "          1.15142125],\n",
       "         [1.0221904 , 1.19609357, 1.1061603 , ..., 1.07521625, 1.11117826,\n",
       "          1.59403996],\n",
       "         ...,\n",
       "         [1.02853296, 1.25123703, 1.11254957, ..., 1.00787104, 1.06054999,\n",
       "          1.72734875],\n",
       "         [1.0930099 , 1.29591371, 1.11676294, ..., 1.03762619, 1.55622024,\n",
       "          1.4012455 ],\n",
       "         [1.04165874, 1.09734826, 1.14890419, ..., 1.03843459, 1.30538952,\n",
       "          1.33461808]]),\n",
       "  array([[1.75229496, 1.39996235, 1.60847291, ..., 1.5237267 , 0.99326476,\n",
       "          2.20439914],\n",
       "         [1.3757134 , 1.89553111, 1.35935946, ..., 1.14702919, 1.04183067,\n",
       "          2.12523809],\n",
       "         [1.04714897, 1.54980834, 1.76830919, ..., 1.5049025 , 1.11117826,\n",
       "          2.07647221],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 1.55294879, ..., 1.37195886, 1.06054999,\n",
       "          2.20350442],\n",
       "         [1.24833316, 1.43051353, 1.85960462, ..., 1.615447  , 1.55622024,\n",
       "          2.29747139],\n",
       "         [1.19976203, 1.44218916, 1.27624195, ..., 1.2821139 , 1.30538952,\n",
       "          1.91581996]]),\n",
       "  array([[1.60847291, 1.51062881, 1.52132349, ..., 1.29564013, 1.39489117,\n",
       "          1.78901639],\n",
       "         [1.35935946, 1.57174699, 1.24583779, ..., 1.19056253, 1.31371781,\n",
       "          1.55205892],\n",
       "         [1.76830919, 1.65440876, 1.12818241, ..., 1.17893099, 1.72530294,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.55294879, 1.15214069, 1.12005351, ..., 1.2812754 , 1.17681188,\n",
       "          1.71962776],\n",
       "         [1.85960462, 1.54446902, 1.1110386 , ..., 1.47628314, 1.56374907,\n",
       "          1.54759422],\n",
       "         [1.27624195, 1.70415854, 1.07273014, ..., 1.28541721, 1.30933179,\n",
       "          1.48490784]]),\n",
       "  array([[2.11370757, 1.05534986, 1.06380828, ..., 1.42125736, 1.01768415,\n",
       "          1.00329243],\n",
       "         [1.89869739, 1.02286742, 1.13090886, ..., 1.40797226, 1.05059697,\n",
       "          1.11084462],\n",
       "         [2.04484497, 0.9842662 , 1.09683043, ..., 1.29282335, 1.00229839,\n",
       "          1.11596713],\n",
       "         ...,\n",
       "         [2.1752741 , 1.07256861, 1.20724213, ..., 1.39333885, 0.96264157,\n",
       "          1.32689281],\n",
       "         [2.07798912, 1.05922363, 1.18054217, ..., 1.47707839, 0.99464783,\n",
       "          1.32256595],\n",
       "         [2.07220472, 0.94520915, 1.1509892 , ..., 1.51482943, 0.99952414,\n",
       "          1.14632257]]),\n",
       "  array([[1.45968794, 1.44614896, 1.18981441, ..., 1.96425968, 2.15728212,\n",
       "          0.99326476],\n",
       "         [1.46707993, 1.36414055, 1.07015243, ..., 1.92361766, 2.26369057,\n",
       "          1.04183067],\n",
       "         [1.46267756, 1.64680352, 1.27549194, ..., 2.03688673, 2.16161361,\n",
       "          1.11117826],\n",
       "         ...,\n",
       "         [1.45117432, 1.61050868, 1.62273201, ..., 1.90923407, 2.1644267 ,\n",
       "          1.06054999],\n",
       "         [1.40885123, 1.4549295 , 1.69682546, ..., 1.7716576 , 1.98564731,\n",
       "          1.55622024],\n",
       "         [1.4032937 , 1.39867421, 1.17960756, ..., 1.81501295, 2.33161428,\n",
       "          1.30538952]]),\n",
       "  array([[2.15522925, 1.74885425, 1.11676901, ..., 1.91405056, 1.32641115,\n",
       "          1.5741009 ],\n",
       "         [2.17515621, 1.06298359, 1.26083804, ..., 2.05467047, 1.12950246,\n",
       "          1.15108094],\n",
       "         [2.08048709, 1.13938308, 1.12987361, ..., 1.87148816, 1.06045605,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [2.21205177, 1.31164223, 1.04131732, ..., 1.85970597, 1.239142  ,\n",
       "          1.4854386 ],\n",
       "         [2.04663546, 2.34542755, 1.3651383 , ..., 1.91177727, 1.11277916,\n",
       "          1.69679409],\n",
       "         [2.07725923, 1.1915328 , 1.48594799, ..., 1.80675301, 1.4850472 ,\n",
       "          1.18713617]]),\n",
       "  array([[2.15522925, 1.45968794, 1.65184439, ..., 1.58982198, 0.99326476,\n",
       "          1.23061716],\n",
       "         [2.17515621, 1.46707993, 1.5361943 , ..., 1.47373381, 1.04183067,\n",
       "          1.18392996],\n",
       "         [2.08048709, 1.46267756, 1.66926676, ..., 1.69020372, 1.11117826,\n",
       "          1.55320612],\n",
       "         ...,\n",
       "         [2.21205177, 1.45117432, 1.81375171, ..., 1.55419103, 1.06054999,\n",
       "          1.6300149 ],\n",
       "         [2.04663546, 1.40885123, 1.85668244, ..., 1.71183886, 1.55622024,\n",
       "          2.0998435 ],\n",
       "         [2.07725923, 1.4032937 , 1.36043676, ..., 1.59186903, 1.30538952,\n",
       "          1.34326663]]),\n",
       "  array([[1.75229496, 1.39996235, 1.24687   , ..., 1.3514235 , 1.5237267 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89553111, 1.11213476, ..., 1.49242114, 1.14702919,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.54980834, 1.29013839, ..., 1.61395233, 1.5049025 ,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 1.14842401, ..., 1.84484785, 1.37195886,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.43051353, 1.18381704, ..., 2.14160989, 1.615447  ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.44218916, 1.50308824, ..., 1.29537749, 1.2821139 ,\n",
       "          1.48490784]]),\n",
       "  array([[2.07023958, 2.08489635, 2.15522925, ..., 1.98387903, 1.91405056,\n",
       "          1.5741009 ],\n",
       "         [2.07630483, 2.15181918, 2.17515621, ..., 1.96185081, 2.05467047,\n",
       "          1.15108094],\n",
       "         [2.1176614 , 1.98054803, 2.08048709, ..., 2.04250882, 1.87148816,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.81160523, 2.23286486, 2.21205177, ..., 2.19016088, 1.85970597,\n",
       "          1.4854386 ],\n",
       "         [1.54072683, 2.49247844, 2.04663546, ..., 2.0618537 , 1.91177727,\n",
       "          1.69679409],\n",
       "         [1.69227599, 1.85103322, 2.07725923, ..., 1.88687232, 1.80675301,\n",
       "          1.18713617]]),\n",
       "  array([[1.72594368, 1.45968794, 1.59923172, ..., 1.96425968, 1.88009401,\n",
       "          1.91956388],\n",
       "         [1.05362179, 1.46707993, 1.52877416, ..., 1.92361766, 2.00394613,\n",
       "          1.98103792],\n",
       "         [1.11047783, 1.46267756, 1.72903255, ..., 2.03688673, 1.94071495,\n",
       "          1.98689455],\n",
       "         ...,\n",
       "         [1.31626344, 1.45117432, 1.67389745, ..., 1.90923407, 1.77292149,\n",
       "          1.91875779],\n",
       "         [2.0308627 , 1.40885123, 1.69815659, ..., 1.7716576 , 1.90354146,\n",
       "          1.82617927],\n",
       "         [1.25344766, 1.4032937 , 1.2812204 , ..., 1.81501295, 1.94376636,\n",
       "          1.79143181]]),\n",
       "  array([[2.08489635, 2.15522925, 1.08974358, ..., 1.32641115, 1.61177011,\n",
       "          1.5741009 ],\n",
       "         [2.15181918, 2.17515621, 1.51074991, ..., 1.12950246, 1.2126352 ,\n",
       "          1.15108094],\n",
       "         [1.98054803, 2.08048709, 1.53020264, ..., 1.06045605, 1.23892683,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [2.23286486, 2.21205177, 1.44092493, ..., 1.239142  , 1.66982811,\n",
       "          1.4854386 ],\n",
       "         [2.49247844, 2.04663546, 1.16093991, ..., 1.11277916, 1.64448561,\n",
       "          1.69679409],\n",
       "         [1.85103322, 2.07725923, 1.14836264, ..., 1.4850472 , 1.22556603,\n",
       "          1.18713617]]),\n",
       "  array([[2.08489635, 1.16024544, 1.18981441, ..., 1.91405056, 1.23061716,\n",
       "          1.29564013],\n",
       "         [2.15181918, 1.12055417, 1.07015243, ..., 2.05467047, 1.18392996,\n",
       "          1.19056253],\n",
       "         [1.98054803, 1.10281393, 1.27549194, ..., 1.87148816, 1.55320612,\n",
       "          1.17893099],\n",
       "         ...,\n",
       "         [2.23286486, 1.08853938, 1.62273201, ..., 1.85970597, 1.6300149 ,\n",
       "          1.2812754 ],\n",
       "         [2.49247844, 1.09058092, 1.69682546, ..., 1.91177727, 2.0998435 ,\n",
       "          1.47628314],\n",
       "         [1.85103322, 1.1028539 , 1.17960756, ..., 1.80675301, 1.34326663,\n",
       "          1.28541721]]),\n",
       "  array([[2.08489635, 1.16136756, 2.18761112, ..., 1.96425968, 1.91405056,\n",
       "          1.29564013],\n",
       "         [2.15181918, 1.0849645 , 2.11051495, ..., 1.92361766, 2.05467047,\n",
       "          1.19056253],\n",
       "         [1.98054803, 1.07859237, 2.09773691, ..., 2.03688673, 1.87148816,\n",
       "          1.17893099],\n",
       "         ...,\n",
       "         [2.23286486, 1.07281672, 2.11497583, ..., 1.90923407, 1.85970597,\n",
       "          1.2812754 ],\n",
       "         [2.49247844, 1.09621154, 1.94145223, ..., 1.7716576 , 1.91177727,\n",
       "          1.47628314],\n",
       "         [1.85103322, 1.04684716, 2.08592546, ..., 1.81501295, 1.80675301,\n",
       "          1.28541721]]),\n",
       "  array([[1.49071219, 1.89046003, 1.87482734, ..., 0.99326476, 1.69926141,\n",
       "          1.94814722],\n",
       "         [1.82317384, 1.78586437, 2.03525269, ..., 1.04183067, 1.76158057,\n",
       "          2.0458707 ],\n",
       "         [1.5164965 , 1.58533758, 1.20489303, ..., 1.11117826, 1.59989489,\n",
       "          2.04187101],\n",
       "         ...,\n",
       "         [1.53257849, 1.74691697, 2.01791251, ..., 1.06054999, 1.22828014,\n",
       "          1.80172749],\n",
       "         [1.50151145, 1.76259337, 1.23806776, ..., 1.55622024, 1.5278795 ,\n",
       "          1.79190071],\n",
       "         [1.67021854, 1.7246061 , 1.93716549, ..., 1.30538952, 1.39375115,\n",
       "          1.91391695]]),\n",
       "  array([[1.83635779, 2.08489635, 1.45968794, ..., 1.91405056, 0.99326476,\n",
       "          2.04385511],\n",
       "         [1.9646279 , 2.15181918, 1.46707993, ..., 2.05467047, 1.04183067,\n",
       "          1.74018722],\n",
       "         [1.78147465, 1.98054803, 1.46267756, ..., 1.87148816, 1.11117826,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [1.68680216, 2.23286486, 1.45117432, ..., 1.85970597, 1.06054999,\n",
       "          1.8359737 ],\n",
       "         [1.37350066, 2.49247844, 1.40885123, ..., 1.91177727, 1.55622024,\n",
       "          1.9253121 ],\n",
       "         [1.66492755, 1.85103322, 1.4032937 , ..., 1.80675301, 1.30538952,\n",
       "          1.31214762]]),\n",
       "  array([[2.07023958, 2.08489635, 2.18445283, ..., 1.88138698, 2.43638473,\n",
       "          0.99326476],\n",
       "         [2.07630483, 2.15181918, 1.99634422, ..., 2.01273752, 1.07727729,\n",
       "          1.04183067],\n",
       "         [2.1176614 , 1.98054803, 2.16209871, ..., 1.90999593, 2.2172255 ,\n",
       "          1.11117826],\n",
       "         ...,\n",
       "         [1.81160523, 2.23286486, 1.97753305, ..., 1.98642572, 1.22011936,\n",
       "          1.06054999],\n",
       "         [1.54072683, 2.49247844, 2.10980785, ..., 1.915977  , 2.26898552,\n",
       "          1.55622024],\n",
       "         [1.69227599, 1.85103322, 2.03371467, ..., 2.04664512, 1.12029426,\n",
       "          1.30538952]]),\n",
       "  array([[1.86088903, 1.45968794, 1.20594384, ..., 1.07575533, 0.99326476,\n",
       "          1.29564013],\n",
       "         [1.17765548, 1.46707993, 1.47103702, ..., 1.1012497 , 1.04183067,\n",
       "          1.19056253],\n",
       "         [1.03528999, 1.46267756, 1.82224895, ..., 1.1028994 , 1.11117826,\n",
       "          1.17893099],\n",
       "         ...,\n",
       "         [1.03280899, 1.45117432, 1.14487099, ..., 1.09105286, 1.06054999,\n",
       "          1.2812754 ],\n",
       "         [1.11352635, 1.40885123, 1.28361821, ..., 1.17522584, 1.55622024,\n",
       "          1.47628314],\n",
       "         [1.00194326, 1.4032937 , 2.0661272 , ..., 1.11463885, 1.30538952,\n",
       "          1.28541721]]),\n",
       "  array([[2.07023958, 1.86088903, 2.15522925, ..., 1.5104227 , 1.0642713 ,\n",
       "          1.61177011],\n",
       "         [2.07630483, 1.17765548, 2.17515621, ..., 1.58823104, 1.09863875,\n",
       "          1.2126352 ],\n",
       "         [2.1176614 , 1.03528999, 2.08048709, ..., 1.84729076, 1.01860558,\n",
       "          1.23892683],\n",
       "         ...,\n",
       "         [1.81160523, 1.03280899, 2.21205177, ..., 1.53414821, 1.07872575,\n",
       "          1.66982811],\n",
       "         [1.54072683, 1.11352635, 2.04663546, ..., 1.76110411, 1.46120524,\n",
       "          1.64448561],\n",
       "         [1.69227599, 1.00194326, 2.07725923, ..., 1.5685212 , 1.10025253,\n",
       "          1.22556603]]),\n",
       "  array([[1.06853772, 1.919772  , 2.10993236, ..., 1.29564013, 1.36262603,\n",
       "          2.04385511],\n",
       "         [1.14488535, 1.90542469, 2.02246393, ..., 1.19056253, 1.22161123,\n",
       "          1.74018722],\n",
       "         [1.00555457, 1.93745413, 2.14272867, ..., 1.17893099, 1.37549081,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [1.01423975, 1.77329359, 1.97934639, ..., 1.2812754 , 1.32780989,\n",
       "          1.8359737 ],\n",
       "         [1.16447072, 1.78022628, 2.16833488, ..., 1.47628314, 1.80768464,\n",
       "          1.9253121 ],\n",
       "         [1.29466697, 1.9721402 , 2.09540949, ..., 1.28541721, 1.36145104,\n",
       "          1.31214762]]),\n",
       "  array([[2.08489635, 2.15522925, 2.10993236, ..., 2.26933477, 1.06779124,\n",
       "          1.12258669],\n",
       "         [2.15181918, 2.17515621, 2.02246393, ..., 2.2636302 , 1.2554284 ,\n",
       "          1.11856363],\n",
       "         [1.98054803, 2.08048709, 2.14272867, ..., 2.3112144 , 1.2628121 ,\n",
       "          1.12202177],\n",
       "         ...,\n",
       "         [2.23286486, 2.21205177, 1.97934639, ..., 2.36506328, 1.08171167,\n",
       "          1.32380284],\n",
       "         [2.49247844, 2.04663546, 2.16833488, ..., 2.32342115, 1.17897114,\n",
       "          1.16884112],\n",
       "         [1.85103322, 2.07725923, 2.09540949, ..., 2.32166869, 1.04028208,\n",
       "          1.82767378]]),\n",
       "  array([[1.86711279, 1.95871214, 1.99986835, ..., 2.43758714, 1.29096037,\n",
       "          2.25824844],\n",
       "         [1.37003818, 1.33247929, 1.99418962, ..., 2.54976349, 1.6224672 ,\n",
       "          2.17397342],\n",
       "         [2.30940618, 1.75343845, 1.80880641, ..., 2.58208098, 1.98783395,\n",
       "          2.24295486],\n",
       "         ...,\n",
       "         [2.13690018, 1.62675013, 1.66164122, ..., 2.51090187, 1.14265046,\n",
       "          2.26118539],\n",
       "         [2.27277597, 1.66741383, 2.034761  , ..., 2.46364351, 1.18489264,\n",
       "          2.16463703],\n",
       "         [2.39663628, 1.36647275, 2.07146593, ..., 2.10598371, 1.41418453,\n",
       "          2.23699328]]),\n",
       "  array([[1.61805485, 2.15522925, 1.9723396 , ..., 2.12695505, 0.99326476,\n",
       "          1.5741009 ],\n",
       "         [1.21460639, 2.17515621, 1.16353949, ..., 1.21853824, 1.04183067,\n",
       "          1.15108094],\n",
       "         [1.32208095, 2.08048709, 1.64727099, ..., 2.04870829, 1.11117826,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [2.03149732, 2.21205177, 1.79735342, ..., 1.23197737, 1.06054999,\n",
       "          1.4854386 ],\n",
       "         [2.19405086, 2.04663546, 1.63533023, ..., 1.22620571, 1.55622024,\n",
       "          1.69679409],\n",
       "         [1.17674742, 2.07725923, 1.59638146, ..., 1.65765548, 1.30538952,\n",
       "          1.18713617]]),\n",
       "  array([[1.8603085 , 2.15522925, 2.10993236, ..., 1.65076199, 1.91405056,\n",
       "          1.5741009 ],\n",
       "         [1.70069377, 2.17515621, 2.02246393, ..., 1.13402999, 2.05467047,\n",
       "          1.15108094],\n",
       "         [1.72271164, 2.08048709, 2.14272867, ..., 1.09393777, 1.87148816,\n",
       "          1.25651027],\n",
       "         ...,\n",
       "         [1.75724392, 2.21205177, 1.97934639, ..., 1.19710844, 1.85970597,\n",
       "          1.4854386 ],\n",
       "         [1.70070169, 2.04663546, 2.16833488, ..., 1.27105338, 1.91177727,\n",
       "          1.69679409],\n",
       "         [2.00432033, 2.07725923, 2.09540949, ..., 1.19619956, 1.80675301,\n",
       "          1.18713617]]),\n",
       "  array([[1.75229496, 2.10333655, 2.07023958, ..., 1.5741009 , 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.21611905, 2.07630483, ..., 1.15108094, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.1789313 , 2.1176614 , ..., 1.25651027, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.18590107, 1.81160523, ..., 1.4854386 , 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.00183703, 1.54072683, ..., 1.69679409, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.13187821, 1.69227599, ..., 1.18713617, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.83122672, 2.15522925, ..., 1.94471567, 1.56183897,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.75601651, 2.17515621, ..., 1.68773335, 1.15142125,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.86308682, 2.08048709, ..., 1.82880119, 1.59403996,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.99791602, 2.21205177, ..., 2.09875452, 1.72734875,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.78465291, 2.04663546, ..., 2.10507291, 1.4012455 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.89545605, 2.07725923, ..., 1.560967  , 1.33461808,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.07023958, 1.71005402, ..., 1.82557653, 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.07630483, 1.70280803, ..., 1.42277476, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.1176614 , 1.72806999, ..., 1.70676048, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.81160523, 1.80336951, ..., 1.54225427, 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.54072683, 1.62721211, ..., 1.80142249, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.69227599, 1.78123099, ..., 1.51188483, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.50415834, 2.15522925, ..., 1.5741009 , 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.03935423, 2.17515621, ..., 1.15108094, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.87182879, 2.08048709, ..., 1.25651027, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.6910132 , 2.21205177, ..., 1.4854386 , 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.16115247, 2.04663546, ..., 1.69679409, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.74911949, 2.07725923, ..., 1.18713617, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.71005402, 2.06072626, ..., 1.08839286, 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.70280803, 1.86050822, ..., 1.04912533, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.72806999, 1.83855123, ..., 1.09307187, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.80336951, 1.88597731, ..., 1.01487412, 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.62721211, 2.09805395, ..., 1.09551702, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.78123099, 2.24973533, ..., 0.95828499, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.18455925, 1.24687   , ..., 1.29125896, 1.5741009 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.24651956, 1.11213476, ..., 1.19379942, 1.15108094,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.39582666, 1.29013839, ..., 1.44208639, 1.25651027,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.69937987, 1.14842401, ..., 1.20247394, 1.4854386 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.034454  , 1.18381704, ..., 1.24552696, 1.69679409,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.87249211, 1.50308824, ..., 1.80370256, 1.18713617,\n",
       "          1.48490784]]),\n",
       "  array([[1.01353975, 1.50415834, 2.1249696 , ..., 1.01471153, 2.37591908,\n",
       "          2.04385511],\n",
       "         [1.0136171 , 1.03935423, 2.12682892, ..., 1.04217554, 1.8006927 ,\n",
       "          1.74018722],\n",
       "         [0.99043443, 1.87182879, 2.1330163 , ..., 1.04842265, 2.26740359,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [0.91087352, 1.6910132 , 2.15198929, ..., 1.18371824, 2.30058124,\n",
       "          1.8359737 ],\n",
       "         [1.02796545, 2.16115247, 2.06824774, ..., 1.15992407, 2.13716871,\n",
       "          1.9253121 ],\n",
       "         [1.40891844, 1.74911949, 1.97936038, ..., 1.61175735, 2.20219802,\n",
       "          1.31214762]]),\n",
       "  array([[2.17435502, 2.26307905, 1.58098443, ..., 1.01471153, 2.37591908,\n",
       "          2.04385511],\n",
       "         [1.2952327 , 2.08614175, 1.32273207, ..., 1.04217554, 1.8006927 ,\n",
       "          1.74018722],\n",
       "         [2.12248882, 2.11737687, 1.59471395, ..., 1.04842265, 2.26740359,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [1.55681602, 1.96585764, 1.39382738, ..., 1.18371824, 2.30058124,\n",
       "          1.8359737 ],\n",
       "         [1.74273618, 1.88999768, 1.10168945, ..., 1.15992407, 2.13716871,\n",
       "          1.9253121 ],\n",
       "         [1.05375208, 1.01021402, 1.09328956, ..., 1.61175735, 2.20219802,\n",
       "          1.31214762]]),\n",
       "  array([[1.75229496, 1.29125896, 1.50415834, ..., 1.5741009 , 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.19379942, 1.03935423, ..., 1.15108094, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.44208639, 1.87182879, ..., 1.25651027, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.20247394, 1.6910132 , ..., 1.4854386 , 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.24552696, 2.16115247, ..., 1.69679409, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.80370256, 1.74911949, ..., 1.18713617, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.50415834, 1.71005402, 2.1249696 , ..., 1.72472204, 1.5741009 ,\n",
       "          1.9585173 ],\n",
       "         [1.03935423, 1.70280803, 2.12682892, ..., 1.40280265, 1.15108094,\n",
       "          1.72264461],\n",
       "         [1.87182879, 1.72806999, 2.1330163 , ..., 1.51098905, 1.25651027,\n",
       "          1.58177802],\n",
       "         ...,\n",
       "         [1.6910132 , 1.80336951, 2.15198929, ..., 1.29195975, 1.4854386 ,\n",
       "          1.60647979],\n",
       "         [2.16115247, 1.62721211, 2.06824774, ..., 1.24988412, 1.69679409,\n",
       "          1.59710946],\n",
       "         [1.74911949, 1.78123099, 1.97936038, ..., 1.34324229, 1.18713617,\n",
       "          1.43491601]]),\n",
       "  array([[1.75229496, 1.71005402, 2.06072626, ..., 1.82557653, 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.70280803, 1.86050822, ..., 1.42277476, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.72806999, 1.83855123, ..., 1.70676048, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.80336951, 1.88597731, ..., 1.54225427, 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.62721211, 2.09805395, ..., 1.80142249, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.78123099, 2.24973533, ..., 1.51188483, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.15522925, 2.00934217, ..., 1.1716103 , 1.00718216,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.17515621, 2.01214278, ..., 1.18926355, 1.04257962,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.08048709, 2.00413486, ..., 1.15179271, 0.97373348,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.21205177, 2.01826492, ..., 1.21641018, 1.0648277 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.04663546, 1.8027581 , ..., 1.20404878, 1.05621983,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.07725923, 2.10242147, ..., 1.2160552 , 2.05725355,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.39996235, 1.60847291, ..., 2.20439914, 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89553111, 1.35935946, ..., 2.12523809, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.54980834, 1.76830919, ..., 2.07647221, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 1.55294879, ..., 2.20350442, 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.43051353, 1.85960462, ..., 2.29747139, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.44218916, 1.27624195, ..., 1.91581996, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 2.26293751, 1.71005402, ..., 1.5741009 , 2.04385511,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 2.27719867, 1.70280803, ..., 1.15108094, 1.74018722,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.18916216, 1.72806999, ..., 1.25651027, 1.64843838,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.24016328, 1.80336951, ..., 1.4854386 , 1.8359737 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.15078532, 1.62721211, ..., 1.69679409, 1.9253121 ,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.20882803, 1.78123099, ..., 1.18713617, 1.31214762,\n",
       "          1.48490784]]),\n",
       "  array([[2.15522925, 2.26307905, 1.16136756, ..., 1.91405056, 1.5741009 ,\n",
       "          2.04385511],\n",
       "         [2.17515621, 2.08614175, 1.0849645 , ..., 2.05467047, 1.15108094,\n",
       "          1.74018722],\n",
       "         [2.08048709, 2.11737687, 1.07859237, ..., 1.87148816, 1.25651027,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [2.21205177, 1.96585764, 1.07281672, ..., 1.85970597, 1.4854386 ,\n",
       "          1.8359737 ],\n",
       "         [2.04663546, 1.88999768, 1.09621154, ..., 1.91177727, 1.69679409,\n",
       "          1.9253121 ],\n",
       "         [2.07725923, 1.01021402, 1.04684716, ..., 1.80675301, 1.18713617,\n",
       "          1.31214762]]),\n",
       "  array([[1.86711279, 2.15522925, 2.26307905, ..., 1.23061716, 1.5741009 ,\n",
       "          2.04385511],\n",
       "         [1.37003818, 2.17515621, 2.08614175, ..., 1.18392996, 1.15108094,\n",
       "          1.74018722],\n",
       "         [2.30940618, 2.08048709, 2.11737687, ..., 1.55320612, 1.25651027,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [2.13690018, 2.21205177, 1.96585764, ..., 1.6300149 , 1.4854386 ,\n",
       "          1.8359737 ],\n",
       "         [2.27277597, 2.04663546, 1.88999768, ..., 2.0998435 , 1.69679409,\n",
       "          1.9253121 ],\n",
       "         [2.39663628, 2.07725923, 1.01021402, ..., 1.34326663, 1.18713617,\n",
       "          1.31214762]]),\n",
       "  array([[1.08195741, 2.15522925, 2.26307905, ..., 1.23061716, 1.5741009 ,\n",
       "          2.04385511],\n",
       "         [1.06325793, 2.17515621, 2.08614175, ..., 1.18392996, 1.15108094,\n",
       "          1.74018722],\n",
       "         [1.81169324, 2.08048709, 2.11737687, ..., 1.55320612, 1.25651027,\n",
       "          1.64843838],\n",
       "         ...,\n",
       "         [1.43157496, 2.21205177, 1.96585764, ..., 1.6300149 , 1.4854386 ,\n",
       "          1.8359737 ],\n",
       "         [1.18184826, 2.04663546, 1.88999768, ..., 2.0998435 , 1.69679409,\n",
       "          1.9253121 ],\n",
       "         [1.05159775, 2.07725923, 1.01021402, ..., 1.34326663, 1.18713617,\n",
       "          1.31214762]]),\n",
       "  array([[1.75229496, 2.03553967, 1.01353975, ..., 1.5741009 , 1.99677152,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.71333408, 1.0136171 , ..., 1.15108094, 1.84218341,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.09501986, 0.99043443, ..., 1.25651027, 1.65890547,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.95060612, 0.91087352, ..., 1.4854386 , 1.94578817,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.23128231, 1.02796545, ..., 1.69679409, 2.03992909,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.87647379, 1.40891844, ..., 1.18713617, 1.26557088,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.12793285, 2.07023958, ..., 1.29125896, 1.5741009 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.1005538 , 2.07630483, ..., 1.19379942, 1.15108094,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.71212879, 2.1176614 , ..., 1.44208639, 1.25651027,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.0519748 , 1.81160523, ..., 1.20247394, 1.4854386 ,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.24399265, 1.54072683, ..., 1.24552696, 1.69679409,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.13660983, 1.69227599, ..., 1.80370256, 1.18713617,\n",
       "          1.48490784]]),\n",
       "  array([[1.75229496, 1.39996235, 1.05028406, ..., 1.5741009 , 1.8083045 ,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.89553111, 1.20993145, ..., 1.15108094, 1.91529173,\n",
       "          1.55205892],\n",
       "         [1.04714897, 1.54980834, 1.5145703 , ..., 1.25651027, 1.76202332,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 1.55607828, 1.58652283, ..., 1.4854386 , 1.73356331,\n",
       "          1.71962776],\n",
       "         [1.24833316, 1.43051353, 1.21913485, ..., 1.69679409, 1.54590377,\n",
       "          1.54759422],\n",
       "         [1.19976203, 1.44218916, 1.05212269, ..., 1.18713617, 1.67241869,\n",
       "          1.48490784]]),\n",
       "  array([[2.08489635, 1.06380828, 1.04165623, ..., 1.42125736, 1.01768415,\n",
       "          2.13666662],\n",
       "         [2.15181918, 1.13090886, 1.06749727, ..., 1.40797226, 1.05059697,\n",
       "          1.946875  ],\n",
       "         [1.98054803, 1.09683043, 1.07442078, ..., 1.29282335, 1.00229839,\n",
       "          1.93136407],\n",
       "         ...,\n",
       "         [2.23286486, 1.20724213, 1.08262627, ..., 1.39333885, 0.96264157,\n",
       "          1.75403774],\n",
       "         [2.49247844, 1.18054217, 1.11883243, ..., 1.47707839, 0.99464783,\n",
       "          2.04238324],\n",
       "         [1.85103322, 1.1509892 , 1.07721099, ..., 1.51482943, 0.99952414,\n",
       "          1.15769442]]),\n",
       "  array([[1.16587867, 2.0025032 , 1.13494531, ..., 1.58982198, 1.14605584,\n",
       "          0.99326476],\n",
       "         [1.44557605, 2.00394777, 1.05925655, ..., 1.47373381, 1.12452159,\n",
       "          1.04183067],\n",
       "         [1.14027519, 2.00384658, 0.98743065, ..., 1.69020372, 1.29773726,\n",
       "          1.11117826],\n",
       "         ...,\n",
       "         [1.50757948, 2.05602952, 1.11289711, ..., 1.55419103, 1.31144586,\n",
       "          1.06054999],\n",
       "         [1.44600618, 2.0055233 , 1.18126679, ..., 1.71183886, 2.19442152,\n",
       "          1.55622024],\n",
       "         [1.86873896, 1.92355718, 1.56480365, ..., 1.59186903, 1.15537141,\n",
       "          1.30538952]]),\n",
       "  array([[1.06380828, 1.20594384, 1.11795947, ..., 1.19629258, 1.42125736,\n",
       "          1.01768415],\n",
       "         [1.13090886, 1.47103702, 1.1078577 , ..., 1.21631542, 1.40797226,\n",
       "          1.05059697],\n",
       "         [1.09683043, 1.82224895, 1.18096099, ..., 1.35356056, 1.29282335,\n",
       "          1.00229839],\n",
       "         ...,\n",
       "         [1.20724213, 1.14487099, 1.29487814, ..., 1.18052415, 1.39333885,\n",
       "          0.96264157],\n",
       "         [1.18054217, 1.28361821, 1.10132581, ..., 1.25002988, 1.47707839,\n",
       "          0.99464783],\n",
       "         [1.1509892 , 2.0661272 , 1.14970618, ..., 1.19301595, 1.51482943,\n",
       "          0.99952414]]),\n",
       "  array([[1.01723553, 0.97826525, 1.71198293, ..., 0.99326476, 1.38212077,\n",
       "          1.29564013],\n",
       "         [0.98797525, 1.04477012, 1.69597825, ..., 1.04183067, 1.60008364,\n",
       "          1.19056253],\n",
       "         [1.84876577, 1.00092216, 1.74070717, ..., 1.11117826, 1.76791006,\n",
       "          1.17893099],\n",
       "         ...,\n",
       "         [1.00916697, 0.95290886, 1.56151906, ..., 1.06054999, 1.55127084,\n",
       "          1.2812754 ],\n",
       "         [2.09600067, 1.13304869, 1.75463588, ..., 1.55622024, 1.5577182 ,\n",
       "          1.47628314],\n",
       "         [1.01669741, 0.94485519, 1.79308991, ..., 1.30538952, 1.60967029,\n",
       "          1.28541721]]),\n",
       "  array([[2.08489635, 1.06380828, 1.11795947, ..., 1.42125736, 1.01768415,\n",
       "          2.13666662],\n",
       "         [2.15181918, 1.13090886, 1.1078577 , ..., 1.40797226, 1.05059697,\n",
       "          1.946875  ],\n",
       "         [1.98054803, 1.09683043, 1.18096099, ..., 1.29282335, 1.00229839,\n",
       "          1.93136407],\n",
       "         ...,\n",
       "         [2.23286486, 1.20724213, 1.29487814, ..., 1.39333885, 0.96264157,\n",
       "          1.75403774],\n",
       "         [2.49247844, 1.18054217, 1.10132581, ..., 1.47707839, 0.99464783,\n",
       "          2.04238324],\n",
       "         [1.85103322, 1.1509892 , 1.14970618, ..., 1.51482943, 0.99952414,\n",
       "          1.15769442]]),\n",
       "  array([[2.08489635, 1.06380828, 1.11795947, ..., 1.01768415, 1.14605584,\n",
       "          2.13666662],\n",
       "         [2.15181918, 1.13090886, 1.1078577 , ..., 1.05059697, 1.12452159,\n",
       "          1.946875  ],\n",
       "         [1.98054803, 1.09683043, 1.18096099, ..., 1.00229839, 1.29773726,\n",
       "          1.93136407],\n",
       "         ...,\n",
       "         [2.23286486, 1.20724213, 1.29487814, ..., 0.96264157, 1.31144586,\n",
       "          1.75403774],\n",
       "         [2.49247844, 1.18054217, 1.10132581, ..., 0.99464783, 2.19442152,\n",
       "          2.04238324],\n",
       "         [1.85103322, 1.1509892 , 1.14970618, ..., 0.99952414, 1.15537141,\n",
       "          1.15769442]]),\n",
       "  array([[2.19251145, 1.06913437, 1.19022403, ..., 1.13608784, 1.02337219,\n",
       "          1.99438775],\n",
       "         [2.03961731, 1.0456313 , 1.22756934, ..., 1.11373221, 1.05265763,\n",
       "          2.1303903 ],\n",
       "         [2.07190205, 1.04087191, 1.17588551, ..., 1.14528517, 1.03104487,\n",
       "          1.92805002],\n",
       "         ...,\n",
       "         [1.80074882, 1.06574913, 1.20362541, ..., 1.12673178, 0.99970229,\n",
       "          2.06723726],\n",
       "         [2.0467531 , 1.23681969, 1.20999529, ..., 1.15921201, 1.21879082,\n",
       "          1.79470818],\n",
       "         [1.92107373, 1.02083646, 1.17026436, ..., 1.18680646, 1.08420947,\n",
       "          1.82452708]]),\n",
       "  array([[1.06062864, 1.1095598 , 1.10863819, ..., 2.20439914, 1.46054516,\n",
       "          2.06244175],\n",
       "         [1.0612046 , 1.10471154, 1.01561107, ..., 2.12523809, 1.19282932,\n",
       "          2.02959005],\n",
       "         [1.02675924, 1.16280604, 1.77124137, ..., 2.07647221, 1.15939498,\n",
       "          2.02033332],\n",
       "         ...,\n",
       "         [1.08901032, 1.06735871, 1.1126908 , ..., 2.20350442, 1.27896888,\n",
       "          2.11250989],\n",
       "         [1.07479227, 1.14506446, 1.07647626, ..., 2.29747139, 1.56930235,\n",
       "          1.92175578],\n",
       "         [1.74269138, 1.15366698, 1.0569786 , ..., 1.91581996, 1.09548947,\n",
       "          1.91407627]]),\n",
       "  array([[1.06062864, 1.1095598 , 1.10863819, ..., 2.20439914, 1.46054516,\n",
       "          2.06244175],\n",
       "         [1.0612046 , 1.10471154, 1.01561107, ..., 2.12523809, 1.19282932,\n",
       "          2.02959005],\n",
       "         [1.02675924, 1.16280604, 1.77124137, ..., 2.07647221, 1.15939498,\n",
       "          2.02033332],\n",
       "         ...,\n",
       "         [1.08901032, 1.06735871, 1.1126908 , ..., 2.20350442, 1.27896888,\n",
       "          2.11250989],\n",
       "         [1.07479227, 1.14506446, 1.07647626, ..., 2.29747139, 1.56930235,\n",
       "          1.92175578],\n",
       "         [1.74269138, 1.15366698, 1.0569786 , ..., 1.91581996, 1.09548947,\n",
       "          1.91407627]]),\n",
       "  array([[1.06062864, 1.1095598 , 1.51062881, ..., 2.20439914, 1.46054516,\n",
       "          2.06244175],\n",
       "         [1.0612046 , 1.10471154, 1.57174699, ..., 2.12523809, 1.19282932,\n",
       "          2.02959005],\n",
       "         [1.02675924, 1.16280604, 1.65440876, ..., 2.07647221, 1.15939498,\n",
       "          2.02033332],\n",
       "         ...,\n",
       "         [1.08901032, 1.06735871, 1.15214069, ..., 2.20350442, 1.27896888,\n",
       "          2.11250989],\n",
       "         [1.07479227, 1.14506446, 1.54446902, ..., 2.29747139, 1.56930235,\n",
       "          1.92175578],\n",
       "         [1.74269138, 1.15366698, 1.70415854, ..., 1.91581996, 1.09548947,\n",
       "          1.91407627]]),\n",
       "  array([[1.75229496, 2.37747944, 1.86088903, ..., 1.96224268, 1.37362353,\n",
       "          1.78901639],\n",
       "         [1.3757134 , 1.93373219, 1.17765548, ..., 2.06138133, 1.93124476,\n",
       "          1.55205892],\n",
       "         [1.04714897, 2.36122218, 1.03528999, ..., 1.94773376, 1.81574941,\n",
       "          1.64322633],\n",
       "         ...,\n",
       "         [1.77179139, 2.37456741, 1.03280899, ..., 1.77411744, 1.27282486,\n",
       "          1.71962776],\n",
       "         [1.24833316, 2.47449087, 1.11352635, ..., 1.73460085, 1.18346705,\n",
       "          1.54759422],\n",
       "         [1.19976203, 2.37103696, 1.00194326, ..., 2.05799662, 1.41524096,\n",
       "          1.48490784]]),\n",
       "  array([[1.06062864, 2.07023958, 1.977652  , ..., 2.21856618, 1.5237267 ,\n",
       "          2.13666662],\n",
       "         [1.0612046 , 2.07630483, 1.75518776, ..., 1.79801521, 1.14702919,\n",
       "          1.946875  ],\n",
       "         [1.02675924, 2.1176614 , 1.71413889, ..., 1.998678  , 1.5049025 ,\n",
       "          1.93136407],\n",
       "         ...,\n",
       "         [1.08901032, 1.81160523, 1.86167654, ..., 1.92350979, 1.37195886,\n",
       "          1.75403774],\n",
       "         [1.07479227, 1.54072683, 1.50530405, ..., 2.29854555, 1.615447  ,\n",
       "          2.04238324],\n",
       "         [1.74269138, 1.69227599, 1.99507045, ..., 2.18613665, 1.2821139 ,\n",
       "          1.15769442]]),\n",
       "  array([[2.07023958, 1.86711279, 1.98994025, ..., 2.43937295, 1.74302309,\n",
       "          1.91504252],\n",
       "         [2.07630483, 1.37003818, 1.8316447 , ..., 1.25826138, 1.45947501,\n",
       "          1.80613305],\n",
       "         [2.1176614 , 2.30940618, 1.95720315, ..., 2.12790003, 1.11831288,\n",
       "          1.7036098 ],\n",
       "         ...,\n",
       "         [1.81160523, 2.13690018, 2.14453035, ..., 2.44198699, 1.57931205,\n",
       "          2.02389054],\n",
       "         [1.54072683, 2.27277597, 1.92853098, ..., 2.49776628, 1.62633917,\n",
       "          1.74523265],\n",
       "         [1.69227599, 2.39663628, 2.08020806, ..., 1.37008463, 1.39079241,\n",
       "          1.79648745]])],\n",
       " 'L': [[<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1006 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 320 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 290 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 88 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 528 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 170 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 390 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 126 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 406 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 132 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2460 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1078 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 322 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 698 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 220 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 308 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 112 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 276 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 96 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 508 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 194 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 62 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 780 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 254 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 382 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 134 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 44 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 518 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 348 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 130 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 398 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 138 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 216 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 700 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 266 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 80 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 380 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 128 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 228 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 86 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 232 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 316 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 130 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 642 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 252 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 352 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 126 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 48 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1008 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 356 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 102 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1012 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 348 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 260 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 88 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 30 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 404 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 138 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 664 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 194 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 480 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 148 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 322 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 106 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 370 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 126 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 366 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 116 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 168 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 68 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 700 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 222 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 404 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 124 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 522 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 208 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 76 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<720x720 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 76390 stored elements in Compressed Sparse Row format>,\n",
       "   <360x360 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 46886 stored elements in Compressed Sparse Row format>,\n",
       "   <180x180 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 21236 stored elements in Compressed Sparse Row format>,\n",
       "   <90x90 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 7360 stored elements in Compressed Sparse Row format>,\n",
       "   <45x45 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2021 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2786 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 940 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 256 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 916 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 294 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1236 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 366 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 102 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2074 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1114 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 460 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2310 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 728 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2048 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 710 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1072 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 394 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 128 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 470 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 148 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 486 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 174 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 586 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1242 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 550 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 208 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 484 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 184 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 54 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 450 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 178 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 56 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 674 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 626 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 184 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 56 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1428 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 446 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 330 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 102 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 504 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 320 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 88 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 440 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 126 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<160x160 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10456 stored elements in Compressed Sparse Row format>,\n",
       "   <80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4600 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1456 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 400 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1804 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 616 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6214 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2704 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 850 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 224 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6696 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2864 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 926 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 256 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4470 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2028 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 730 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1818 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 818 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 274 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2552 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1176 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 420 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 7674 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3074 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 902 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 256 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1418 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 454 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4848 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1852 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 516 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2640 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 862 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 226 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1592 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 684 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 194 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3136 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1130 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 324 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1262 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 470 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 142 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1878 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 776 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 258 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 80 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2388 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 962 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 312 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4732 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2074 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 662 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 170 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4846 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1466 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 400 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1320 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 502 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 166 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2064 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 736 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1908 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 690 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 194 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 406 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 158 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1758 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 744 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 224 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2886 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1256 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 416 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2832 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1334 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 452 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4202 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2128 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 756 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 222 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2166 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1004 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 378 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 110 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1930 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 644 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2556 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1042 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 366 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 120 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<192x192 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 14058 stored elements in Compressed Sparse Row format>,\n",
       "   <96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5844 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1776 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 522 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1212 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 474 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 162 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4574 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1814 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 516 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1110 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 428 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 684 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 232 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 370 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 106 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 192 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 30 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2426 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 962 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 296 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4346 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1642 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 496 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 180 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 80 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 7 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1422 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 606 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 206 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1512 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 640 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 218 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4230 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1694 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 478 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 546 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 180 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 228 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 96 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2004 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 672 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5624 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2302 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 704 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 676 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 240 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2298 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 896 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 276 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2546 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 938 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 256 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4122 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1660 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 546 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 298 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 8340 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3206 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 944 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 256 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2504 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 770 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2432 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1042 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 330 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 98 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 980 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 370 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 102 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3468 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1690 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 602 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 170 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1246 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 440 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4364 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1674 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 442 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1532 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 478 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2144 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 850 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 250 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 846 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 312 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1148 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 464 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 164 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1722 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 510 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3170 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1260 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 382 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 448 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 158 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 254 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 86 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 32 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1774 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 692 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3946 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1414 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 398 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1958 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 702 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2164 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 822 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 224 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1780 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 588 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1706 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 634 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 192 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2644 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 972 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 282 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1726 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 562 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1430 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 472 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 984 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 314 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1310 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 542 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 182 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2984 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1042 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 288 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1790 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 774 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 242 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 508 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 180 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1072 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 340 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 102 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1308 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 590 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 232 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 76 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 710 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 238 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 598 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 244 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 88 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2504 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 986 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 292 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1080 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 382 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 114 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1246 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 450 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 142 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1864 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 638 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1334 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 502 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 168 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 610 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 240 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 78 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1846 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 736 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 240 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 612 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 176 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 86 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 180 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 78 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1454 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 554 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5762 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2380 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 758 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1910 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 802 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 276 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 622 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 286 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 96 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1528 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 526 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 300 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 134 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 46 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1100 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 410 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 120 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1898 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 788 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 264 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 80 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1484 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 518 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 162 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1124 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 388 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 102 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 490 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 214 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 92 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1466 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 614 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 564 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 236 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2498 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1022 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 288 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 990 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 380 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 136 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 442 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 184 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 684 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 272 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 986 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 418 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 140 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3188 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1190 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 342 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 384 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 148 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 60 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1266 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 524 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 166 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 628 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 224 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 60 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 280 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 98 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1468 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 598 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 188 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1032 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 370 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 124 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3684 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1292 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 352 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2278 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1100 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 384 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2320 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 876 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 256 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 560 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 60 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 218 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 80 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 214 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 626 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 222 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 74 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 740 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 290 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 608 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 274 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 114 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 188 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 88 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 934 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 302 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 110 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 40 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 472 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 210 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 76 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 398 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 66 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 478 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 158 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5280 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2388 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 822 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 244 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1910 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 888 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 322 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 98 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2660 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1330 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 550 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 194 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1320 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 600 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 256 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 88 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 564 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 286 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 108 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1658 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 718 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 298 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 96 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<176x176 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 8246 stored elements in Compressed Sparse Row format>,\n",
       "   <88x88 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4024 stored elements in Compressed Sparse Row format>,\n",
       "   <44x44 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1418 stored elements in Compressed Sparse Row format>,\n",
       "   <22x22 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 422 stored elements in Compressed Sparse Row format>,\n",
       "   <11x11 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 121 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 208 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 88 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 432 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 182 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 834 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 300 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 856 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 352 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 114 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1020 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 426 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 448 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 156 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 58 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 784 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 294 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 104 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 34 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4392 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1918 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 652 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4920 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2248 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 756 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 226 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<144x144 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6286 stored elements in Compressed Sparse Row format>,\n",
       "   <72x72 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2776 stored elements in Compressed Sparse Row format>,\n",
       "   <36x36 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 950 stored elements in Compressed Sparse Row format>,\n",
       "   <18x18 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 286 stored elements in Compressed Sparse Row format>,\n",
       "   <9x9 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 81 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1112 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 436 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 142 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 568 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 216 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2600 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 962 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 284 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1262 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 480 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1906 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 706 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 218 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 230 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 116 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 558 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 194 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2094 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 832 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 270 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1878 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 804 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 286 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 574 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 212 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 78 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 24 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3496 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1516 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 476 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 140 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3342 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1290 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 392 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 116 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4642 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1608 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 440 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3362 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1270 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 360 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6698 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2688 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 810 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 226 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3850 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1514 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 464 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<208x208 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 11538 stored elements in Compressed Sparse Row format>,\n",
       "   <104x104 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5326 stored elements in Compressed Sparse Row format>,\n",
       "   <52x52 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1922 stored elements in Compressed Sparse Row format>,\n",
       "   <26x26 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 592 stored elements in Compressed Sparse Row format>,\n",
       "   <13x13 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 169 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 7012 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2894 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 940 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 254 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5880 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2348 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 650 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 168 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<784x784 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4998 stored elements in Compressed Sparse Row format>,\n",
       "   <392x392 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2566 stored elements in Compressed Sparse Row format>,\n",
       "   <196x196 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1136 stored elements in Compressed Sparse Row format>,\n",
       "   <98x98 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 478 stored elements in Compressed Sparse Row format>,\n",
       "   <49x49 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 225 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6524 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2598 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 752 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 226 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4770 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2020 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 708 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3476 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1278 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 408 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 116 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<320x320 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36586 stored elements in Compressed Sparse Row format>,\n",
       "   <160x160 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 17044 stored elements in Compressed Sparse Row format>,\n",
       "   <80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5406 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1514 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 400 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5114 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2504 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 764 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3400 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1540 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 470 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 586 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<128x128 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 8364 stored elements in Compressed Sparse Row format>,\n",
       "   <64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3090 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 872 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 226 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6814 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2750 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 782 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 196 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1732 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 638 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 676 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 226 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 62 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1708 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 592 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 856 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 272 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1580 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 588 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2208 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 734 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 198 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 372 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 132 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 734 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 230 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 78 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1650 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 532 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 784 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 226 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 64 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1758 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 600 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 924 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 322 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 84 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2272 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 870 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 264 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 82 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1768 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 630 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 172 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 50 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<80x80 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4072 stored elements in Compressed Sparse Row format>,\n",
       "   <40x40 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1450 stored elements in Compressed Sparse Row format>,\n",
       "   <20x20 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 398 stored elements in Compressed Sparse Row format>,\n",
       "   <10x10 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <5x5 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 25 stored elements in Compressed Sparse Row format>],\n",
       "  [<112x112 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4790 stored elements in Compressed Sparse Row format>,\n",
       "   <56x56 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1924 stored elements in Compressed Sparse Row format>,\n",
       "   <28x28 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 612 stored elements in Compressed Sparse Row format>,\n",
       "   <14x14 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 168 stored elements in Compressed Sparse Row format>,\n",
       "   <7x7 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 49 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4556 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1718 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 484 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1158 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 398 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 118 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 856 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 354 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 116 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1546 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 506 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 144 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 400 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 124 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1072 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 346 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 100 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 356 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 116 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 46 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2052 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 674 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 206 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 56 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 446 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 126 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 504 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 148 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 316 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 108 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   <2x2 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 774 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 298 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 104 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 34 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 862 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 290 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 90 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 820 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 332 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 120 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<64x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1332 stored elements in Compressed Sparse Row format>,\n",
       "   <32x32 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 524 stored elements in Compressed Sparse Row format>,\n",
       "   <16x16 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 178 stored elements in Compressed Sparse Row format>,\n",
       "   <8x8 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 58 stored elements in Compressed Sparse Row format>,\n",
       "   <4x4 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 16 stored elements in Compressed Sparse Row format>],\n",
       "  [<48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 770 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 262 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 92 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 24 stored elements in Compressed Sparse Row format>,\n",
       "   <3x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 9 stored elements in Compressed Sparse Row format>],\n",
       "  [<96x96 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3186 stored elements in Compressed Sparse Row format>,\n",
       "   <48x48 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1342 stored elements in Compressed Sparse Row format>,\n",
       "   <24x24 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 424 stored elements in Compressed Sparse Row format>,\n",
       "   <12x12 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 122 stored elements in Compressed Sparse Row format>,\n",
       "   <6x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 36 stored elements in Compressed Sparse Row format>]],\n",
       " 'perm': [[6,\n",
       "   7,\n",
       "   0,\n",
       "   22,\n",
       "   15,\n",
       "   19,\n",
       "   25,\n",
       "   32,\n",
       "   17,\n",
       "   24,\n",
       "   12,\n",
       "   18,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   21,\n",
       "   29,\n",
       "   11,\n",
       "   23,\n",
       "   8,\n",
       "   9,\n",
       "   1,\n",
       "   20,\n",
       "   10,\n",
       "   13,\n",
       "   14,\n",
       "   26,\n",
       "   5,\n",
       "   30,\n",
       "   2,\n",
       "   16,\n",
       "   27,\n",
       "   33,\n",
       "   28,\n",
       "   39,\n",
       "   3,\n",
       "   31,\n",
       "   4,\n",
       "   34,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [8,\n",
       "   10,\n",
       "   2,\n",
       "   17,\n",
       "   11,\n",
       "   12,\n",
       "   1,\n",
       "   9,\n",
       "   0,\n",
       "   5,\n",
       "   7,\n",
       "   14,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   4,\n",
       "   15,\n",
       "   13,\n",
       "   16,\n",
       "   3,\n",
       "   6,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [2,\n",
       "   24,\n",
       "   15,\n",
       "   18,\n",
       "   1,\n",
       "   19,\n",
       "   12,\n",
       "   21,\n",
       "   10,\n",
       "   25,\n",
       "   6,\n",
       "   13,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   9,\n",
       "   23,\n",
       "   3,\n",
       "   8,\n",
       "   11,\n",
       "   17,\n",
       "   30,\n",
       "   31,\n",
       "   5,\n",
       "   14,\n",
       "   16,\n",
       "   20,\n",
       "   0,\n",
       "   4,\n",
       "   7,\n",
       "   22],\n",
       "  [15,\n",
       "   16,\n",
       "   6,\n",
       "   11,\n",
       "   9,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   3,\n",
       "   17,\n",
       "   1,\n",
       "   4,\n",
       "   12,\n",
       "   14,\n",
       "   7,\n",
       "   19,\n",
       "   5,\n",
       "   8,\n",
       "   10,\n",
       "   23,\n",
       "   0,\n",
       "   2,\n",
       "   13,\n",
       "   18,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [5,\n",
       "   10,\n",
       "   2,\n",
       "   15,\n",
       "   6,\n",
       "   22,\n",
       "   8,\n",
       "   23,\n",
       "   12,\n",
       "   16,\n",
       "   0,\n",
       "   13,\n",
       "   18,\n",
       "   21,\n",
       "   1,\n",
       "   14,\n",
       "   7,\n",
       "   11,\n",
       "   3,\n",
       "   20,\n",
       "   17,\n",
       "   19,\n",
       "   4,\n",
       "   9,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [32,\n",
       "   34,\n",
       "   40,\n",
       "   68,\n",
       "   49,\n",
       "   64,\n",
       "   9,\n",
       "   59,\n",
       "   8,\n",
       "   54,\n",
       "   17,\n",
       "   48,\n",
       "   23,\n",
       "   29,\n",
       "   22,\n",
       "   25,\n",
       "   63,\n",
       "   66,\n",
       "   50,\n",
       "   61,\n",
       "   10,\n",
       "   57,\n",
       "   45,\n",
       "   53,\n",
       "   35,\n",
       "   44,\n",
       "   46,\n",
       "   60,\n",
       "   16,\n",
       "   47,\n",
       "   11,\n",
       "   41,\n",
       "   2,\n",
       "   51,\n",
       "   52,\n",
       "   69,\n",
       "   33,\n",
       "   42,\n",
       "   70,\n",
       "   71,\n",
       "   7,\n",
       "   21,\n",
       "   31,\n",
       "   62,\n",
       "   6,\n",
       "   15,\n",
       "   13,\n",
       "   19,\n",
       "   56,\n",
       "   58,\n",
       "   4,\n",
       "   43,\n",
       "   0,\n",
       "   3,\n",
       "   26,\n",
       "   37,\n",
       "   14,\n",
       "   27,\n",
       "   20,\n",
       "   55,\n",
       "   30,\n",
       "   67,\n",
       "   5,\n",
       "   38,\n",
       "   1,\n",
       "   18,\n",
       "   39,\n",
       "   65,\n",
       "   28,\n",
       "   36,\n",
       "   12,\n",
       "   24,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [15,\n",
       "   18,\n",
       "   0,\n",
       "   22,\n",
       "   20,\n",
       "   28,\n",
       "   30,\n",
       "   31,\n",
       "   1,\n",
       "   23,\n",
       "   19,\n",
       "   27,\n",
       "   3,\n",
       "   21,\n",
       "   8,\n",
       "   10,\n",
       "   5,\n",
       "   17,\n",
       "   12,\n",
       "   24,\n",
       "   4,\n",
       "   13,\n",
       "   6,\n",
       "   14,\n",
       "   2,\n",
       "   16,\n",
       "   7,\n",
       "   11,\n",
       "   25,\n",
       "   26,\n",
       "   9,\n",
       "   29],\n",
       "  [14,\n",
       "   16,\n",
       "   6,\n",
       "   18,\n",
       "   9,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   3,\n",
       "   5,\n",
       "   10,\n",
       "   13,\n",
       "   0,\n",
       "   4,\n",
       "   1,\n",
       "   8,\n",
       "   7,\n",
       "   15,\n",
       "   11,\n",
       "   23,\n",
       "   2,\n",
       "   19,\n",
       "   12,\n",
       "   17,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [3,\n",
       "   12,\n",
       "   4,\n",
       "   16,\n",
       "   14,\n",
       "   18,\n",
       "   8,\n",
       "   15,\n",
       "   1,\n",
       "   9,\n",
       "   5,\n",
       "   10,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   7,\n",
       "   19,\n",
       "   6,\n",
       "   13,\n",
       "   2,\n",
       "   11,\n",
       "   0,\n",
       "   17,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [3,\n",
       "   7,\n",
       "   15,\n",
       "   30,\n",
       "   24,\n",
       "   26,\n",
       "   9,\n",
       "   23,\n",
       "   1,\n",
       "   12,\n",
       "   2,\n",
       "   21,\n",
       "   11,\n",
       "   16,\n",
       "   6,\n",
       "   18,\n",
       "   20,\n",
       "   28,\n",
       "   19,\n",
       "   31,\n",
       "   8,\n",
       "   25,\n",
       "   14,\n",
       "   17,\n",
       "   0,\n",
       "   13,\n",
       "   10,\n",
       "   27,\n",
       "   5,\n",
       "   22,\n",
       "   4,\n",
       "   29],\n",
       "  [15,\n",
       "   24,\n",
       "   5,\n",
       "   11,\n",
       "   6,\n",
       "   8,\n",
       "   2,\n",
       "   21,\n",
       "   10,\n",
       "   30,\n",
       "   26,\n",
       "   31,\n",
       "   13,\n",
       "   25,\n",
       "   3,\n",
       "   19,\n",
       "   0,\n",
       "   18,\n",
       "   4,\n",
       "   27,\n",
       "   7,\n",
       "   12,\n",
       "   9,\n",
       "   23,\n",
       "   14,\n",
       "   16,\n",
       "   22,\n",
       "   29,\n",
       "   17,\n",
       "   28,\n",
       "   1,\n",
       "   20],\n",
       "  [18,\n",
       "   25,\n",
       "   2,\n",
       "   12,\n",
       "   5,\n",
       "   16,\n",
       "   10,\n",
       "   11,\n",
       "   23,\n",
       "   26,\n",
       "   15,\n",
       "   31,\n",
       "   20,\n",
       "   29,\n",
       "   1,\n",
       "   9,\n",
       "   8,\n",
       "   27,\n",
       "   3,\n",
       "   19,\n",
       "   22,\n",
       "   30,\n",
       "   24,\n",
       "   28,\n",
       "   4,\n",
       "   14,\n",
       "   6,\n",
       "   13,\n",
       "   17,\n",
       "   21,\n",
       "   0,\n",
       "   7],\n",
       "  [25,\n",
       "   28,\n",
       "   17,\n",
       "   29,\n",
       "   3,\n",
       "   12,\n",
       "   11,\n",
       "   16,\n",
       "   10,\n",
       "   21,\n",
       "   5,\n",
       "   14,\n",
       "   1,\n",
       "   7,\n",
       "   0,\n",
       "   26,\n",
       "   9,\n",
       "   18,\n",
       "   6,\n",
       "   23,\n",
       "   15,\n",
       "   24,\n",
       "   8,\n",
       "   20,\n",
       "   4,\n",
       "   22,\n",
       "   19,\n",
       "   27,\n",
       "   2,\n",
       "   13,\n",
       "   30,\n",
       "   31],\n",
       "  [0,\n",
       "   9,\n",
       "   2,\n",
       "   19,\n",
       "   13,\n",
       "   21,\n",
       "   14,\n",
       "   22,\n",
       "   6,\n",
       "   8,\n",
       "   17,\n",
       "   20,\n",
       "   4,\n",
       "   16,\n",
       "   15,\n",
       "   23,\n",
       "   12,\n",
       "   18,\n",
       "   3,\n",
       "   7,\n",
       "   5,\n",
       "   10,\n",
       "   1,\n",
       "   11,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [3,\n",
       "   7,\n",
       "   12,\n",
       "   13,\n",
       "   5,\n",
       "   18,\n",
       "   8,\n",
       "   21,\n",
       "   11,\n",
       "   15,\n",
       "   17,\n",
       "   22,\n",
       "   10,\n",
       "   23,\n",
       "   0,\n",
       "   6,\n",
       "   1,\n",
       "   20,\n",
       "   2,\n",
       "   19,\n",
       "   9,\n",
       "   16,\n",
       "   4,\n",
       "   14,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [3,\n",
       "   10,\n",
       "   7,\n",
       "   16,\n",
       "   4,\n",
       "   5,\n",
       "   17,\n",
       "   18,\n",
       "   6,\n",
       "   12,\n",
       "   15,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   2,\n",
       "   13,\n",
       "   9,\n",
       "   11,\n",
       "   8,\n",
       "   14,\n",
       "   0,\n",
       "   1,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [14,\n",
       "   30,\n",
       "   18,\n",
       "   20,\n",
       "   11,\n",
       "   15,\n",
       "   2,\n",
       "   22,\n",
       "   6,\n",
       "   17,\n",
       "   0,\n",
       "   31,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   3,\n",
       "   9,\n",
       "   8,\n",
       "   38,\n",
       "   21,\n",
       "   29,\n",
       "   28,\n",
       "   32,\n",
       "   10,\n",
       "   13,\n",
       "   5,\n",
       "   39,\n",
       "   1,\n",
       "   23,\n",
       "   24,\n",
       "   27,\n",
       "   12,\n",
       "   26,\n",
       "   19,\n",
       "   33,\n",
       "   4,\n",
       "   7,\n",
       "   16,\n",
       "   25,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [5,\n",
       "   6,\n",
       "   0,\n",
       "   15,\n",
       "   9,\n",
       "   14,\n",
       "   7,\n",
       "   10,\n",
       "   16,\n",
       "   20,\n",
       "   11,\n",
       "   17,\n",
       "   3,\n",
       "   19,\n",
       "   12,\n",
       "   23,\n",
       "   18,\n",
       "   21,\n",
       "   2,\n",
       "   8,\n",
       "   4,\n",
       "   22,\n",
       "   1,\n",
       "   13,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [1,\n",
       "   9,\n",
       "   0,\n",
       "   17,\n",
       "   3,\n",
       "   5,\n",
       "   4,\n",
       "   11,\n",
       "   2,\n",
       "   8,\n",
       "   13,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   6,\n",
       "   14,\n",
       "   7,\n",
       "   15,\n",
       "   10,\n",
       "   12,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [13,\n",
       "   18,\n",
       "   20,\n",
       "   21,\n",
       "   4,\n",
       "   14,\n",
       "   0,\n",
       "   9,\n",
       "   5,\n",
       "   8,\n",
       "   15,\n",
       "   19,\n",
       "   7,\n",
       "   12,\n",
       "   22,\n",
       "   23,\n",
       "   11,\n",
       "   16,\n",
       "   3,\n",
       "   6,\n",
       "   1,\n",
       "   17,\n",
       "   2,\n",
       "   10,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [8,\n",
       "   18,\n",
       "   3,\n",
       "   17,\n",
       "   16,\n",
       "   20,\n",
       "   0,\n",
       "   1,\n",
       "   15,\n",
       "   21,\n",
       "   4,\n",
       "   11,\n",
       "   2,\n",
       "   19,\n",
       "   7,\n",
       "   14,\n",
       "   22,\n",
       "   23,\n",
       "   10,\n",
       "   12,\n",
       "   5,\n",
       "   13,\n",
       "   26,\n",
       "   27,\n",
       "   9,\n",
       "   25,\n",
       "   6,\n",
       "   24,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [6,\n",
       "   15,\n",
       "   2,\n",
       "   9,\n",
       "   4,\n",
       "   22,\n",
       "   0,\n",
       "   3,\n",
       "   5,\n",
       "   30,\n",
       "   7,\n",
       "   23,\n",
       "   11,\n",
       "   25,\n",
       "   10,\n",
       "   29,\n",
       "   1,\n",
       "   14,\n",
       "   12,\n",
       "   27,\n",
       "   21,\n",
       "   24,\n",
       "   8,\n",
       "   13,\n",
       "   20,\n",
       "   26,\n",
       "   28,\n",
       "   31,\n",
       "   17,\n",
       "   18,\n",
       "   16,\n",
       "   19],\n",
       "  [9,\n",
       "   22,\n",
       "   14,\n",
       "   25,\n",
       "   6,\n",
       "   12,\n",
       "   0,\n",
       "   15,\n",
       "   11,\n",
       "   17,\n",
       "   5,\n",
       "   8,\n",
       "   4,\n",
       "   24,\n",
       "   18,\n",
       "   20,\n",
       "   3,\n",
       "   13,\n",
       "   7,\n",
       "   23,\n",
       "   1,\n",
       "   2,\n",
       "   26,\n",
       "   27,\n",
       "   16,\n",
       "   19,\n",
       "   10,\n",
       "   21,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [2,\n",
       "   23,\n",
       "   15,\n",
       "   18,\n",
       "   13,\n",
       "   14,\n",
       "   37,\n",
       "   38,\n",
       "   8,\n",
       "   16,\n",
       "   3,\n",
       "   10,\n",
       "   11,\n",
       "   28,\n",
       "   21,\n",
       "   32,\n",
       "   30,\n",
       "   34,\n",
       "   6,\n",
       "   9,\n",
       "   17,\n",
       "   31,\n",
       "   12,\n",
       "   20,\n",
       "   7,\n",
       "   25,\n",
       "   22,\n",
       "   26,\n",
       "   0,\n",
       "   1,\n",
       "   33,\n",
       "   36,\n",
       "   4,\n",
       "   19,\n",
       "   29,\n",
       "   39,\n",
       "   5,\n",
       "   27,\n",
       "   24,\n",
       "   35,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [10,\n",
       "   24,\n",
       "   18,\n",
       "   33,\n",
       "   11,\n",
       "   17,\n",
       "   22,\n",
       "   30,\n",
       "   2,\n",
       "   32,\n",
       "   6,\n",
       "   8,\n",
       "   7,\n",
       "   23,\n",
       "   12,\n",
       "   35,\n",
       "   20,\n",
       "   38,\n",
       "   13,\n",
       "   39,\n",
       "   27,\n",
       "   37,\n",
       "   3,\n",
       "   14,\n",
       "   1,\n",
       "   29,\n",
       "   26,\n",
       "   36,\n",
       "   5,\n",
       "   25,\n",
       "   0,\n",
       "   9,\n",
       "   15,\n",
       "   19,\n",
       "   4,\n",
       "   21,\n",
       "   16,\n",
       "   31,\n",
       "   28,\n",
       "   34,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [10,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   7,\n",
       "   13,\n",
       "   16,\n",
       "   17,\n",
       "   4,\n",
       "   18,\n",
       "   3,\n",
       "   5,\n",
       "   6,\n",
       "   15,\n",
       "   22,\n",
       "   23,\n",
       "   0,\n",
       "   11,\n",
       "   8,\n",
       "   14,\n",
       "   9,\n",
       "   12,\n",
       "   1,\n",
       "   2,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [16,\n",
       "   17,\n",
       "   18,\n",
       "   23,\n",
       "   9,\n",
       "   12,\n",
       "   4,\n",
       "   8,\n",
       "   3,\n",
       "   5,\n",
       "   1,\n",
       "   19,\n",
       "   10,\n",
       "   14,\n",
       "   6,\n",
       "   22,\n",
       "   2,\n",
       "   20,\n",
       "   15,\n",
       "   21,\n",
       "   11,\n",
       "   13,\n",
       "   0,\n",
       "   7,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [18,\n",
       "   20,\n",
       "   2,\n",
       "   10,\n",
       "   0,\n",
       "   13,\n",
       "   24,\n",
       "   26,\n",
       "   4,\n",
       "   23,\n",
       "   8,\n",
       "   16,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   3,\n",
       "   27,\n",
       "   7,\n",
       "   14,\n",
       "   21,\n",
       "   22,\n",
       "   17,\n",
       "   25,\n",
       "   6,\n",
       "   11,\n",
       "   5,\n",
       "   15,\n",
       "   9,\n",
       "   12,\n",
       "   1,\n",
       "   19],\n",
       "  [10,\n",
       "   11,\n",
       "   7,\n",
       "   23,\n",
       "   3,\n",
       "   13,\n",
       "   12,\n",
       "   20,\n",
       "   6,\n",
       "   17,\n",
       "   1,\n",
       "   8,\n",
       "   14,\n",
       "   22,\n",
       "   9,\n",
       "   21,\n",
       "   5,\n",
       "   15,\n",
       "   0,\n",
       "   2,\n",
       "   16,\n",
       "   19,\n",
       "   4,\n",
       "   18,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [6,\n",
       "   18,\n",
       "   2,\n",
       "   19,\n",
       "   13,\n",
       "   17,\n",
       "   3,\n",
       "   5,\n",
       "   12,\n",
       "   15,\n",
       "   9,\n",
       "   16,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   1,\n",
       "   10,\n",
       "   7,\n",
       "   8,\n",
       "   4,\n",
       "   11,\n",
       "   0,\n",
       "   14,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [16,\n",
       "   21,\n",
       "   10,\n",
       "   22,\n",
       "   1,\n",
       "   15,\n",
       "   7,\n",
       "   20,\n",
       "   8,\n",
       "   13,\n",
       "   2,\n",
       "   17,\n",
       "   14,\n",
       "   19,\n",
       "   9,\n",
       "   11,\n",
       "   3,\n",
       "   4,\n",
       "   12,\n",
       "   23,\n",
       "   0,\n",
       "   5,\n",
       "   6,\n",
       "   18,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [15,\n",
       "   17,\n",
       "   4,\n",
       "   12,\n",
       "   8,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   0,\n",
       "   16,\n",
       "   2,\n",
       "   10,\n",
       "   1,\n",
       "   19,\n",
       "   6,\n",
       "   18,\n",
       "   11,\n",
       "   20,\n",
       "   5,\n",
       "   14,\n",
       "   7,\n",
       "   13,\n",
       "   3,\n",
       "   9,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [4,\n",
       "   15,\n",
       "   0,\n",
       "   12,\n",
       "   3,\n",
       "   8,\n",
       "   17,\n",
       "   18,\n",
       "   6,\n",
       "   14,\n",
       "   1,\n",
       "   5,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   10,\n",
       "   16,\n",
       "   9,\n",
       "   23,\n",
       "   11,\n",
       "   13,\n",
       "   2,\n",
       "   7,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [4,\n",
       "   5,\n",
       "   15,\n",
       "   21,\n",
       "   2,\n",
       "   23,\n",
       "   7,\n",
       "   25,\n",
       "   12,\n",
       "   26,\n",
       "   9,\n",
       "   20,\n",
       "   6,\n",
       "   11,\n",
       "   17,\n",
       "   19,\n",
       "   1,\n",
       "   3,\n",
       "   0,\n",
       "   18,\n",
       "   14,\n",
       "   24,\n",
       "   30,\n",
       "   31,\n",
       "   8,\n",
       "   28,\n",
       "   16,\n",
       "   22,\n",
       "   27,\n",
       "   29,\n",
       "   10,\n",
       "   13],\n",
       "  [1,\n",
       "   3,\n",
       "   4,\n",
       "   13,\n",
       "   10,\n",
       "   12,\n",
       "   22,\n",
       "   23,\n",
       "   14,\n",
       "   17,\n",
       "   0,\n",
       "   11,\n",
       "   2,\n",
       "   7,\n",
       "   20,\n",
       "   21,\n",
       "   6,\n",
       "   15,\n",
       "   9,\n",
       "   19,\n",
       "   16,\n",
       "   18,\n",
       "   5,\n",
       "   8,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [22,\n",
       "   25,\n",
       "   4,\n",
       "   24,\n",
       "   9,\n",
       "   30,\n",
       "   26,\n",
       "   32,\n",
       "   19,\n",
       "   27,\n",
       "   0,\n",
       "   15,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   8,\n",
       "   16,\n",
       "   3,\n",
       "   12,\n",
       "   6,\n",
       "   33,\n",
       "   38,\n",
       "   39,\n",
       "   2,\n",
       "   23,\n",
       "   20,\n",
       "   31,\n",
       "   1,\n",
       "   21,\n",
       "   5,\n",
       "   13,\n",
       "   14,\n",
       "   17,\n",
       "   10,\n",
       "   18,\n",
       "   28,\n",
       "   29,\n",
       "   7,\n",
       "   11,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [70,\n",
       "   540,\n",
       "   636,\n",
       "   650,\n",
       "   2,\n",
       "   23,\n",
       "   187,\n",
       "   429,\n",
       "   221,\n",
       "   374,\n",
       "   129,\n",
       "   607,\n",
       "   256,\n",
       "   649,\n",
       "   383,\n",
       "   451,\n",
       "   263,\n",
       "   652,\n",
       "   56,\n",
       "   642,\n",
       "   50,\n",
       "   114,\n",
       "   280,\n",
       "   353,\n",
       "   315,\n",
       "   396,\n",
       "   144,\n",
       "   477,\n",
       "   78,\n",
       "   473,\n",
       "   326,\n",
       "   411,\n",
       "   34,\n",
       "   88,\n",
       "   107,\n",
       "   684,\n",
       "   52,\n",
       "   301,\n",
       "   568,\n",
       "   693,\n",
       "   308,\n",
       "   524,\n",
       "   506,\n",
       "   551,\n",
       "   35,\n",
       "   194,\n",
       "   156,\n",
       "   706,\n",
       "   154,\n",
       "   698,\n",
       "   527,\n",
       "   704,\n",
       "   423,\n",
       "   438,\n",
       "   288,\n",
       "   561,\n",
       "   86,\n",
       "   186,\n",
       "   574,\n",
       "   712,\n",
       "   183,\n",
       "   262,\n",
       "   338,\n",
       "   471,\n",
       "   395,\n",
       "   509,\n",
       "   331,\n",
       "   537,\n",
       "   146,\n",
       "   357,\n",
       "   134,\n",
       "   214,\n",
       "   57,\n",
       "   634,\n",
       "   294,\n",
       "   445,\n",
       "   189,\n",
       "   413,\n",
       "   501,\n",
       "   517,\n",
       "   131,\n",
       "   135,\n",
       "   319,\n",
       "   346,\n",
       "   293,\n",
       "   369,\n",
       "   125,\n",
       "   392,\n",
       "   136,\n",
       "   178,\n",
       "   42,\n",
       "   253,\n",
       "   255,\n",
       "   323,\n",
       "   82,\n",
       "   688,\n",
       "   398,\n",
       "   466,\n",
       "   217,\n",
       "   235,\n",
       "   339,\n",
       "   651,\n",
       "   163,\n",
       "   389,\n",
       "   454,\n",
       "   678,\n",
       "   17,\n",
       "   662,\n",
       "   51,\n",
       "   555,\n",
       "   626,\n",
       "   630,\n",
       "   190,\n",
       "   508,\n",
       "   7,\n",
       "   343,\n",
       "   47,\n",
       "   120,\n",
       "   90,\n",
       "   94,\n",
       "   49,\n",
       "   663,\n",
       "   185,\n",
       "   207,\n",
       "   305,\n",
       "   548,\n",
       "   73,\n",
       "   441,\n",
       "   266,\n",
       "   619,\n",
       "   230,\n",
       "   671,\n",
       "   83,\n",
       "   143,\n",
       "   348,\n",
       "   546,\n",
       "   166,\n",
       "   601,\n",
       "   622,\n",
       "   700,\n",
       "   403,\n",
       "   496,\n",
       "   267,\n",
       "   498,\n",
       "   38,\n",
       "   511,\n",
       "   31,\n",
       "   633,\n",
       "   91,\n",
       "   427,\n",
       "   26,\n",
       "   344,\n",
       "   182,\n",
       "   363,\n",
       "   210,\n",
       "   677,\n",
       "   260,\n",
       "   434,\n",
       "   575,\n",
       "   713,\n",
       "   450,\n",
       "   453,\n",
       "   412,\n",
       "   624,\n",
       "   370,\n",
       "   513,\n",
       "   149,\n",
       "   656,\n",
       "   291,\n",
       "   418,\n",
       "   270,\n",
       "   495,\n",
       "   196,\n",
       "   486,\n",
       "   211,\n",
       "   447,\n",
       "   3,\n",
       "   324,\n",
       "   41,\n",
       "   596,\n",
       "   384,\n",
       "   557,\n",
       "   121,\n",
       "   502,\n",
       "   97,\n",
       "   328,\n",
       "   223,\n",
       "   303,\n",
       "   45,\n",
       "   567,\n",
       "   165,\n",
       "   518,\n",
       "   307,\n",
       "   696,\n",
       "   533,\n",
       "   604,\n",
       "   313,\n",
       "   657,\n",
       "   27,\n",
       "   703,\n",
       "   63,\n",
       "   179,\n",
       "   295,\n",
       "   647,\n",
       "   507,\n",
       "   707,\n",
       "   521,\n",
       "   665,\n",
       "   201,\n",
       "   570,\n",
       "   152,\n",
       "   379,\n",
       "   300,\n",
       "   457,\n",
       "   59,\n",
       "   598,\n",
       "   220,\n",
       "   588,\n",
       "   352,\n",
       "   629,\n",
       "   113,\n",
       "   252,\n",
       "   362,\n",
       "   669,\n",
       "   531,\n",
       "   583,\n",
       "   173,\n",
       "   431,\n",
       "   349,\n",
       "   448,\n",
       "   304,\n",
       "   616,\n",
       "   32,\n",
       "   261,\n",
       "   15,\n",
       "   218,\n",
       "   354,\n",
       "   493,\n",
       "   655,\n",
       "   690,\n",
       "   198,\n",
       "   228,\n",
       "   60,\n",
       "   439,\n",
       "   417,\n",
       "   461,\n",
       "   21,\n",
       "   172,\n",
       "   231,\n",
       "   233,\n",
       "   276,\n",
       "   490,\n",
       "   161,\n",
       "   334,\n",
       "   155,\n",
       "   241,\n",
       "   664,\n",
       "   672,\n",
       "   118,\n",
       "   386,\n",
       "   12,\n",
       "   378,\n",
       "   459,\n",
       "   609,\n",
       "   538,\n",
       "   660,\n",
       "   212,\n",
       "   387,\n",
       "   175,\n",
       "   388,\n",
       "   76,\n",
       "   479,\n",
       "   75,\n",
       "   287,\n",
       "   99,\n",
       "   702,\n",
       "   199,\n",
       "   224,\n",
       "   483,\n",
       "   668,\n",
       "   111,\n",
       "   342,\n",
       "   213,\n",
       "   542,\n",
       "   109,\n",
       "   162,\n",
       "   251,\n",
       "   514,\n",
       "   0,\n",
       "   336,\n",
       "   108,\n",
       "   426,\n",
       "   39,\n",
       "   576,\n",
       "   119,\n",
       "   452,\n",
       "   202,\n",
       "   390,\n",
       "   64,\n",
       "   646,\n",
       "   528,\n",
       "   589,\n",
       "   273,\n",
       "   345,\n",
       "   65,\n",
       "   245,\n",
       "   8,\n",
       "   503,\n",
       "   37,\n",
       "   80,\n",
       "   14,\n",
       "   29,\n",
       "   58,\n",
       "   367,\n",
       "   95,\n",
       "   410,\n",
       "   510,\n",
       "   534,\n",
       "   464,\n",
       "   515,\n",
       "   98,\n",
       "   314,\n",
       "   33,\n",
       "   532,\n",
       "   48,\n",
       "   310,\n",
       "   181,\n",
       "   449,\n",
       "   227,\n",
       "   710,\n",
       "   472,\n",
       "   494,\n",
       "   419,\n",
       "   554,\n",
       "   422,\n",
       "   500,\n",
       "   10,\n",
       "   283,\n",
       "   505,\n",
       "   536,\n",
       "   209,\n",
       "   430,\n",
       "   381,\n",
       "   628,\n",
       "   399,\n",
       "   437,\n",
       "   414,\n",
       "   487,\n",
       "   714,\n",
       "   715,\n",
       "   716,\n",
       "   717,\n",
       "   117,\n",
       "   203,\n",
       "   265,\n",
       "   691,\n",
       "   132,\n",
       "   580,\n",
       "   318,\n",
       "   516,\n",
       "   605,\n",
       "   620,\n",
       "   329,\n",
       "   416,\n",
       "   468,\n",
       "   563,\n",
       "   608,\n",
       "   711,\n",
       "   539,\n",
       "   667,\n",
       "   376,\n",
       "   569,\n",
       "   444,\n",
       "   632,\n",
       "   645,\n",
       "   666,\n",
       "   188,\n",
       "   709,\n",
       "   138,\n",
       "   499,\n",
       "   101,\n",
       "   347,\n",
       "   72,\n",
       "   259,\n",
       "   476,\n",
       "   621,\n",
       "   446,\n",
       "   692,\n",
       "   368,\n",
       "   640,\n",
       "   46,\n",
       "   66,\n",
       "   269,\n",
       "   639,\n",
       "   460,\n",
       "   520,\n",
       "   225,\n",
       "   436,\n",
       "   409,\n",
       "   433,\n",
       "   480,\n",
       "   579,\n",
       "   122,\n",
       "   455,\n",
       "   337,\n",
       "   391,\n",
       "   170,\n",
       "   240,\n",
       "   594,\n",
       "   708,\n",
       "   564,\n",
       "   683,\n",
       "   71,\n",
       "   591,\n",
       "   222,\n",
       "   366,\n",
       "   610,\n",
       "   654,\n",
       "   177,\n",
       "   236,\n",
       "   77,\n",
       "   275,\n",
       "   192,\n",
       "   602,\n",
       "   474,\n",
       "   638,\n",
       "   311,\n",
       "   385,\n",
       "   110,\n",
       "   481,\n",
       "   442,\n",
       "   549,\n",
       "   106,\n",
       "   206,\n",
       "   89,\n",
       "   282,\n",
       "   237,\n",
       "   475,\n",
       "   400,\n",
       "   415,\n",
       "   153,\n",
       "   584,\n",
       "   277,\n",
       "   689,\n",
       "   216,\n",
       "   595,\n",
       "   115,\n",
       "   577,\n",
       "   193,\n",
       "   627,\n",
       "   171,\n",
       "   679,\n",
       "   623,\n",
       "   695,\n",
       "   312,\n",
       "   420,\n",
       "   148,\n",
       "   504,\n",
       "   13,\n",
       "   248,\n",
       "   5,\n",
       "   151,\n",
       "   9,\n",
       "   407,\n",
       "   123,\n",
       "   249,\n",
       "   1,\n",
       "   341,\n",
       "   147,\n",
       "   258,\n",
       "   330,\n",
       "   612,\n",
       "   6,\n",
       "   523,\n",
       "   174,\n",
       "   264,\n",
       "   116,\n",
       "   680,\n",
       "   169,\n",
       "   377,\n",
       "   356,\n",
       "   397,\n",
       "   254,\n",
       "   686,\n",
       "   16,\n",
       "   360,\n",
       "   718,\n",
       "   719,\n",
       "   243,\n",
       "   544,\n",
       "   572,\n",
       "   611,\n",
       "   239,\n",
       "   547,\n",
       "   405,\n",
       "   597,\n",
       "   44,\n",
       "   355,\n",
       "   552,\n",
       "   560,\n",
       "   296,\n",
       "   462,\n",
       "   351,\n",
       "   581,\n",
       "   104,\n",
       "   497,\n",
       "   402,\n",
       "   675,\n",
       "   105,\n",
       "   373,\n",
       "   309,\n",
       "   670,\n",
       "   519,\n",
       "   573,\n",
       "   317,\n",
       "   676,\n",
       "   40,\n",
       "   327,\n",
       "   465,\n",
       "   578,\n",
       "   160,\n",
       "   687,\n",
       "   61,\n",
       "   93,\n",
       "   30,\n",
       "   625,\n",
       "   333,\n",
       "   440,\n",
       "   133,\n",
       "   142,\n",
       "   306,\n",
       "   478,\n",
       "   279,\n",
       "   543,\n",
       "   158,\n",
       "   234,\n",
       "   159,\n",
       "   603,\n",
       "   11,\n",
       "   184,\n",
       "   364,\n",
       "   562,\n",
       "   62,\n",
       "   145,\n",
       "   274,\n",
       "   586,\n",
       "   127,\n",
       "   699,\n",
       "   36,\n",
       "   482,\n",
       "   204,\n",
       "   278,\n",
       "   55,\n",
       "   84,\n",
       "   67,\n",
       "   289,\n",
       "   406,\n",
       "   565,\n",
       "   244,\n",
       "   469,\n",
       "   100,\n",
       "   617,\n",
       "   87,\n",
       "   322,\n",
       "   250,\n",
       "   456,\n",
       "   297,\n",
       "   340,\n",
       "   141,\n",
       "   285,\n",
       "   96,\n",
       "   102,\n",
       "   18,\n",
       "   408,\n",
       "   271,\n",
       "   424,\n",
       "   590,\n",
       "   600,\n",
       "   653,\n",
       "   694,\n",
       "   232,\n",
       "   299,\n",
       "   257,\n",
       "   529,\n",
       "   150,\n",
       "   614,\n",
       "   139,\n",
       "   525,\n",
       "   24,\n",
       "   458,\n",
       "   130,\n",
       "   372,\n",
       "   541,\n",
       "   648,\n",
       "   644,\n",
       "   674,\n",
       "   176,\n",
       "   635,\n",
       "   128,\n",
       "   157,\n",
       "   229,\n",
       "   659,\n",
       "   22,\n",
       "   197,\n",
       "   556,\n",
       "   558,\n",
       "   335,\n",
       "   350,\n",
       "   432,\n",
       "   530,\n",
       "   488,\n",
       "   491,\n",
       "   4,\n",
       "   205,\n",
       "   404,\n",
       "   606,\n",
       "   25,\n",
       "   246,\n",
       "   43,\n",
       "   215,\n",
       "   281,\n",
       "   550,\n",
       "   566,\n",
       "   641,\n",
       "   140,\n",
       "   168,\n",
       "   592,\n",
       "   613,\n",
       "   320,\n",
       "   643,\n",
       "   208,\n",
       "   238,\n",
       "   103,\n",
       "   658,\n",
       "   69,\n",
       "   701,\n",
       "   290,\n",
       "   571,\n",
       "   428,\n",
       "   631,\n",
       "   292,\n",
       "   467,\n",
       "   126,\n",
       "   332,\n",
       "   28,\n",
       "   137,\n",
       "   371,\n",
       "   545,\n",
       "   124,\n",
       "   365,\n",
       "   112,\n",
       "   443,\n",
       "   20,\n",
       "   358,\n",
       "   361,\n",
       "   599,\n",
       "   191,\n",
       "   673,\n",
       "   242,\n",
       "   705,\n",
       "   268,\n",
       "   401,\n",
       "   489,\n",
       "   637,\n",
       "   321,\n",
       "   682,\n",
       "   74,\n",
       "   375,\n",
       "   681,\n",
       "   685,\n",
       "   19,\n",
       "   697,\n",
       "   302,\n",
       "   492,\n",
       "   394,\n",
       "   485,\n",
       "   272,\n",
       "   286,\n",
       "   68,\n",
       "   593,\n",
       "   298,\n",
       "   587,\n",
       "   81,\n",
       "   195,\n",
       "   54,\n",
       "   284,\n",
       "   325,\n",
       "   582,\n",
       "   421,\n",
       "   535,\n",
       "   200,\n",
       "   615,\n",
       "   425,\n",
       "   618,\n",
       "   382,\n",
       "   661,\n",
       "   167,\n",
       "   526,\n",
       "   226,\n",
       "   553,\n",
       "   470,\n",
       "   585,\n",
       "   79,\n",
       "   380,\n",
       "   85,\n",
       "   180,\n",
       "   512,\n",
       "   559,\n",
       "   92,\n",
       "   164,\n",
       "   316,\n",
       "   435,\n",
       "   219,\n",
       "   484,\n",
       "   359,\n",
       "   522,\n",
       "   247,\n",
       "   393,\n",
       "   53,\n",
       "   463],\n",
       "  [8,\n",
       "   17,\n",
       "   58,\n",
       "   61,\n",
       "   19,\n",
       "   41,\n",
       "   1,\n",
       "   34,\n",
       "   20,\n",
       "   49,\n",
       "   15,\n",
       "   30,\n",
       "   16,\n",
       "   38,\n",
       "   22,\n",
       "   45,\n",
       "   35,\n",
       "   37,\n",
       "   4,\n",
       "   40,\n",
       "   9,\n",
       "   10,\n",
       "   62,\n",
       "   63,\n",
       "   6,\n",
       "   18,\n",
       "   7,\n",
       "   25,\n",
       "   5,\n",
       "   24,\n",
       "   0,\n",
       "   28,\n",
       "   14,\n",
       "   29,\n",
       "   42,\n",
       "   47,\n",
       "   3,\n",
       "   60,\n",
       "   21,\n",
       "   36,\n",
       "   13,\n",
       "   54,\n",
       "   53,\n",
       "   59,\n",
       "   43,\n",
       "   48,\n",
       "   2,\n",
       "   32,\n",
       "   39,\n",
       "   44,\n",
       "   26,\n",
       "   33,\n",
       "   31,\n",
       "   55,\n",
       "   12,\n",
       "   50,\n",
       "   46,\n",
       "   57,\n",
       "   51,\n",
       "   52,\n",
       "   11,\n",
       "   56,\n",
       "   23,\n",
       "   27],\n",
       "  [2,\n",
       "   16,\n",
       "   4,\n",
       "   28,\n",
       "   10,\n",
       "   22,\n",
       "   33,\n",
       "   34,\n",
       "   19,\n",
       "   27,\n",
       "   12,\n",
       "   21,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   13,\n",
       "   24,\n",
       "   0,\n",
       "   3,\n",
       "   5,\n",
       "   8,\n",
       "   18,\n",
       "   26,\n",
       "   23,\n",
       "   25,\n",
       "   20,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   7,\n",
       "   9,\n",
       "   1,\n",
       "   15,\n",
       "   14,\n",
       "   17,\n",
       "   11,\n",
       "   32,\n",
       "   6,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [4,\n",
       "   9,\n",
       "   3,\n",
       "   14,\n",
       "   10,\n",
       "   29,\n",
       "   38,\n",
       "   39,\n",
       "   1,\n",
       "   26,\n",
       "   17,\n",
       "   22,\n",
       "   33,\n",
       "   36,\n",
       "   13,\n",
       "   20,\n",
       "   8,\n",
       "   35,\n",
       "   6,\n",
       "   31,\n",
       "   12,\n",
       "   15,\n",
       "   0,\n",
       "   19,\n",
       "   11,\n",
       "   28,\n",
       "   16,\n",
       "   24,\n",
       "   21,\n",
       "   34,\n",
       "   5,\n",
       "   25,\n",
       "   2,\n",
       "   7,\n",
       "   27,\n",
       "   30,\n",
       "   23,\n",
       "   37,\n",
       "   18,\n",
       "   32,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [40,\n",
       "   69,\n",
       "   72,\n",
       "   82,\n",
       "   11,\n",
       "   16,\n",
       "   31,\n",
       "   50,\n",
       "   53,\n",
       "   78,\n",
       "   49,\n",
       "   63,\n",
       "   65,\n",
       "   77,\n",
       "   27,\n",
       "   58,\n",
       "   23,\n",
       "   61,\n",
       "   26,\n",
       "   83,\n",
       "   24,\n",
       "   39,\n",
       "   37,\n",
       "   59,\n",
       "   47,\n",
       "   81,\n",
       "   48,\n",
       "   74,\n",
       "   35,\n",
       "   76,\n",
       "   84,\n",
       "   85,\n",
       "   55,\n",
       "   56,\n",
       "   14,\n",
       "   75,\n",
       "   38,\n",
       "   68,\n",
       "   10,\n",
       "   71,\n",
       "   5,\n",
       "   80,\n",
       "   3,\n",
       "   34,\n",
       "   43,\n",
       "   57,\n",
       "   32,\n",
       "   36,\n",
       "   13,\n",
       "   42,\n",
       "   6,\n",
       "   17,\n",
       "   1,\n",
       "   66,\n",
       "   29,\n",
       "   30,\n",
       "   33,\n",
       "   60,\n",
       "   22,\n",
       "   70,\n",
       "   7,\n",
       "   21,\n",
       "   15,\n",
       "   52,\n",
       "   18,\n",
       "   79,\n",
       "   41,\n",
       "   86,\n",
       "   28,\n",
       "   51,\n",
       "   54,\n",
       "   64,\n",
       "   0,\n",
       "   4,\n",
       "   19,\n",
       "   87,\n",
       "   25,\n",
       "   67,\n",
       "   12,\n",
       "   62,\n",
       "   2,\n",
       "   20,\n",
       "   8,\n",
       "   45,\n",
       "   9,\n",
       "   44,\n",
       "   46,\n",
       "   73,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [2,\n",
       "   40,\n",
       "   19,\n",
       "   33,\n",
       "   44,\n",
       "   49,\n",
       "   53,\n",
       "   54,\n",
       "   34,\n",
       "   41,\n",
       "   24,\n",
       "   43,\n",
       "   8,\n",
       "   22,\n",
       "   13,\n",
       "   29,\n",
       "   17,\n",
       "   32,\n",
       "   30,\n",
       "   55,\n",
       "   38,\n",
       "   45,\n",
       "   26,\n",
       "   42,\n",
       "   21,\n",
       "   48,\n",
       "   3,\n",
       "   11,\n",
       "   46,\n",
       "   52,\n",
       "   15,\n",
       "   25,\n",
       "   1,\n",
       "   6,\n",
       "   4,\n",
       "   14,\n",
       "   39,\n",
       "   50,\n",
       "   27,\n",
       "   51,\n",
       "   16,\n",
       "   47,\n",
       "   7,\n",
       "   31,\n",
       "   0,\n",
       "   9,\n",
       "   12,\n",
       "   28,\n",
       "   10,\n",
       "   20,\n",
       "   23,\n",
       "   37,\n",
       "   18,\n",
       "   36,\n",
       "   5,\n",
       "   35,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [42,\n",
       "   53,\n",
       "   28,\n",
       "   49,\n",
       "   1,\n",
       "   24,\n",
       "   6,\n",
       "   12,\n",
       "   2,\n",
       "   34,\n",
       "   17,\n",
       "   36,\n",
       "   20,\n",
       "   27,\n",
       "   9,\n",
       "   21,\n",
       "   19,\n",
       "   33,\n",
       "   23,\n",
       "   30,\n",
       "   41,\n",
       "   52,\n",
       "   10,\n",
       "   48,\n",
       "   38,\n",
       "   45,\n",
       "   7,\n",
       "   31,\n",
       "   0,\n",
       "   13,\n",
       "   32,\n",
       "   54,\n",
       "   35,\n",
       "   50,\n",
       "   29,\n",
       "   40,\n",
       "   3,\n",
       "   46,\n",
       "   14,\n",
       "   37,\n",
       "   16,\n",
       "   44,\n",
       "   47,\n",
       "   51,\n",
       "   5,\n",
       "   11,\n",
       "   26,\n",
       "   55,\n",
       "   15,\n",
       "   25,\n",
       "   8,\n",
       "   22,\n",
       "   4,\n",
       "   18,\n",
       "   39,\n",
       "   43,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [9,\n",
       "   10,\n",
       "   31,\n",
       "   47,\n",
       "   25,\n",
       "   43,\n",
       "   26,\n",
       "   33,\n",
       "   29,\n",
       "   30,\n",
       "   1,\n",
       "   14,\n",
       "   41,\n",
       "   44,\n",
       "   5,\n",
       "   38,\n",
       "   27,\n",
       "   37,\n",
       "   2,\n",
       "   22,\n",
       "   18,\n",
       "   42,\n",
       "   16,\n",
       "   20,\n",
       "   7,\n",
       "   12,\n",
       "   8,\n",
       "   46,\n",
       "   0,\n",
       "   34,\n",
       "   39,\n",
       "   40,\n",
       "   3,\n",
       "   6,\n",
       "   4,\n",
       "   23,\n",
       "   15,\n",
       "   28,\n",
       "   32,\n",
       "   45,\n",
       "   13,\n",
       "   17,\n",
       "   11,\n",
       "   21,\n",
       "   19,\n",
       "   35,\n",
       "   24,\n",
       "   36],\n",
       "  [1,\n",
       "   4,\n",
       "   20,\n",
       "   23,\n",
       "   15,\n",
       "   22,\n",
       "   2,\n",
       "   6,\n",
       "   0,\n",
       "   16,\n",
       "   5,\n",
       "   13,\n",
       "   3,\n",
       "   19,\n",
       "   7,\n",
       "   9,\n",
       "   8,\n",
       "   14,\n",
       "   17,\n",
       "   21,\n",
       "   11,\n",
       "   18,\n",
       "   10,\n",
       "   12,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [8,\n",
       "   21,\n",
       "   5,\n",
       "   10,\n",
       "   12,\n",
       "   19,\n",
       "   11,\n",
       "   24,\n",
       "   18,\n",
       "   20,\n",
       "   9,\n",
       "   13,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   0,\n",
       "   22,\n",
       "   3,\n",
       "   31,\n",
       "   2,\n",
       "   26,\n",
       "   15,\n",
       "   23,\n",
       "   7,\n",
       "   25,\n",
       "   1,\n",
       "   17,\n",
       "   4,\n",
       "   6,\n",
       "   14,\n",
       "   16],\n",
       "  [8,\n",
       "   20,\n",
       "   5,\n",
       "   25,\n",
       "   18,\n",
       "   22,\n",
       "   11,\n",
       "   17,\n",
       "   7,\n",
       "   23,\n",
       "   9,\n",
       "   10,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   13,\n",
       "   21,\n",
       "   0,\n",
       "   19,\n",
       "   4,\n",
       "   14,\n",
       "   30,\n",
       "   31,\n",
       "   3,\n",
       "   12,\n",
       "   6,\n",
       "   15,\n",
       "   2,\n",
       "   16,\n",
       "   1,\n",
       "   24],\n",
       "  [25,\n",
       "   32,\n",
       "   10,\n",
       "   30,\n",
       "   35,\n",
       "   36,\n",
       "   14,\n",
       "   49,\n",
       "   3,\n",
       "   47,\n",
       "   19,\n",
       "   48,\n",
       "   6,\n",
       "   18,\n",
       "   11,\n",
       "   51,\n",
       "   8,\n",
       "   23,\n",
       "   24,\n",
       "   46,\n",
       "   26,\n",
       "   34,\n",
       "   57,\n",
       "   58,\n",
       "   33,\n",
       "   39,\n",
       "   17,\n",
       "   22,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   2,\n",
       "   38,\n",
       "   21,\n",
       "   40,\n",
       "   53,\n",
       "   54,\n",
       "   13,\n",
       "   28,\n",
       "   12,\n",
       "   31,\n",
       "   16,\n",
       "   56,\n",
       "   29,\n",
       "   37,\n",
       "   1,\n",
       "   7,\n",
       "   0,\n",
       "   45,\n",
       "   42,\n",
       "   50,\n",
       "   5,\n",
       "   9,\n",
       "   15,\n",
       "   55,\n",
       "   27,\n",
       "   52,\n",
       "   4,\n",
       "   63,\n",
       "   41,\n",
       "   44,\n",
       "   20,\n",
       "   43],\n",
       "  [0,\n",
       "   29,\n",
       "   2,\n",
       "   24,\n",
       "   26,\n",
       "   28,\n",
       "   20,\n",
       "   25,\n",
       "   4,\n",
       "   27,\n",
       "   9,\n",
       "   21,\n",
       "   13,\n",
       "   17,\n",
       "   1,\n",
       "   30,\n",
       "   5,\n",
       "   23,\n",
       "   14,\n",
       "   18,\n",
       "   10,\n",
       "   11,\n",
       "   6,\n",
       "   16,\n",
       "   7,\n",
       "   19,\n",
       "   12,\n",
       "   31,\n",
       "   3,\n",
       "   22,\n",
       "   8,\n",
       "   15],\n",
       "  [19,\n",
       "   29,\n",
       "   0,\n",
       "   17,\n",
       "   22,\n",
       "   25,\n",
       "   23,\n",
       "   30,\n",
       "   1,\n",
       "   8,\n",
       "   3,\n",
       "   14,\n",
       "   18,\n",
       "   24,\n",
       "   13,\n",
       "   20,\n",
       "   2,\n",
       "   6,\n",
       "   9,\n",
       "   31,\n",
       "   15,\n",
       "   28,\n",
       "   5,\n",
       "   7,\n",
       "   10,\n",
       "   12,\n",
       "   11,\n",
       "   21,\n",
       "   4,\n",
       "   26,\n",
       "   16,\n",
       "   27],\n",
       "  [11,\n",
       "   14,\n",
       "   5,\n",
       "   22,\n",
       "   4,\n",
       "   23,\n",
       "   3,\n",
       "   6,\n",
       "   0,\n",
       "   10,\n",
       "   7,\n",
       "   15,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   1,\n",
       "   16,\n",
       "   12,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   30,\n",
       "   31,\n",
       "   8,\n",
       "   25,\n",
       "   13,\n",
       "   19,\n",
       "   2,\n",
       "   17,\n",
       "   9,\n",
       "   18],\n",
       "  [12,\n",
       "   14,\n",
       "   11,\n",
       "   28,\n",
       "   9,\n",
       "   17,\n",
       "   29,\n",
       "   30,\n",
       "   7,\n",
       "   20,\n",
       "   2,\n",
       "   6,\n",
       "   8,\n",
       "   27,\n",
       "   16,\n",
       "   19,\n",
       "   4,\n",
       "   21,\n",
       "   25,\n",
       "   31,\n",
       "   1,\n",
       "   5,\n",
       "   0,\n",
       "   18,\n",
       "   10,\n",
       "   24,\n",
       "   3,\n",
       "   23,\n",
       "   15,\n",
       "   26,\n",
       "   13,\n",
       "   22],\n",
       "  [11,\n",
       "   32,\n",
       "   5,\n",
       "   19,\n",
       "   26,\n",
       "   30,\n",
       "   18,\n",
       "   29,\n",
       "   16,\n",
       "   25,\n",
       "   31,\n",
       "   34,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   9,\n",
       "   21,\n",
       "   4,\n",
       "   10,\n",
       "   13,\n",
       "   39,\n",
       "   3,\n",
       "   36,\n",
       "   23,\n",
       "   35,\n",
       "   12,\n",
       "   17,\n",
       "   14,\n",
       "   33,\n",
       "   1,\n",
       "   28,\n",
       "   15,\n",
       "   40,\n",
       "   20,\n",
       "   37,\n",
       "   41,\n",
       "   42,\n",
       "   24,\n",
       "   43,\n",
       "   8,\n",
       "   38,\n",
       "   7,\n",
       "   22,\n",
       "   2,\n",
       "   27,\n",
       "   0,\n",
       "   6],\n",
       "  [2,\n",
       "   14,\n",
       "   15,\n",
       "   19,\n",
       "   6,\n",
       "   9,\n",
       "   4,\n",
       "   12,\n",
       "   10,\n",
       "   13,\n",
       "   11,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   0,\n",
       "   5,\n",
       "   7,\n",
       "   8,\n",
       "   3,\n",
       "   18,\n",
       "   1,\n",
       "   16,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [11,\n",
       "   19,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   8,\n",
       "   5,\n",
       "   7,\n",
       "   1,\n",
       "   22,\n",
       "   6,\n",
       "   15,\n",
       "   14,\n",
       "   20,\n",
       "   3,\n",
       "   10,\n",
       "   9,\n",
       "   13,\n",
       "   17,\n",
       "   18,\n",
       "   4,\n",
       "   12,\n",
       "   16,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [3,\n",
       "   14,\n",
       "   0,\n",
       "   12,\n",
       "   9,\n",
       "   10,\n",
       "   18,\n",
       "   19,\n",
       "   2,\n",
       "   13,\n",
       "   5,\n",
       "   11,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   4,\n",
       "   15,\n",
       "   1,\n",
       "   8,\n",
       "   7,\n",
       "   17,\n",
       "   6,\n",
       "   16,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [2,\n",
       "   13,\n",
       "   8,\n",
       "   14,\n",
       "   0,\n",
       "   4,\n",
       "   22,\n",
       "   23,\n",
       "   7,\n",
       "   17,\n",
       "   5,\n",
       "   21,\n",
       "   18,\n",
       "   19,\n",
       "   10,\n",
       "   16,\n",
       "   1,\n",
       "   15,\n",
       "   6,\n",
       "   12,\n",
       "   3,\n",
       "   20,\n",
       "   9,\n",
       "   11,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [26,\n",
       "   111,\n",
       "   7,\n",
       "   141,\n",
       "   52,\n",
       "   125,\n",
       "   86,\n",
       "   90,\n",
       "   64,\n",
       "   106,\n",
       "   3,\n",
       "   142,\n",
       "   130,\n",
       "   150,\n",
       "   20,\n",
       "   87,\n",
       "   30,\n",
       "   146,\n",
       "   55,\n",
       "   80,\n",
       "   21,\n",
       "   79,\n",
       "   37,\n",
       "   117,\n",
       "   6,\n",
       "   50,\n",
       "   15,\n",
       "   22,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   78,\n",
       "   88,\n",
       "   84,\n",
       "   104,\n",
       "   89,\n",
       "   105,\n",
       "   158,\n",
       "   159,\n",
       "   1,\n",
       "   61,\n",
       "   99,\n",
       "   102,\n",
       "   74,\n",
       "   149,\n",
       "   68,\n",
       "   95,\n",
       "   71,\n",
       "   77,\n",
       "   129,\n",
       "   138,\n",
       "   25,\n",
       "   109,\n",
       "   38,\n",
       "   76,\n",
       "   8,\n",
       "   63,\n",
       "   34,\n",
       "   126,\n",
       "   9,\n",
       "   53,\n",
       "   36,\n",
       "   49,\n",
       "   112,\n",
       "   135,\n",
       "   17,\n",
       "   45,\n",
       "   60,\n",
       "   96,\n",
       "   31,\n",
       "   116,\n",
       "   24,\n",
       "   39,\n",
       "   82,\n",
       "   100,\n",
       "   4,\n",
       "   13,\n",
       "   118,\n",
       "   120,\n",
       "   5,\n",
       "   62,\n",
       "   19,\n",
       "   139,\n",
       "   29,\n",
       "   42,\n",
       "   46,\n",
       "   98,\n",
       "   28,\n",
       "   32,\n",
       "   69,\n",
       "   115,\n",
       "   114,\n",
       "   136,\n",
       "   35,\n",
       "   91,\n",
       "   40,\n",
       "   93,\n",
       "   41,\n",
       "   148,\n",
       "   27,\n",
       "   153,\n",
       "   66,\n",
       "   107,\n",
       "   108,\n",
       "   131,\n",
       "   122,\n",
       "   124,\n",
       "   73,\n",
       "   92,\n",
       "   83,\n",
       "   119,\n",
       "   18,\n",
       "   81,\n",
       "   65,\n",
       "   145,\n",
       "   10,\n",
       "   54,\n",
       "   57,\n",
       "   70,\n",
       "   0,\n",
       "   48,\n",
       "   75,\n",
       "   123,\n",
       "   97,\n",
       "   140,\n",
       "   144,\n",
       "   152,\n",
       "   59,\n",
       "   67,\n",
       "   14,\n",
       "   121,\n",
       "   44,\n",
       "   132,\n",
       "   127,\n",
       "   128,\n",
       "   110,\n",
       "   147,\n",
       "   12,\n",
       "   16,\n",
       "   2,\n",
       "   11,\n",
       "   51,\n",
       "   85,\n",
       "   133,\n",
       "   151,\n",
       "   47,\n",
       "   113,\n",
       "   94,\n",
       "   134,\n",
       "   56,\n",
       "   101,\n",
       "   58,\n",
       "   72,\n",
       "   23,\n",
       "   33,\n",
       "   103,\n",
       "   137,\n",
       "   43,\n",
       "   143],\n",
       "  [28,\n",
       "   32,\n",
       "   6,\n",
       "   17,\n",
       "   21,\n",
       "   38,\n",
       "   12,\n",
       "   41,\n",
       "   5,\n",
       "   20,\n",
       "   2,\n",
       "   37,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   36,\n",
       "   40,\n",
       "   7,\n",
       "   46,\n",
       "   0,\n",
       "   4,\n",
       "   10,\n",
       "   30,\n",
       "   11,\n",
       "   48,\n",
       "   14,\n",
       "   44,\n",
       "   16,\n",
       "   47,\n",
       "   15,\n",
       "   23,\n",
       "   1,\n",
       "   19,\n",
       "   8,\n",
       "   45,\n",
       "   18,\n",
       "   35,\n",
       "   53,\n",
       "   54,\n",
       "   13,\n",
       "   29,\n",
       "   22,\n",
       "   24,\n",
       "   31,\n",
       "   43,\n",
       "   26,\n",
       "   42,\n",
       "   3,\n",
       "   25,\n",
       "   27,\n",
       "   55,\n",
       "   34,\n",
       "   39,\n",
       "   9,\n",
       "   33,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [11,\n",
       "   114,\n",
       "   42,\n",
       "   102,\n",
       "   16,\n",
       "   41,\n",
       "   4,\n",
       "   54,\n",
       "   88,\n",
       "   106,\n",
       "   24,\n",
       "   32,\n",
       "   25,\n",
       "   116,\n",
       "   27,\n",
       "   86,\n",
       "   35,\n",
       "   44,\n",
       "   1,\n",
       "   92,\n",
       "   59,\n",
       "   79,\n",
       "   0,\n",
       "   84,\n",
       "   14,\n",
       "   20,\n",
       "   8,\n",
       "   53,\n",
       "   65,\n",
       "   83,\n",
       "   31,\n",
       "   96,\n",
       "   17,\n",
       "   50,\n",
       "   73,\n",
       "   110,\n",
       "   39,\n",
       "   47,\n",
       "   68,\n",
       "   100,\n",
       "   6,\n",
       "   55,\n",
       "   21,\n",
       "   52,\n",
       "   75,\n",
       "   104,\n",
       "   38,\n",
       "   43,\n",
       "   22,\n",
       "   34,\n",
       "   81,\n",
       "   98,\n",
       "   10,\n",
       "   64,\n",
       "   23,\n",
       "   80,\n",
       "   30,\n",
       "   51,\n",
       "   12,\n",
       "   89,\n",
       "   95,\n",
       "   101,\n",
       "   58,\n",
       "   61,\n",
       "   28,\n",
       "   87,\n",
       "   37,\n",
       "   74,\n",
       "   3,\n",
       "   82,\n",
       "   2,\n",
       "   45,\n",
       "   7,\n",
       "   113,\n",
       "   9,\n",
       "   40,\n",
       "   13,\n",
       "   85,\n",
       "   99,\n",
       "   115,\n",
       "   76,\n",
       "   108,\n",
       "   49,\n",
       "   66,\n",
       "   15,\n",
       "   57,\n",
       "   117,\n",
       "   118,\n",
       "   19,\n",
       "   111,\n",
       "   46,\n",
       "   90,\n",
       "   29,\n",
       "   71,\n",
       "   48,\n",
       "   109,\n",
       "   56,\n",
       "   70,\n",
       "   18,\n",
       "   119,\n",
       "   93,\n",
       "   105,\n",
       "   72,\n",
       "   78,\n",
       "   63,\n",
       "   69,\n",
       "   77,\n",
       "   107,\n",
       "   5,\n",
       "   60,\n",
       "   62,\n",
       "   112,\n",
       "   26,\n",
       "   36,\n",
       "   67,\n",
       "   91,\n",
       "   94,\n",
       "   103,\n",
       "   33,\n",
       "   97,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127],\n",
       "  [31,\n",
       "   44,\n",
       "   60,\n",
       "   95,\n",
       "   23,\n",
       "   28,\n",
       "   19,\n",
       "   73,\n",
       "   25,\n",
       "   43,\n",
       "   63,\n",
       "   119,\n",
       "   13,\n",
       "   15,\n",
       "   27,\n",
       "   101,\n",
       "   7,\n",
       "   57,\n",
       "   20,\n",
       "   65,\n",
       "   6,\n",
       "   37,\n",
       "   77,\n",
       "   82,\n",
       "   54,\n",
       "   58,\n",
       "   16,\n",
       "   52,\n",
       "   69,\n",
       "   98,\n",
       "   47,\n",
       "   64,\n",
       "   35,\n",
       "   40,\n",
       "   75,\n",
       "   109,\n",
       "   5,\n",
       "   87,\n",
       "   22,\n",
       "   59,\n",
       "   100,\n",
       "   104,\n",
       "   71,\n",
       "   102,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   30,\n",
       "   91,\n",
       "   50,\n",
       "   92,\n",
       "   17,\n",
       "   80,\n",
       "   3,\n",
       "   26,\n",
       "   88,\n",
       "   97,\n",
       "   38,\n",
       "   49,\n",
       "   34,\n",
       "   84,\n",
       "   74,\n",
       "   86,\n",
       "   53,\n",
       "   85,\n",
       "   8,\n",
       "   105,\n",
       "   90,\n",
       "   99,\n",
       "   12,\n",
       "   78,\n",
       "   108,\n",
       "   117,\n",
       "   0,\n",
       "   120,\n",
       "   96,\n",
       "   110,\n",
       "   67,\n",
       "   68,\n",
       "   29,\n",
       "   113,\n",
       "   10,\n",
       "   79,\n",
       "   36,\n",
       "   45,\n",
       "   41,\n",
       "   89,\n",
       "   4,\n",
       "   66,\n",
       "   107,\n",
       "   125,\n",
       "   18,\n",
       "   81,\n",
       "   72,\n",
       "   112,\n",
       "   24,\n",
       "   111,\n",
       "   21,\n",
       "   39,\n",
       "   51,\n",
       "   55,\n",
       "   126,\n",
       "   127,\n",
       "   9,\n",
       "   46,\n",
       "   42,\n",
       "   93,\n",
       "   14,\n",
       "   56,\n",
       "   94,\n",
       "   114,\n",
       "   61,\n",
       "   106,\n",
       "   83,\n",
       "   116,\n",
       "   2,\n",
       "   62,\n",
       "   48,\n",
       "   76,\n",
       "   11,\n",
       "   103,\n",
       "   32,\n",
       "   115,\n",
       "   1,\n",
       "   118,\n",
       "   33,\n",
       "   70],\n",
       "  [3,\n",
       "   86,\n",
       "   76,\n",
       "   96,\n",
       "   74,\n",
       "   87,\n",
       "   0,\n",
       "   29,\n",
       "   26,\n",
       "   66,\n",
       "   7,\n",
       "   44,\n",
       "   60,\n",
       "   92,\n",
       "   30,\n",
       "   81,\n",
       "   37,\n",
       "   73,\n",
       "   102,\n",
       "   109,\n",
       "   64,\n",
       "   70,\n",
       "   48,\n",
       "   67,\n",
       "   28,\n",
       "   52,\n",
       "   15,\n",
       "   63,\n",
       "   22,\n",
       "   72,\n",
       "   33,\n",
       "   97,\n",
       "   25,\n",
       "   107,\n",
       "   71,\n",
       "   106,\n",
       "   21,\n",
       "   35,\n",
       "   32,\n",
       "   45,\n",
       "   6,\n",
       "   89,\n",
       "   16,\n",
       "   59,\n",
       "   58,\n",
       "   91,\n",
       "   69,\n",
       "   83,\n",
       "   77,\n",
       "   82,\n",
       "   36,\n",
       "   78,\n",
       "   93,\n",
       "   101,\n",
       "   10,\n",
       "   65,\n",
       "   19,\n",
       "   41,\n",
       "   103,\n",
       "   108,\n",
       "   31,\n",
       "   42,\n",
       "   11,\n",
       "   62,\n",
       "   85,\n",
       "   98,\n",
       "   38,\n",
       "   79,\n",
       "   14,\n",
       "   43,\n",
       "   110,\n",
       "   111,\n",
       "   17,\n",
       "   51,\n",
       "   34,\n",
       "   100,\n",
       "   24,\n",
       "   90,\n",
       "   40,\n",
       "   49,\n",
       "   61,\n",
       "   95,\n",
       "   23,\n",
       "   27,\n",
       "   47,\n",
       "   99,\n",
       "   4,\n",
       "   55,\n",
       "   50,\n",
       "   68,\n",
       "   46,\n",
       "   75,\n",
       "   12,\n",
       "   56,\n",
       "   1,\n",
       "   84,\n",
       "   80,\n",
       "   94,\n",
       "   9,\n",
       "   54,\n",
       "   57,\n",
       "   104,\n",
       "   88,\n",
       "   105,\n",
       "   2,\n",
       "   13,\n",
       "   18,\n",
       "   39,\n",
       "   8,\n",
       "   53,\n",
       "   5,\n",
       "   20],\n",
       "  [28,\n",
       "   32,\n",
       "   47,\n",
       "   66,\n",
       "   42,\n",
       "   52,\n",
       "   13,\n",
       "   37,\n",
       "   11,\n",
       "   61,\n",
       "   41,\n",
       "   46,\n",
       "   19,\n",
       "   29,\n",
       "   10,\n",
       "   49,\n",
       "   17,\n",
       "   57,\n",
       "   7,\n",
       "   22,\n",
       "   4,\n",
       "   63,\n",
       "   15,\n",
       "   62,\n",
       "   8,\n",
       "   24,\n",
       "   9,\n",
       "   53,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   30,\n",
       "   51,\n",
       "   58,\n",
       "   71,\n",
       "   1,\n",
       "   21,\n",
       "   20,\n",
       "   39,\n",
       "   2,\n",
       "   40,\n",
       "   35,\n",
       "   65,\n",
       "   25,\n",
       "   54,\n",
       "   43,\n",
       "   50,\n",
       "   26,\n",
       "   60,\n",
       "   38,\n",
       "   45,\n",
       "   31,\n",
       "   36,\n",
       "   3,\n",
       "   33,\n",
       "   5,\n",
       "   34,\n",
       "   48,\n",
       "   64,\n",
       "   12,\n",
       "   59,\n",
       "   6,\n",
       "   56,\n",
       "   0,\n",
       "   18,\n",
       "   44,\n",
       "   55,\n",
       "   16,\n",
       "   23,\n",
       "   14,\n",
       "   27,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [1,\n",
       "   71,\n",
       "   69,\n",
       "   81,\n",
       "   33,\n",
       "   39,\n",
       "   19,\n",
       "   58,\n",
       "   74,\n",
       "   78,\n",
       "   10,\n",
       "   31,\n",
       "   51,\n",
       "   55,\n",
       "   14,\n",
       "   24,\n",
       "   8,\n",
       "   38,\n",
       "   9,\n",
       "   43,\n",
       "   40,\n",
       "   47,\n",
       "   13,\n",
       "   61,\n",
       "   21,\n",
       "   54,\n",
       "   11,\n",
       "   56,\n",
       "   23,\n",
       "   34,\n",
       "   16,\n",
       "   27,\n",
       "   17,\n",
       "   20,\n",
       "   12,\n",
       "   45,\n",
       "   50,\n",
       "   63,\n",
       "   36,\n",
       "   77,\n",
       "   3,\n",
       "   52,\n",
       "   7,\n",
       "   64,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   59,\n",
       "   65,\n",
       "   73,\n",
       "   80,\n",
       "   18,\n",
       "   37,\n",
       "   86,\n",
       "   87,\n",
       "   66,\n",
       "   67,\n",
       "   28,\n",
       "   30,\n",
       "   22,\n",
       "   76,\n",
       "   41,\n",
       "   53,\n",
       "   0,\n",
       "   72,\n",
       "   29,\n",
       "   62,\n",
       "   57,\n",
       "   70,\n",
       "   46,\n",
       "   60,\n",
       "   4,\n",
       "   35,\n",
       "   32,\n",
       "   42,\n",
       "   15,\n",
       "   26,\n",
       "   48,\n",
       "   49,\n",
       "   25,\n",
       "   75,\n",
       "   5,\n",
       "   68,\n",
       "   2,\n",
       "   44,\n",
       "   6,\n",
       "   79,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [65,\n",
       "   67,\n",
       "   37,\n",
       "   82,\n",
       "   29,\n",
       "   68,\n",
       "   100,\n",
       "   102,\n",
       "   62,\n",
       "   90,\n",
       "   81,\n",
       "   94,\n",
       "   84,\n",
       "   121,\n",
       "   38,\n",
       "   99,\n",
       "   8,\n",
       "   64,\n",
       "   51,\n",
       "   110,\n",
       "   7,\n",
       "   39,\n",
       "   42,\n",
       "   113,\n",
       "   25,\n",
       "   59,\n",
       "   41,\n",
       "   50,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   2,\n",
       "   19,\n",
       "   14,\n",
       "   117,\n",
       "   0,\n",
       "   24,\n",
       "   74,\n",
       "   85,\n",
       "   3,\n",
       "   106,\n",
       "   6,\n",
       "   116,\n",
       "   60,\n",
       "   77,\n",
       "   28,\n",
       "   45,\n",
       "   107,\n",
       "   118,\n",
       "   46,\n",
       "   95,\n",
       "   53,\n",
       "   61,\n",
       "   43,\n",
       "   79,\n",
       "   44,\n",
       "   52,\n",
       "   98,\n",
       "   109,\n",
       "   17,\n",
       "   47,\n",
       "   13,\n",
       "   55,\n",
       "   15,\n",
       "   21,\n",
       "   36,\n",
       "   127,\n",
       "   97,\n",
       "   101,\n",
       "   49,\n",
       "   78,\n",
       "   4,\n",
       "   88,\n",
       "   30,\n",
       "   34,\n",
       "   66,\n",
       "   114,\n",
       "   69,\n",
       "   122,\n",
       "   27,\n",
       "   119,\n",
       "   31,\n",
       "   86,\n",
       "   71,\n",
       "   105,\n",
       "   104,\n",
       "   120,\n",
       "   9,\n",
       "   22,\n",
       "   72,\n",
       "   92,\n",
       "   48,\n",
       "   63,\n",
       "   33,\n",
       "   103,\n",
       "   32,\n",
       "   108,\n",
       "   80,\n",
       "   96,\n",
       "   40,\n",
       "   89,\n",
       "   54,\n",
       "   87,\n",
       "   75,\n",
       "   115,\n",
       "   76,\n",
       "   112,\n",
       "   12,\n",
       "   58,\n",
       "   11,\n",
       "   18,\n",
       "   16,\n",
       "   83,\n",
       "   73,\n",
       "   91,\n",
       "   26,\n",
       "   70,\n",
       "   20,\n",
       "   111,\n",
       "   10,\n",
       "   93,\n",
       "   1,\n",
       "   57,\n",
       "   5,\n",
       "   56,\n",
       "   23,\n",
       "   35],\n",
       "  [17,\n",
       "   37,\n",
       "   1,\n",
       "   28,\n",
       "   12,\n",
       "   23,\n",
       "   11,\n",
       "   29,\n",
       "   34,\n",
       "   36,\n",
       "   2,\n",
       "   13,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   3,\n",
       "   39,\n",
       "   31,\n",
       "   33,\n",
       "   0,\n",
       "   9,\n",
       "   30,\n",
       "   42,\n",
       "   10,\n",
       "   21,\n",
       "   5,\n",
       "   35,\n",
       "   8,\n",
       "   27,\n",
       "   16,\n",
       "   41,\n",
       "   7,\n",
       "   24,\n",
       "   4,\n",
       "   19,\n",
       "   26,\n",
       "   32,\n",
       "   6,\n",
       "   25,\n",
       "   18,\n",
       "   20,\n",
       "   14,\n",
       "   38,\n",
       "   15,\n",
       "   43,\n",
       "   22,\n",
       "   40],\n",
       "  [72,\n",
       "   77,\n",
       "   41,\n",
       "   70,\n",
       "   42,\n",
       "   56,\n",
       "   38,\n",
       "   61,\n",
       "   45,\n",
       "   57,\n",
       "   43,\n",
       "   79,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   4,\n",
       "   60,\n",
       "   84,\n",
       "   88,\n",
       "   28,\n",
       "   69,\n",
       "   20,\n",
       "   66,\n",
       "   15,\n",
       "   49,\n",
       "   80,\n",
       "   81,\n",
       "   19,\n",
       "   37,\n",
       "   2,\n",
       "   23,\n",
       "   51,\n",
       "   59,\n",
       "   1,\n",
       "   74,\n",
       "   3,\n",
       "   7,\n",
       "   8,\n",
       "   53,\n",
       "   11,\n",
       "   32,\n",
       "   82,\n",
       "   89,\n",
       "   39,\n",
       "   67,\n",
       "   26,\n",
       "   76,\n",
       "   17,\n",
       "   44,\n",
       "   14,\n",
       "   63,\n",
       "   36,\n",
       "   40,\n",
       "   21,\n",
       "   25,\n",
       "   52,\n",
       "   68,\n",
       "   33,\n",
       "   35,\n",
       "   9,\n",
       "   30,\n",
       "   46,\n",
       "   55,\n",
       "   31,\n",
       "   65,\n",
       "   6,\n",
       "   73,\n",
       "   24,\n",
       "   47,\n",
       "   22,\n",
       "   27,\n",
       "   0,\n",
       "   16,\n",
       "   54,\n",
       "   62,\n",
       "   29,\n",
       "   34,\n",
       "   64,\n",
       "   85,\n",
       "   10,\n",
       "   71,\n",
       "   87,\n",
       "   90,\n",
       "   13,\n",
       "   78,\n",
       "   83,\n",
       "   86,\n",
       "   50,\n",
       "   58,\n",
       "   12,\n",
       "   75,\n",
       "   48,\n",
       "   91,\n",
       "   5,\n",
       "   18],\n",
       "  [24,\n",
       "   49,\n",
       "   45,\n",
       "   51,\n",
       "   17,\n",
       "   23,\n",
       "   7,\n",
       "   14,\n",
       "   22,\n",
       "   50,\n",
       "   30,\n",
       "   57,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   54,\n",
       "   55,\n",
       "   12,\n",
       "   59,\n",
       "   4,\n",
       "   56,\n",
       "   2,\n",
       "   58,\n",
       "   6,\n",
       "   29,\n",
       "   28,\n",
       "   40,\n",
       "   42,\n",
       "   53,\n",
       "   9,\n",
       "   41,\n",
       "   19,\n",
       "   47,\n",
       "   32,\n",
       "   43,\n",
       "   13,\n",
       "   48,\n",
       "   8,\n",
       "   31,\n",
       "   25,\n",
       "   46,\n",
       "   20,\n",
       "   33,\n",
       "   0,\n",
       "   3,\n",
       "   39,\n",
       "   52,\n",
       "   21,\n",
       "   35,\n",
       "   16,\n",
       "   34,\n",
       "   1,\n",
       "   44,\n",
       "   11,\n",
       "   36,\n",
       "   18,\n",
       "   38,\n",
       "   10,\n",
       "   26,\n",
       "   15,\n",
       "   37,\n",
       "   5,\n",
       "   27],\n",
       "  [31,\n",
       "   35,\n",
       "   28,\n",
       "   47,\n",
       "   2,\n",
       "   33,\n",
       "   39,\n",
       "   52,\n",
       "   7,\n",
       "   40,\n",
       "   0,\n",
       "   50,\n",
       "   9,\n",
       "   38,\n",
       "   14,\n",
       "   51,\n",
       "   22,\n",
       "   44,\n",
       "   12,\n",
       "   25,\n",
       "   17,\n",
       "   34,\n",
       "   15,\n",
       "   54,\n",
       "   23,\n",
       "   36,\n",
       "   32,\n",
       "   41,\n",
       "   37,\n",
       "   49,\n",
       "   8,\n",
       "   29,\n",
       "   1,\n",
       "   16,\n",
       "   27,\n",
       "   55,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   11,\n",
       "   30,\n",
       "   48,\n",
       "   43,\n",
       "   45,\n",
       "   13,\n",
       "   21,\n",
       "   19,\n",
       "   53,\n",
       "   20,\n",
       "   46,\n",
       "   24,\n",
       "   42,\n",
       "   18,\n",
       "   26,\n",
       "   5,\n",
       "   10,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [36,\n",
       "   66,\n",
       "   33,\n",
       "   44,\n",
       "   20,\n",
       "   23,\n",
       "   59,\n",
       "   69,\n",
       "   27,\n",
       "   53,\n",
       "   0,\n",
       "   2,\n",
       "   31,\n",
       "   48,\n",
       "   42,\n",
       "   52,\n",
       "   43,\n",
       "   55,\n",
       "   13,\n",
       "   29,\n",
       "   32,\n",
       "   58,\n",
       "   6,\n",
       "   39,\n",
       "   5,\n",
       "   12,\n",
       "   11,\n",
       "   15,\n",
       "   3,\n",
       "   54,\n",
       "   8,\n",
       "   63,\n",
       "   25,\n",
       "   37,\n",
       "   62,\n",
       "   70,\n",
       "   21,\n",
       "   61,\n",
       "   17,\n",
       "   40,\n",
       "   34,\n",
       "   56,\n",
       "   24,\n",
       "   64,\n",
       "   46,\n",
       "   60,\n",
       "   18,\n",
       "   45,\n",
       "   38,\n",
       "   67,\n",
       "   22,\n",
       "   51,\n",
       "   9,\n",
       "   19,\n",
       "   1,\n",
       "   50,\n",
       "   26,\n",
       "   68,\n",
       "   7,\n",
       "   30,\n",
       "   41,\n",
       "   49,\n",
       "   35,\n",
       "   65,\n",
       "   4,\n",
       "   71,\n",
       "   10,\n",
       "   57,\n",
       "   16,\n",
       "   28,\n",
       "   14,\n",
       "   47,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [28,\n",
       "   31,\n",
       "   20,\n",
       "   32,\n",
       "   13,\n",
       "   29,\n",
       "   33,\n",
       "   40,\n",
       "   3,\n",
       "   5,\n",
       "   4,\n",
       "   43,\n",
       "   14,\n",
       "   19,\n",
       "   9,\n",
       "   17,\n",
       "   34,\n",
       "   39,\n",
       "   25,\n",
       "   44,\n",
       "   7,\n",
       "   12,\n",
       "   46,\n",
       "   47,\n",
       "   21,\n",
       "   27,\n",
       "   15,\n",
       "   35,\n",
       "   1,\n",
       "   11,\n",
       "   6,\n",
       "   8,\n",
       "   2,\n",
       "   22,\n",
       "   16,\n",
       "   24,\n",
       "   38,\n",
       "   41,\n",
       "   37,\n",
       "   42,\n",
       "   23,\n",
       "   36,\n",
       "   0,\n",
       "   18,\n",
       "   26,\n",
       "   30,\n",
       "   10,\n",
       "   45],\n",
       "  [15,\n",
       "   25,\n",
       "   14,\n",
       "   65,\n",
       "   21,\n",
       "   47,\n",
       "   38,\n",
       "   39,\n",
       "   10,\n",
       "   35,\n",
       "   4,\n",
       "   11,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   8,\n",
       "   57,\n",
       "   3,\n",
       "   70,\n",
       "   0,\n",
       "   58,\n",
       "   44,\n",
       "   64,\n",
       "   34,\n",
       "   43,\n",
       "   26,\n",
       "   52,\n",
       "   60,\n",
       "   62,\n",
       "   1,\n",
       "   54,\n",
       "   27,\n",
       "   32,\n",
       "   40,\n",
       "   49,\n",
       "   50,\n",
       "   61,\n",
       "   48,\n",
       "   51,\n",
       "   31,\n",
       "   37,\n",
       "   29,\n",
       "   42,\n",
       "   7,\n",
       "   63,\n",
       "   5,\n",
       "   24,\n",
       "   6,\n",
       "   22,\n",
       "   18,\n",
       "   71,\n",
       "   20,\n",
       "   55,\n",
       "   33,\n",
       "   45,\n",
       "   2,\n",
       "   41,\n",
       "   19,\n",
       "   56,\n",
       "   17,\n",
       "   30,\n",
       "   9,\n",
       "   53,\n",
       "   23,\n",
       "   36,\n",
       "   16,\n",
       "   59,\n",
       "   12,\n",
       "   28,\n",
       "   13,\n",
       "   46,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [37,\n",
       "   60,\n",
       "   5,\n",
       "   30,\n",
       "   33,\n",
       "   46,\n",
       "   48,\n",
       "   56,\n",
       "   49,\n",
       "   59,\n",
       "   24,\n",
       "   69,\n",
       "   25,\n",
       "   51,\n",
       "   1,\n",
       "   16,\n",
       "   31,\n",
       "   63,\n",
       "   39,\n",
       "   67,\n",
       "   28,\n",
       "   50,\n",
       "   70,\n",
       "   71,\n",
       "   57,\n",
       "   64,\n",
       "   42,\n",
       "   52,\n",
       "   38,\n",
       "   54,\n",
       "   2,\n",
       "   43,\n",
       "   3,\n",
       "   14,\n",
       "   40,\n",
       "   45,\n",
       "   6,\n",
       "   23,\n",
       "   47,\n",
       "   65,\n",
       "   18,\n",
       "   61,\n",
       "   20,\n",
       "   41,\n",
       "   19,\n",
       "   36,\n",
       "   10,\n",
       "   58,\n",
       "   22,\n",
       "   35,\n",
       "   12,\n",
       "   27,\n",
       "   9,\n",
       "   13,\n",
       "   21,\n",
       "   29,\n",
       "   15,\n",
       "   26,\n",
       "   4,\n",
       "   66,\n",
       "   32,\n",
       "   55,\n",
       "   8,\n",
       "   62,\n",
       "   11,\n",
       "   53,\n",
       "   17,\n",
       "   44,\n",
       "   0,\n",
       "   34,\n",
       "   7,\n",
       "   68,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [50,\n",
       "   56,\n",
       "   4,\n",
       "   48,\n",
       "   46,\n",
       "   66,\n",
       "   1,\n",
       "   11,\n",
       "   32,\n",
       "   81,\n",
       "   75,\n",
       "   93,\n",
       "   5,\n",
       "   22,\n",
       "   26,\n",
       "   90,\n",
       "   23,\n",
       "   47,\n",
       "   20,\n",
       "   25,\n",
       "   3,\n",
       "   31,\n",
       "   33,\n",
       "   97,\n",
       "   54,\n",
       "   65,\n",
       "   72,\n",
       "   73,\n",
       "   29,\n",
       "   35,\n",
       "   16,\n",
       "   82,\n",
       "   39,\n",
       "   99,\n",
       "   17,\n",
       "   79,\n",
       "   40,\n",
       "   77,\n",
       "   7,\n",
       "   76,\n",
       "   24,\n",
       "   68,\n",
       "   27,\n",
       "   58,\n",
       "   13,\n",
       "   45,\n",
       "   15,\n",
       "   67,\n",
       "   59,\n",
       "   64,\n",
       "   55,\n",
       "   60,\n",
       "   12,\n",
       "   21,\n",
       "   18,\n",
       "   85,\n",
       "   43,\n",
       "   71,\n",
       "   0,\n",
       "   101,\n",
       "   70,\n",
       "   96,\n",
       "   44,\n",
       "   49,\n",
       "   8,\n",
       "   94,\n",
       "   36,\n",
       "   100,\n",
       "   87,\n",
       "   89,\n",
       "   92,\n",
       "   95,\n",
       "   52,\n",
       "   78,\n",
       "   63,\n",
       "   83,\n",
       "   19,\n",
       "   53,\n",
       "   10,\n",
       "   51,\n",
       "   34,\n",
       "   61,\n",
       "   62,\n",
       "   86,\n",
       "   9,\n",
       "   84,\n",
       "   69,\n",
       "   74,\n",
       "   28,\n",
       "   37,\n",
       "   57,\n",
       "   98,\n",
       "   41,\n",
       "   42,\n",
       "   6,\n",
       "   91,\n",
       "   14,\n",
       "   80,\n",
       "   30,\n",
       "   38,\n",
       "   2,\n",
       "   88,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111],\n",
       "  [49,\n",
       "   54,\n",
       "   38,\n",
       "   58,\n",
       "   73,\n",
       "   75,\n",
       "   55,\n",
       "   69,\n",
       "   32,\n",
       "   66,\n",
       "   1,\n",
       "   64,\n",
       "   7,\n",
       "   57,\n",
       "   78,\n",
       "   79,\n",
       "   39,\n",
       "   62,\n",
       "   53,\n",
       "   60,\n",
       "   5,\n",
       "   50,\n",
       "   25,\n",
       "   68,\n",
       "   43,\n",
       "   65,\n",
       "   13,\n",
       "   72,\n",
       "   15,\n",
       "   47,\n",
       "   12,\n",
       "   36,\n",
       "   23,\n",
       "   77,\n",
       "   21,\n",
       "   22,\n",
       "   9,\n",
       "   28,\n",
       "   24,\n",
       "   59,\n",
       "   10,\n",
       "   46,\n",
       "   51,\n",
       "   70,\n",
       "   17,\n",
       "   34,\n",
       "   4,\n",
       "   74,\n",
       "   16,\n",
       "   20,\n",
       "   29,\n",
       "   42,\n",
       "   35,\n",
       "   76,\n",
       "   56,\n",
       "   71,\n",
       "   11,\n",
       "   44,\n",
       "   40,\n",
       "   48,\n",
       "   3,\n",
       "   8,\n",
       "   14,\n",
       "   26,\n",
       "   19,\n",
       "   45,\n",
       "   33,\n",
       "   37,\n",
       "   0,\n",
       "   52,\n",
       "   30,\n",
       "   41,\n",
       "   27,\n",
       "   67,\n",
       "   2,\n",
       "   61,\n",
       "   31,\n",
       "   63,\n",
       "   6,\n",
       "   18],\n",
       "  [15,\n",
       "   38,\n",
       "   19,\n",
       "   25,\n",
       "   6,\n",
       "   44,\n",
       "   4,\n",
       "   21,\n",
       "   35,\n",
       "   48,\n",
       "   27,\n",
       "   46,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   7,\n",
       "   32,\n",
       "   26,\n",
       "   47,\n",
       "   11,\n",
       "   14,\n",
       "   54,\n",
       "   55,\n",
       "   3,\n",
       "   16,\n",
       "   5,\n",
       "   9,\n",
       "   28,\n",
       "   49,\n",
       "   2,\n",
       "   45,\n",
       "   29,\n",
       "   40,\n",
       "   1,\n",
       "   43,\n",
       "   23,\n",
       "   39,\n",
       "   36,\n",
       "   42,\n",
       "   12,\n",
       "   17,\n",
       "   0,\n",
       "   13,\n",
       "   30,\n",
       "   34,\n",
       "   8,\n",
       "   31,\n",
       "   20,\n",
       "   24,\n",
       "   33,\n",
       "   41,\n",
       "   18,\n",
       "   37,\n",
       "   10,\n",
       "   22,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [14,\n",
       "   24,\n",
       "   52,\n",
       "   55,\n",
       "   16,\n",
       "   17,\n",
       "   19,\n",
       "   26,\n",
       "   29,\n",
       "   37,\n",
       "   3,\n",
       "   13,\n",
       "   25,\n",
       "   49,\n",
       "   2,\n",
       "   27,\n",
       "   28,\n",
       "   43,\n",
       "   1,\n",
       "   53,\n",
       "   22,\n",
       "   39,\n",
       "   9,\n",
       "   36,\n",
       "   18,\n",
       "   35,\n",
       "   21,\n",
       "   48,\n",
       "   0,\n",
       "   34,\n",
       "   23,\n",
       "   32,\n",
       "   12,\n",
       "   42,\n",
       "   15,\n",
       "   20,\n",
       "   8,\n",
       "   30,\n",
       "   31,\n",
       "   41,\n",
       "   47,\n",
       "   54,\n",
       "   33,\n",
       "   50,\n",
       "   7,\n",
       "   40,\n",
       "   5,\n",
       "   45,\n",
       "   10,\n",
       "   38,\n",
       "   4,\n",
       "   51,\n",
       "   6,\n",
       "   44,\n",
       "   11,\n",
       "   46,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [28,\n",
       "   41,\n",
       "   16,\n",
       "   24,\n",
       "   9,\n",
       "   26,\n",
       "   8,\n",
       "   44,\n",
       "   30,\n",
       "   48,\n",
       "   0,\n",
       "   29,\n",
       "   7,\n",
       "   52,\n",
       "   13,\n",
       "   25,\n",
       "   6,\n",
       "   22,\n",
       "   17,\n",
       "   20,\n",
       "   36,\n",
       "   39,\n",
       "   54,\n",
       "   55,\n",
       "   35,\n",
       "   46,\n",
       "   38,\n",
       "   49,\n",
       "   19,\n",
       "   53,\n",
       "   3,\n",
       "   11,\n",
       "   23,\n",
       "   51,\n",
       "   18,\n",
       "   34,\n",
       "   43,\n",
       "   47,\n",
       "   37,\n",
       "   40,\n",
       "   21,\n",
       "   42,\n",
       "   1,\n",
       "   27,\n",
       "   2,\n",
       "   12,\n",
       "   15,\n",
       "   33,\n",
       "   4,\n",
       "   45,\n",
       "   14,\n",
       "   32,\n",
       "   31,\n",
       "   50,\n",
       "   5,\n",
       "   10,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [7,\n",
       "   21,\n",
       "   9,\n",
       "   16,\n",
       "   18,\n",
       "   20,\n",
       "   10,\n",
       "   19,\n",
       "   12,\n",
       "   14,\n",
       "   13,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   0,\n",
       "   15,\n",
       "   1,\n",
       "   11,\n",
       "   6,\n",
       "   8,\n",
       "   29,\n",
       "   30,\n",
       "   4,\n",
       "   23,\n",
       "   5,\n",
       "   31,\n",
       "   2,\n",
       "   17,\n",
       "   3,\n",
       "   22],\n",
       "  [11,\n",
       "   44,\n",
       "   7,\n",
       "   54,\n",
       "   6,\n",
       "   15,\n",
       "   29,\n",
       "   59,\n",
       "   17,\n",
       "   18,\n",
       "   25,\n",
       "   46,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   55,\n",
       "   58,\n",
       "   5,\n",
       "   50,\n",
       "   36,\n",
       "   56,\n",
       "   23,\n",
       "   40,\n",
       "   19,\n",
       "   31,\n",
       "   27,\n",
       "   28,\n",
       "   22,\n",
       "   32,\n",
       "   42,\n",
       "   51,\n",
       "   1,\n",
       "   30,\n",
       "   33,\n",
       "   35,\n",
       "   16,\n",
       "   26,\n",
       "   24,\n",
       "   37,\n",
       "   38,\n",
       "   49,\n",
       "   45,\n",
       "   47,\n",
       "   21,\n",
       "   52,\n",
       "   0,\n",
       "   13,\n",
       "   4,\n",
       "   9,\n",
       "   3,\n",
       "   10,\n",
       "   14,\n",
       "   41,\n",
       "   43,\n",
       "   48,\n",
       "   12,\n",
       "   57,\n",
       "   2,\n",
       "   34,\n",
       "   20,\n",
       "   53,\n",
       "   8,\n",
       "   39],\n",
       "  [55,\n",
       "   59,\n",
       "   42,\n",
       "   69,\n",
       "   4,\n",
       "   62,\n",
       "   6,\n",
       "   29,\n",
       "   0,\n",
       "   63,\n",
       "   43,\n",
       "   50,\n",
       "   27,\n",
       "   51,\n",
       "   81,\n",
       "   82,\n",
       "   22,\n",
       "   31,\n",
       "   20,\n",
       "   61,\n",
       "   26,\n",
       "   70,\n",
       "   36,\n",
       "   56,\n",
       "   32,\n",
       "   40,\n",
       "   8,\n",
       "   47,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   33,\n",
       "   68,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   54,\n",
       "   65,\n",
       "   72,\n",
       "   10,\n",
       "   30,\n",
       "   39,\n",
       "   57,\n",
       "   60,\n",
       "   76,\n",
       "   5,\n",
       "   45,\n",
       "   25,\n",
       "   67,\n",
       "   2,\n",
       "   12,\n",
       "   58,\n",
       "   73,\n",
       "   9,\n",
       "   23,\n",
       "   28,\n",
       "   35,\n",
       "   75,\n",
       "   87,\n",
       "   16,\n",
       "   44,\n",
       "   52,\n",
       "   64,\n",
       "   53,\n",
       "   80,\n",
       "   21,\n",
       "   77,\n",
       "   48,\n",
       "   79,\n",
       "   34,\n",
       "   74,\n",
       "   46,\n",
       "   49,\n",
       "   1,\n",
       "   7,\n",
       "   13,\n",
       "   41,\n",
       "   3,\n",
       "   14,\n",
       "   24,\n",
       "   78,\n",
       "   18,\n",
       "   38,\n",
       "   66,\n",
       "   71,\n",
       "   11,\n",
       "   37,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [50,\n",
       "   74,\n",
       "   36,\n",
       "   52,\n",
       "   5,\n",
       "   34,\n",
       "   2,\n",
       "   41,\n",
       "   14,\n",
       "   60,\n",
       "   11,\n",
       "   30,\n",
       "   1,\n",
       "   26,\n",
       "   0,\n",
       "   37,\n",
       "   15,\n",
       "   79,\n",
       "   55,\n",
       "   68,\n",
       "   31,\n",
       "   57,\n",
       "   63,\n",
       "   72,\n",
       "   21,\n",
       "   22,\n",
       "   29,\n",
       "   62,\n",
       "   59,\n",
       "   84,\n",
       "   4,\n",
       "   13,\n",
       "   9,\n",
       "   54,\n",
       "   6,\n",
       "   45,\n",
       "   48,\n",
       "   81,\n",
       "   39,\n",
       "   80,\n",
       "   32,\n",
       "   61,\n",
       "   17,\n",
       "   75,\n",
       "   8,\n",
       "   56,\n",
       "   10,\n",
       "   23,\n",
       "   20,\n",
       "   51,\n",
       "   77,\n",
       "   82,\n",
       "   12,\n",
       "   46,\n",
       "   7,\n",
       "   73,\n",
       "   35,\n",
       "   64,\n",
       "   3,\n",
       "   40,\n",
       "   44,\n",
       "   71,\n",
       "   25,\n",
       "   47,\n",
       "   18,\n",
       "   67,\n",
       "   66,\n",
       "   85,\n",
       "   33,\n",
       "   65,\n",
       "   86,\n",
       "   87,\n",
       "   42,\n",
       "   70,\n",
       "   16,\n",
       "   27,\n",
       "   49,\n",
       "   83,\n",
       "   43,\n",
       "   58,\n",
       "   19,\n",
       "   78,\n",
       "   69,\n",
       "   76,\n",
       "   24,\n",
       "   38,\n",
       "   28,\n",
       "   53,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [0,\n",
       "   68,\n",
       "   84,\n",
       "   115,\n",
       "   62,\n",
       "   72,\n",
       "   3,\n",
       "   6,\n",
       "   55,\n",
       "   87,\n",
       "   12,\n",
       "   59,\n",
       "   14,\n",
       "   51,\n",
       "   26,\n",
       "   58,\n",
       "   73,\n",
       "   96,\n",
       "   28,\n",
       "   103,\n",
       "   31,\n",
       "   39,\n",
       "   44,\n",
       "   76,\n",
       "   10,\n",
       "   35,\n",
       "   20,\n",
       "   74,\n",
       "   32,\n",
       "   80,\n",
       "   47,\n",
       "   79,\n",
       "   17,\n",
       "   66,\n",
       "   67,\n",
       "   116,\n",
       "   25,\n",
       "   94,\n",
       "   24,\n",
       "   114,\n",
       "   82,\n",
       "   107,\n",
       "   1,\n",
       "   43,\n",
       "   90,\n",
       "   111,\n",
       "   78,\n",
       "   99,\n",
       "   5,\n",
       "   42,\n",
       "   105,\n",
       "   117,\n",
       "   33,\n",
       "   77,\n",
       "   65,\n",
       "   89,\n",
       "   29,\n",
       "   37,\n",
       "   2,\n",
       "   104,\n",
       "   60,\n",
       "   100,\n",
       "   48,\n",
       "   70,\n",
       "   8,\n",
       "   86,\n",
       "   56,\n",
       "   75,\n",
       "   38,\n",
       "   69,\n",
       "   41,\n",
       "   81,\n",
       "   4,\n",
       "   85,\n",
       "   11,\n",
       "   112,\n",
       "   16,\n",
       "   71,\n",
       "   15,\n",
       "   92,\n",
       "   91,\n",
       "   98,\n",
       "   18,\n",
       "   88,\n",
       "   21,\n",
       "   30,\n",
       "   63,\n",
       "   110,\n",
       "   53,\n",
       "   109,\n",
       "   61,\n",
       "   64,\n",
       "   13,\n",
       "   46,\n",
       "   23,\n",
       "   102,\n",
       "   50,\n",
       "   54,\n",
       "   9,\n",
       "   101,\n",
       "   7,\n",
       "   19,\n",
       "   40,\n",
       "   97,\n",
       "   27,\n",
       "   45,\n",
       "   57,\n",
       "   106,\n",
       "   93,\n",
       "   95,\n",
       "   118,\n",
       "   119,\n",
       "   22,\n",
       "   108,\n",
       "   36,\n",
       "   52,\n",
       "   34,\n",
       "   113,\n",
       "   49,\n",
       "   83,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127],\n",
       "  [20,\n",
       "   33,\n",
       "   46,\n",
       "   83,\n",
       "   21,\n",
       "   58,\n",
       "   1,\n",
       "   18,\n",
       "   17,\n",
       "   75,\n",
       "   25,\n",
       "   64,\n",
       "   55,\n",
       "   71,\n",
       "   62,\n",
       "   67,\n",
       "   9,\n",
       "   69,\n",
       "   11,\n",
       "   66,\n",
       "   2,\n",
       "   14,\n",
       "   35,\n",
       "   57,\n",
       "   80,\n",
       "   82,\n",
       "   34,\n",
       "   45,\n",
       "   6,\n",
       "   52,\n",
       "   13,\n",
       "   68,\n",
       "   8,\n",
       "   22,\n",
       "   72,\n",
       "   73,\n",
       "   3,\n",
       "   38,\n",
       "   19,\n",
       "   43,\n",
       "   31,\n",
       "   77,\n",
       "   7,\n",
       "   36,\n",
       "   27,\n",
       "   81,\n",
       "   48,\n",
       "   51,\n",
       "   41,\n",
       "   74,\n",
       "   4,\n",
       "   44,\n",
       "   24,\n",
       "   61,\n",
       "   84,\n",
       "   85,\n",
       "   0,\n",
       "   60,\n",
       "   30,\n",
       "   32,\n",
       "   16,\n",
       "   53,\n",
       "   5,\n",
       "   63,\n",
       "   28,\n",
       "   76,\n",
       "   23,\n",
       "   40,\n",
       "   42,\n",
       "   54,\n",
       "   12,\n",
       "   50,\n",
       "   29,\n",
       "   37,\n",
       "   59,\n",
       "   65,\n",
       "   26,\n",
       "   56,\n",
       "   70,\n",
       "   79,\n",
       "   15,\n",
       "   47,\n",
       "   10,\n",
       "   78,\n",
       "   39,\n",
       "   49,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [15,\n",
       "   24,\n",
       "   16,\n",
       "   45,\n",
       "   34,\n",
       "   46,\n",
       "   41,\n",
       "   42,\n",
       "   8,\n",
       "   12,\n",
       "   37,\n",
       "   49,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   19,\n",
       "   35,\n",
       "   1,\n",
       "   50,\n",
       "   14,\n",
       "   36,\n",
       "   5,\n",
       "   33,\n",
       "   3,\n",
       "   43,\n",
       "   2,\n",
       "   20,\n",
       "   31,\n",
       "   38,\n",
       "   22,\n",
       "   29,\n",
       "   10,\n",
       "   18,\n",
       "   11,\n",
       "   40,\n",
       "   26,\n",
       "   39,\n",
       "   7,\n",
       "   25,\n",
       "   30,\n",
       "   48,\n",
       "   4,\n",
       "   13,\n",
       "   17,\n",
       "   47,\n",
       "   9,\n",
       "   32,\n",
       "   28,\n",
       "   44,\n",
       "   6,\n",
       "   21,\n",
       "   0,\n",
       "   27,\n",
       "   23,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [5,\n",
       "   55,\n",
       "   67,\n",
       "   77,\n",
       "   28,\n",
       "   35,\n",
       "   29,\n",
       "   76,\n",
       "   38,\n",
       "   58,\n",
       "   18,\n",
       "   24,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   15,\n",
       "   48,\n",
       "   74,\n",
       "   78,\n",
       "   40,\n",
       "   50,\n",
       "   34,\n",
       "   52,\n",
       "   33,\n",
       "   73,\n",
       "   9,\n",
       "   42,\n",
       "   8,\n",
       "   51,\n",
       "   57,\n",
       "   84,\n",
       "   26,\n",
       "   32,\n",
       "   16,\n",
       "   56,\n",
       "   65,\n",
       "   75,\n",
       "   13,\n",
       "   70,\n",
       "   17,\n",
       "   64,\n",
       "   45,\n",
       "   68,\n",
       "   39,\n",
       "   54,\n",
       "   14,\n",
       "   59,\n",
       "   21,\n",
       "   46,\n",
       "   41,\n",
       "   44,\n",
       "   30,\n",
       "   36,\n",
       "   85,\n",
       "   86,\n",
       "   20,\n",
       "   61,\n",
       "   69,\n",
       "   87,\n",
       "   6,\n",
       "   63,\n",
       "   0,\n",
       "   43,\n",
       "   1,\n",
       "   27,\n",
       "   19,\n",
       "   66,\n",
       "   23,\n",
       "   60,\n",
       "   22,\n",
       "   25,\n",
       "   12,\n",
       "   53,\n",
       "   31,\n",
       "   62,\n",
       "   7,\n",
       "   49,\n",
       "   3,\n",
       "   79,\n",
       "   10,\n",
       "   71,\n",
       "   37,\n",
       "   47,\n",
       "   4,\n",
       "   11,\n",
       "   2,\n",
       "   72,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [141,\n",
       "   172,\n",
       "   102,\n",
       "   156,\n",
       "   149,\n",
       "   163,\n",
       "   58,\n",
       "   59,\n",
       "   28,\n",
       "   50,\n",
       "   80,\n",
       "   108,\n",
       "   69,\n",
       "   147,\n",
       "   65,\n",
       "   73,\n",
       "   40,\n",
       "   176,\n",
       "   51,\n",
       "   139,\n",
       "   43,\n",
       "   99,\n",
       "   45,\n",
       "   125,\n",
       "   82,\n",
       "   104,\n",
       "   9,\n",
       "   103,\n",
       "   81,\n",
       "   91,\n",
       "   23,\n",
       "   131,\n",
       "   37,\n",
       "   171,\n",
       "   10,\n",
       "   22,\n",
       "   84,\n",
       "   129,\n",
       "   44,\n",
       "   136,\n",
       "   6,\n",
       "   97,\n",
       "   130,\n",
       "   164,\n",
       "   66,\n",
       "   175,\n",
       "   101,\n",
       "   133,\n",
       "   145,\n",
       "   165,\n",
       "   33,\n",
       "   168,\n",
       "   24,\n",
       "   25,\n",
       "   35,\n",
       "   68,\n",
       "   110,\n",
       "   153,\n",
       "   70,\n",
       "   162,\n",
       "   46,\n",
       "   64,\n",
       "   3,\n",
       "   52,\n",
       "   42,\n",
       "   169,\n",
       "   117,\n",
       "   134,\n",
       "   2,\n",
       "   120,\n",
       "   78,\n",
       "   142,\n",
       "   86,\n",
       "   128,\n",
       "   15,\n",
       "   49,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   88,\n",
       "   118,\n",
       "   94,\n",
       "   150,\n",
       "   53,\n",
       "   83,\n",
       "   7,\n",
       "   55,\n",
       "   39,\n",
       "   57,\n",
       "   135,\n",
       "   143,\n",
       "   96,\n",
       "   126,\n",
       "   77,\n",
       "   119,\n",
       "   14,\n",
       "   79,\n",
       "   29,\n",
       "   146,\n",
       "   41,\n",
       "   93,\n",
       "   107,\n",
       "   109,\n",
       "   123,\n",
       "   167,\n",
       "   34,\n",
       "   177,\n",
       "   1,\n",
       "   90,\n",
       "   5,\n",
       "   154,\n",
       "   32,\n",
       "   173,\n",
       "   36,\n",
       "   178,\n",
       "   72,\n",
       "   157,\n",
       "   4,\n",
       "   98,\n",
       "   31,\n",
       "   75,\n",
       "   112,\n",
       "   152,\n",
       "   48,\n",
       "   122,\n",
       "   47,\n",
       "   159,\n",
       "   16,\n",
       "   151,\n",
       "   74,\n",
       "   115,\n",
       "   114,\n",
       "   138,\n",
       "   67,\n",
       "   158,\n",
       "   54,\n",
       "   105,\n",
       "   61,\n",
       "   124,\n",
       "   148,\n",
       "   155,\n",
       "   26,\n",
       "   179,\n",
       "   60,\n",
       "   160,\n",
       "   8,\n",
       "   113,\n",
       "   20,\n",
       "   170,\n",
       "   21,\n",
       "   95,\n",
       "   56,\n",
       "   85,\n",
       "   19,\n",
       "   132,\n",
       "   111,\n",
       "   161,\n",
       "   27,\n",
       "   63,\n",
       "   76,\n",
       "   100,\n",
       "   13,\n",
       "   127,\n",
       "   137,\n",
       "   144,\n",
       "   30,\n",
       "   38,\n",
       "   0,\n",
       "   11,\n",
       "   89,\n",
       "   166,\n",
       "   106,\n",
       "   121,\n",
       "   17,\n",
       "   71,\n",
       "   12,\n",
       "   116,\n",
       "   18,\n",
       "   140,\n",
       "   87,\n",
       "   174,\n",
       "   62,\n",
       "   92,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191],\n",
       "  [30,\n",
       "   42,\n",
       "   10,\n",
       "   39,\n",
       "   22,\n",
       "   48,\n",
       "   12,\n",
       "   41,\n",
       "   9,\n",
       "   17,\n",
       "   31,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   15,\n",
       "   36,\n",
       "   1,\n",
       "   34,\n",
       "   8,\n",
       "   14,\n",
       "   3,\n",
       "   11,\n",
       "   5,\n",
       "   33,\n",
       "   4,\n",
       "   43,\n",
       "   13,\n",
       "   18,\n",
       "   54,\n",
       "   55,\n",
       "   37,\n",
       "   47,\n",
       "   0,\n",
       "   6,\n",
       "   32,\n",
       "   40,\n",
       "   35,\n",
       "   38,\n",
       "   16,\n",
       "   45,\n",
       "   7,\n",
       "   26,\n",
       "   25,\n",
       "   46,\n",
       "   23,\n",
       "   24,\n",
       "   21,\n",
       "   29,\n",
       "   2,\n",
       "   27,\n",
       "   20,\n",
       "   28,\n",
       "   19,\n",
       "   44,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [2,\n",
       "   64,\n",
       "   15,\n",
       "   70,\n",
       "   12,\n",
       "   44,\n",
       "   61,\n",
       "   87,\n",
       "   10,\n",
       "   37,\n",
       "   30,\n",
       "   76,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   5,\n",
       "   39,\n",
       "   46,\n",
       "   78,\n",
       "   42,\n",
       "   63,\n",
       "   54,\n",
       "   79,\n",
       "   20,\n",
       "   83,\n",
       "   68,\n",
       "   75,\n",
       "   11,\n",
       "   60,\n",
       "   40,\n",
       "   50,\n",
       "   28,\n",
       "   55,\n",
       "   32,\n",
       "   71,\n",
       "   38,\n",
       "   65,\n",
       "   14,\n",
       "   74,\n",
       "   22,\n",
       "   69,\n",
       "   4,\n",
       "   27,\n",
       "   26,\n",
       "   58,\n",
       "   34,\n",
       "   59,\n",
       "   18,\n",
       "   19,\n",
       "   48,\n",
       "   90,\n",
       "   0,\n",
       "   29,\n",
       "   47,\n",
       "   88,\n",
       "   9,\n",
       "   57,\n",
       "   73,\n",
       "   95,\n",
       "   8,\n",
       "   62,\n",
       "   3,\n",
       "   52,\n",
       "   43,\n",
       "   67,\n",
       "   6,\n",
       "   25,\n",
       "   33,\n",
       "   41,\n",
       "   31,\n",
       "   51,\n",
       "   23,\n",
       "   49,\n",
       "   24,\n",
       "   84,\n",
       "   13,\n",
       "   72,\n",
       "   17,\n",
       "   53,\n",
       "   66,\n",
       "   80,\n",
       "   7,\n",
       "   86,\n",
       "   35,\n",
       "   89,\n",
       "   45,\n",
       "   82,\n",
       "   36,\n",
       "   77,\n",
       "   81,\n",
       "   85,\n",
       "   1,\n",
       "   21,\n",
       "   16,\n",
       "   56],\n",
       "  [30,\n",
       "   34,\n",
       "   13,\n",
       "   31,\n",
       "   3,\n",
       "   9,\n",
       "   2,\n",
       "   17,\n",
       "   5,\n",
       "   18,\n",
       "   7,\n",
       "   14,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   0,\n",
       "   33,\n",
       "   10,\n",
       "   29,\n",
       "   40,\n",
       "   41,\n",
       "   1,\n",
       "   21,\n",
       "   6,\n",
       "   32,\n",
       "   23,\n",
       "   42,\n",
       "   8,\n",
       "   28,\n",
       "   4,\n",
       "   36,\n",
       "   19,\n",
       "   24,\n",
       "   37,\n",
       "   47,\n",
       "   16,\n",
       "   35,\n",
       "   20,\n",
       "   22,\n",
       "   11,\n",
       "   39,\n",
       "   12,\n",
       "   26,\n",
       "   15,\n",
       "   27,\n",
       "   25,\n",
       "   38],\n",
       "  [14,\n",
       "   20,\n",
       "   17,\n",
       "   18,\n",
       "   3,\n",
       "   7,\n",
       "   5,\n",
       "   31,\n",
       "   9,\n",
       "   10,\n",
       "   6,\n",
       "   29,\n",
       "   2,\n",
       "   26,\n",
       "   11,\n",
       "   24,\n",
       "   15,\n",
       "   27,\n",
       "   12,\n",
       "   21,\n",
       "   1,\n",
       "   19,\n",
       "   8,\n",
       "   23,\n",
       "   0,\n",
       "   30,\n",
       "   16,\n",
       "   28,\n",
       "   4,\n",
       "   22,\n",
       "   13,\n",
       "   25],\n",
       "  [2,\n",
       "   7,\n",
       "   10,\n",
       "   19,\n",
       "   3,\n",
       "   16,\n",
       "   4,\n",
       "   12,\n",
       "   8,\n",
       "   15,\n",
       "   6,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   9,\n",
       "   11,\n",
       "   0,\n",
       "   5,\n",
       "   13,\n",
       "   18,\n",
       "   1,\n",
       "   14,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [17,\n",
       "   18,\n",
       "   1,\n",
       "   16,\n",
       "   11,\n",
       "   22,\n",
       "   10,\n",
       "   23,\n",
       "   2,\n",
       "   9,\n",
       "   0,\n",
       "   19,\n",
       "   5,\n",
       "   7,\n",
       "   8,\n",
       "   13,\n",
       "   3,\n",
       "   15,\n",
       "   6,\n",
       "   14,\n",
       "   12,\n",
       "   21,\n",
       "   4,\n",
       "   20,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [15,\n",
       "   61,\n",
       "   28,\n",
       "   58,\n",
       "   34,\n",
       "   66,\n",
       "   31,\n",
       "   70,\n",
       "   21,\n",
       "   45,\n",
       "   30,\n",
       "   57,\n",
       "   48,\n",
       "   64,\n",
       "   60,\n",
       "   63,\n",
       "   19,\n",
       "   68,\n",
       "   52,\n",
       "   71,\n",
       "   33,\n",
       "   35,\n",
       "   62,\n",
       "   69,\n",
       "   14,\n",
       "   39,\n",
       "   36,\n",
       "   50,\n",
       "   8,\n",
       "   56,\n",
       "   2,\n",
       "   47,\n",
       "   13,\n",
       "   32,\n",
       "   16,\n",
       "   67,\n",
       "   6,\n",
       "   55,\n",
       "   26,\n",
       "   51,\n",
       "   0,\n",
       "   5,\n",
       "   25,\n",
       "   29,\n",
       "   42,\n",
       "   65,\n",
       "   17,\n",
       "   41,\n",
       "   38,\n",
       "   59,\n",
       "   11,\n",
       "   46,\n",
       "   1,\n",
       "   23,\n",
       "   12,\n",
       "   49,\n",
       "   4,\n",
       "   9,\n",
       "   3,\n",
       "   53,\n",
       "   7,\n",
       "   40,\n",
       "   18,\n",
       "   54,\n",
       "   10,\n",
       "   24,\n",
       "   22,\n",
       "   37,\n",
       "   43,\n",
       "   44,\n",
       "   20,\n",
       "   27,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [79,\n",
       "   82,\n",
       "   38,\n",
       "   68,\n",
       "   19,\n",
       "   74,\n",
       "   16,\n",
       "   17,\n",
       "   5,\n",
       "   57,\n",
       "   21,\n",
       "   33,\n",
       "   37,\n",
       "   46,\n",
       "   9,\n",
       "   27,\n",
       "   51,\n",
       "   76,\n",
       "   48,\n",
       "   66,\n",
       "   36,\n",
       "   45,\n",
       "   0,\n",
       "   78,\n",
       "   54,\n",
       "   63,\n",
       "   30,\n",
       "   72,\n",
       "   29,\n",
       "   59,\n",
       "   24,\n",
       "   28,\n",
       "   8,\n",
       "   35,\n",
       "   2,\n",
       "   42,\n",
       "   4,\n",
       "   75,\n",
       "   89,\n",
       "   90,\n",
       "   23,\n",
       "   53,\n",
       "   22,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   11,\n",
       "   44,\n",
       "   49,\n",
       "   77,\n",
       "   32,\n",
       "   70,\n",
       "   73,\n",
       "   84,\n",
       "   1,\n",
       "   56,\n",
       "   18,\n",
       "   87,\n",
       "   40,\n",
       "   61,\n",
       "   6,\n",
       "   80,\n",
       "   43,\n",
       "   55,\n",
       "   67,\n",
       "   71,\n",
       "   10,\n",
       "   83,\n",
       "   52,\n",
       "   58,\n",
       "   13,\n",
       "   69,\n",
       "   20,\n",
       "   34,\n",
       "   50,\n",
       "   81,\n",
       "   85,\n",
       "   86,\n",
       "   3,\n",
       "   31,\n",
       "   39,\n",
       "   88,\n",
       "   14,\n",
       "   64,\n",
       "   15,\n",
       "   60,\n",
       "   25,\n",
       "   65,\n",
       "   41,\n",
       "   47,\n",
       "   7,\n",
       "   26,\n",
       "   12,\n",
       "   62],\n",
       "  [16,\n",
       "   18,\n",
       "   19,\n",
       "   22,\n",
       "   1,\n",
       "   9,\n",
       "   0,\n",
       "   20,\n",
       "   6,\n",
       "   21,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   3,\n",
       "   10,\n",
       "   2,\n",
       "   17,\n",
       "   12,\n",
       "   15,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   8,\n",
       "   14,\n",
       "   7,\n",
       "   39,\n",
       "   4,\n",
       "   13,\n",
       "   5,\n",
       "   11,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [13,\n",
       "   32,\n",
       "   16,\n",
       "   33,\n",
       "   49,\n",
       "   52,\n",
       "   9,\n",
       "   54,\n",
       "   23,\n",
       "   43,\n",
       "   37,\n",
       "   46,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   34,\n",
       "   42,\n",
       "   14,\n",
       "   28,\n",
       "   11,\n",
       "   47,\n",
       "   8,\n",
       "   48,\n",
       "   17,\n",
       "   41,\n",
       "   27,\n",
       "   29,\n",
       "   12,\n",
       "   51,\n",
       "   22,\n",
       "   39,\n",
       "   31,\n",
       "   56,\n",
       "   5,\n",
       "   18,\n",
       "   0,\n",
       "   45,\n",
       "   19,\n",
       "   57,\n",
       "   2,\n",
       "   36,\n",
       "   3,\n",
       "   63,\n",
       "   1,\n",
       "   25,\n",
       "   26,\n",
       "   38,\n",
       "   20,\n",
       "   44,\n",
       "   15,\n",
       "   24,\n",
       "   10,\n",
       "   35,\n",
       "   21,\n",
       "   30,\n",
       "   4,\n",
       "   6,\n",
       "   40,\n",
       "   50,\n",
       "   55,\n",
       "   58,\n",
       "   7,\n",
       "   53],\n",
       "  [5,\n",
       "   15,\n",
       "   18,\n",
       "   44,\n",
       "   21,\n",
       "   30,\n",
       "   3,\n",
       "   14,\n",
       "   19,\n",
       "   36,\n",
       "   22,\n",
       "   45,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   26,\n",
       "   42,\n",
       "   6,\n",
       "   28,\n",
       "   17,\n",
       "   33,\n",
       "   20,\n",
       "   43,\n",
       "   51,\n",
       "   55,\n",
       "   2,\n",
       "   16,\n",
       "   1,\n",
       "   47,\n",
       "   61,\n",
       "   62,\n",
       "   7,\n",
       "   11,\n",
       "   50,\n",
       "   63,\n",
       "   31,\n",
       "   39,\n",
       "   13,\n",
       "   53,\n",
       "   4,\n",
       "   32,\n",
       "   27,\n",
       "   48,\n",
       "   9,\n",
       "   49,\n",
       "   8,\n",
       "   52,\n",
       "   10,\n",
       "   23,\n",
       "   35,\n",
       "   40,\n",
       "   0,\n",
       "   54,\n",
       "   29,\n",
       "   56,\n",
       "   34,\n",
       "   37,\n",
       "   12,\n",
       "   24,\n",
       "   25,\n",
       "   38,\n",
       "   41,\n",
       "   46],\n",
       "  [33,\n",
       "   59,\n",
       "   15,\n",
       "   51,\n",
       "   23,\n",
       "   62,\n",
       "   16,\n",
       "   63,\n",
       "   44,\n",
       "   65,\n",
       "   43,\n",
       "   69,\n",
       "   37,\n",
       "   71,\n",
       "   5,\n",
       "   54,\n",
       "   61,\n",
       "   64,\n",
       "   8,\n",
       "   78,\n",
       "   6,\n",
       "   56,\n",
       "   25,\n",
       "   35,\n",
       "   45,\n",
       "   67,\n",
       "   58,\n",
       "   82,\n",
       "   21,\n",
       "   85,\n",
       "   49,\n",
       "   77,\n",
       "   32,\n",
       "   47,\n",
       "   52,\n",
       "   83,\n",
       "   34,\n",
       "   40,\n",
       "   22,\n",
       "   72,\n",
       "   13,\n",
       "   20,\n",
       "   3,\n",
       "   53,\n",
       "   11,\n",
       "   87,\n",
       "   19,\n",
       "   36,\n",
       "   26,\n",
       "   48,\n",
       "   18,\n",
       "   81,\n",
       "   7,\n",
       "   41,\n",
       "   38,\n",
       "   84,\n",
       "   60,\n",
       "   79,\n",
       "   0,\n",
       "   24,\n",
       "   14,\n",
       "   27,\n",
       "   17,\n",
       "   30,\n",
       "   29,\n",
       "   46,\n",
       "   12,\n",
       "   42,\n",
       "   70,\n",
       "   76,\n",
       "   57,\n",
       "   80,\n",
       "   55,\n",
       "   74,\n",
       "   4,\n",
       "   86,\n",
       "   1,\n",
       "   75,\n",
       "   2,\n",
       "   66,\n",
       "   10,\n",
       "   68,\n",
       "   28,\n",
       "   31,\n",
       "   50,\n",
       "   73,\n",
       "   9,\n",
       "   39,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [7,\n",
       "   26,\n",
       "   12,\n",
       "   25,\n",
       "   0,\n",
       "   19,\n",
       "   15,\n",
       "   20,\n",
       "   3,\n",
       "   14,\n",
       "   5,\n",
       "   24,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   1,\n",
       "   4,\n",
       "   2,\n",
       "   11,\n",
       "   8,\n",
       "   17,\n",
       "   16,\n",
       "   27,\n",
       "   6,\n",
       "   22,\n",
       "   13,\n",
       "   21,\n",
       "   9,\n",
       "   23,\n",
       "   10,\n",
       "   18],\n",
       "  [13,\n",
       "   14,\n",
       "   2,\n",
       "   21,\n",
       "   9,\n",
       "   16,\n",
       "   22,\n",
       "   23,\n",
       "   0,\n",
       "   1,\n",
       "   4,\n",
       "   10,\n",
       "   5,\n",
       "   12,\n",
       "   7,\n",
       "   11,\n",
       "   6,\n",
       "   17,\n",
       "   8,\n",
       "   15,\n",
       "   3,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [7,\n",
       "   35,\n",
       "   22,\n",
       "   31,\n",
       "   16,\n",
       "   37,\n",
       "   6,\n",
       "   34,\n",
       "   28,\n",
       "   41,\n",
       "   13,\n",
       "   47,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   10,\n",
       "   46,\n",
       "   21,\n",
       "   49,\n",
       "   19,\n",
       "   43,\n",
       "   1,\n",
       "   26,\n",
       "   29,\n",
       "   44,\n",
       "   17,\n",
       "   48,\n",
       "   4,\n",
       "   20,\n",
       "   2,\n",
       "   15,\n",
       "   27,\n",
       "   39,\n",
       "   9,\n",
       "   32,\n",
       "   33,\n",
       "   38,\n",
       "   0,\n",
       "   30,\n",
       "   3,\n",
       "   50,\n",
       "   5,\n",
       "   8,\n",
       "   23,\n",
       "   51,\n",
       "   12,\n",
       "   18,\n",
       "   24,\n",
       "   25,\n",
       "   14,\n",
       "   40,\n",
       "   36,\n",
       "   42,\n",
       "   11,\n",
       "   45,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [13,\n",
       "   81,\n",
       "   26,\n",
       "   96,\n",
       "   35,\n",
       "   100,\n",
       "   84,\n",
       "   97,\n",
       "   12,\n",
       "   67,\n",
       "   50,\n",
       "   87,\n",
       "   51,\n",
       "   83,\n",
       "   34,\n",
       "   88,\n",
       "   3,\n",
       "   70,\n",
       "   80,\n",
       "   91,\n",
       "   65,\n",
       "   92,\n",
       "   0,\n",
       "   93,\n",
       "   28,\n",
       "   72,\n",
       "   63,\n",
       "   76,\n",
       "   36,\n",
       "   41,\n",
       "   14,\n",
       "   71,\n",
       "   19,\n",
       "   55,\n",
       "   40,\n",
       "   89,\n",
       "   17,\n",
       "   21,\n",
       "   32,\n",
       "   78,\n",
       "   24,\n",
       "   37,\n",
       "   46,\n",
       "   56,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   20,\n",
       "   31,\n",
       "   11,\n",
       "   15,\n",
       "   54,\n",
       "   74,\n",
       "   53,\n",
       "   99,\n",
       "   18,\n",
       "   90,\n",
       "   8,\n",
       "   79,\n",
       "   30,\n",
       "   73,\n",
       "   64,\n",
       "   109,\n",
       "   29,\n",
       "   52,\n",
       "   22,\n",
       "   61,\n",
       "   4,\n",
       "   57,\n",
       "   6,\n",
       "   59,\n",
       "   10,\n",
       "   44,\n",
       "   2,\n",
       "   9,\n",
       "   33,\n",
       "   49,\n",
       "   110,\n",
       "   111,\n",
       "   94,\n",
       "   98,\n",
       "   1,\n",
       "   75,\n",
       "   43,\n",
       "   48,\n",
       "   68,\n",
       "   82,\n",
       "   60,\n",
       "   95,\n",
       "   58,\n",
       "   66,\n",
       "   16,\n",
       "   85,\n",
       "   38,\n",
       "   42,\n",
       "   25,\n",
       "   27,\n",
       "   77,\n",
       "   102,\n",
       "   23,\n",
       "   104,\n",
       "   62,\n",
       "   69,\n",
       "   47,\n",
       "   103,\n",
       "   7,\n",
       "   101,\n",
       "   39,\n",
       "   86,\n",
       "   5,\n",
       "   45],\n",
       "  [11,\n",
       "   29,\n",
       "   18,\n",
       "   31,\n",
       "   3,\n",
       "   25,\n",
       "   13,\n",
       "   16,\n",
       "   8,\n",
       "   24,\n",
       "   7,\n",
       "   15,\n",
       "   12,\n",
       "   19,\n",
       "   22,\n",
       "   23,\n",
       "   5,\n",
       "   28,\n",
       "   9,\n",
       "   26,\n",
       "   0,\n",
       "   2,\n",
       "   6,\n",
       "   20,\n",
       "   4,\n",
       "   27,\n",
       "   1,\n",
       "   30,\n",
       "   10,\n",
       "   14,\n",
       "   17,\n",
       "   21],\n",
       "  [5,\n",
       "   32,\n",
       "   53,\n",
       "   66,\n",
       "   41,\n",
       "   55,\n",
       "   17,\n",
       "   48,\n",
       "   54,\n",
       "   65,\n",
       "   29,\n",
       "   61,\n",
       "   16,\n",
       "   33,\n",
       "   21,\n",
       "   51,\n",
       "   4,\n",
       "   19,\n",
       "   34,\n",
       "   43,\n",
       "   7,\n",
       "   38,\n",
       "   13,\n",
       "   58,\n",
       "   8,\n",
       "   14,\n",
       "   2,\n",
       "   60,\n",
       "   15,\n",
       "   42,\n",
       "   36,\n",
       "   49,\n",
       "   9,\n",
       "   45,\n",
       "   1,\n",
       "   67,\n",
       "   22,\n",
       "   46,\n",
       "   11,\n",
       "   28,\n",
       "   3,\n",
       "   6,\n",
       "   26,\n",
       "   30,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   47,\n",
       "   57,\n",
       "   23,\n",
       "   63,\n",
       "   35,\n",
       "   39,\n",
       "   12,\n",
       "   20,\n",
       "   31,\n",
       "   50,\n",
       "   10,\n",
       "   37,\n",
       "   0,\n",
       "   25,\n",
       "   27,\n",
       "   56,\n",
       "   18,\n",
       "   64,\n",
       "   52,\n",
       "   62,\n",
       "   24,\n",
       "   59,\n",
       "   40,\n",
       "   44,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [2,\n",
       "   6,\n",
       "   28,\n",
       "   47,\n",
       "   49,\n",
       "   54,\n",
       "   17,\n",
       "   36,\n",
       "   20,\n",
       "   25,\n",
       "   31,\n",
       "   52,\n",
       "   5,\n",
       "   42,\n",
       "   3,\n",
       "   40,\n",
       "   50,\n",
       "   61,\n",
       "   16,\n",
       "   26,\n",
       "   10,\n",
       "   23,\n",
       "   22,\n",
       "   43,\n",
       "   4,\n",
       "   44,\n",
       "   1,\n",
       "   13,\n",
       "   18,\n",
       "   34,\n",
       "   27,\n",
       "   51,\n",
       "   33,\n",
       "   62,\n",
       "   30,\n",
       "   48,\n",
       "   12,\n",
       "   55,\n",
       "   7,\n",
       "   9,\n",
       "   37,\n",
       "   60,\n",
       "   8,\n",
       "   46,\n",
       "   14,\n",
       "   63,\n",
       "   0,\n",
       "   11,\n",
       "   15,\n",
       "   41,\n",
       "   24,\n",
       "   35,\n",
       "   56,\n",
       "   57,\n",
       "   38,\n",
       "   39,\n",
       "   29,\n",
       "   59,\n",
       "   32,\n",
       "   58,\n",
       "   19,\n",
       "   45,\n",
       "   21,\n",
       "   53],\n",
       "  [12,\n",
       "   78,\n",
       "   61,\n",
       "   76,\n",
       "   58,\n",
       "   87,\n",
       "   19,\n",
       "   88,\n",
       "   23,\n",
       "   89,\n",
       "   70,\n",
       "   84,\n",
       "   11,\n",
       "   24,\n",
       "   45,\n",
       "   83,\n",
       "   44,\n",
       "   64,\n",
       "   79,\n",
       "   92,\n",
       "   17,\n",
       "   29,\n",
       "   46,\n",
       "   54,\n",
       "   10,\n",
       "   82,\n",
       "   18,\n",
       "   73,\n",
       "   5,\n",
       "   52,\n",
       "   34,\n",
       "   56,\n",
       "   28,\n",
       "   80,\n",
       "   41,\n",
       "   60,\n",
       "   6,\n",
       "   62,\n",
       "   30,\n",
       "   53,\n",
       "   38,\n",
       "   40,\n",
       "   2,\n",
       "   15,\n",
       "   42,\n",
       "   51,\n",
       "   21,\n",
       "   36,\n",
       "   55,\n",
       "   65,\n",
       "   25,\n",
       "   93,\n",
       "   3,\n",
       "   35,\n",
       "   47,\n",
       "   48,\n",
       "   8,\n",
       "   16,\n",
       "   49,\n",
       "   59,\n",
       "   13,\n",
       "   22,\n",
       "   67,\n",
       "   74,\n",
       "   20,\n",
       "   90,\n",
       "   0,\n",
       "   71,\n",
       "   69,\n",
       "   91,\n",
       "   14,\n",
       "   68,\n",
       "   26,\n",
       "   50,\n",
       "   43,\n",
       "   86,\n",
       "   1,\n",
       "   63,\n",
       "   9,\n",
       "   27,\n",
       "   66,\n",
       "   85,\n",
       "   72,\n",
       "   75,\n",
       "   4,\n",
       "   33,\n",
       "   94,\n",
       "   95,\n",
       "   39,\n",
       "   57,\n",
       "   7,\n",
       "   81,\n",
       "   31,\n",
       "   32,\n",
       "   37,\n",
       "   77],\n",
       "  [1,\n",
       "   15,\n",
       "   0,\n",
       "   19,\n",
       "   2,\n",
       "   10,\n",
       "   11,\n",
       "   16,\n",
       "   4,\n",
       "   5,\n",
       "   3,\n",
       "   6,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   7,\n",
       "   18,\n",
       "   12,\n",
       "   13,\n",
       "   8,\n",
       "   17,\n",
       "   9,\n",
       "   14,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [36,\n",
       "   81,\n",
       "   78,\n",
       "   94,\n",
       "   0,\n",
       "   115,\n",
       "   55,\n",
       "   82,\n",
       "   10,\n",
       "   76,\n",
       "   104,\n",
       "   121,\n",
       "   7,\n",
       "   47,\n",
       "   63,\n",
       "   97,\n",
       "   102,\n",
       "   116,\n",
       "   17,\n",
       "   95,\n",
       "   22,\n",
       "   90,\n",
       "   83,\n",
       "   86,\n",
       "   58,\n",
       "   109,\n",
       "   24,\n",
       "   61,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   43,\n",
       "   49,\n",
       "   30,\n",
       "   33,\n",
       "   53,\n",
       "   118,\n",
       "   60,\n",
       "   64,\n",
       "   37,\n",
       "   117,\n",
       "   66,\n",
       "   75,\n",
       "   38,\n",
       "   68,\n",
       "   15,\n",
       "   65,\n",
       "   13,\n",
       "   106,\n",
       "   28,\n",
       "   40,\n",
       "   4,\n",
       "   71,\n",
       "   74,\n",
       "   77,\n",
       "   9,\n",
       "   50,\n",
       "   18,\n",
       "   62,\n",
       "   34,\n",
       "   110,\n",
       "   25,\n",
       "   101,\n",
       "   14,\n",
       "   114,\n",
       "   39,\n",
       "   98,\n",
       "   23,\n",
       "   67,\n",
       "   80,\n",
       "   111,\n",
       "   73,\n",
       "   99,\n",
       "   1,\n",
       "   105,\n",
       "   70,\n",
       "   112,\n",
       "   8,\n",
       "   93,\n",
       "   27,\n",
       "   92,\n",
       "   5,\n",
       "   42,\n",
       "   48,\n",
       "   107,\n",
       "   54,\n",
       "   57,\n",
       "   35,\n",
       "   91,\n",
       "   2,\n",
       "   72,\n",
       "   29,\n",
       "   84,\n",
       "   16,\n",
       "   108,\n",
       "   45,\n",
       "   79,\n",
       "   3,\n",
       "   85,\n",
       "   21,\n",
       "   103,\n",
       "   126,\n",
       "   127,\n",
       "   52,\n",
       "   113,\n",
       "   12,\n",
       "   32,\n",
       "   31,\n",
       "   41,\n",
       "   26,\n",
       "   59,\n",
       "   96,\n",
       "   119,\n",
       "   19,\n",
       "   87,\n",
       "   44,\n",
       "   88,\n",
       "   20,\n",
       "   120,\n",
       "   46,\n",
       "   89,\n",
       "   6,\n",
       "   11,\n",
       "   51,\n",
       "   100,\n",
       "   56,\n",
       "   69],\n",
       "  [3,\n",
       "   41,\n",
       "   51,\n",
       "   52,\n",
       "   30,\n",
       "   42,\n",
       "   7,\n",
       "   17,\n",
       "   14,\n",
       "   48,\n",
       "   11,\n",
       "   47,\n",
       "   0,\n",
       "   32,\n",
       "   35,\n",
       "   55,\n",
       "   21,\n",
       "   26,\n",
       "   18,\n",
       "   39,\n",
       "   36,\n",
       "   49,\n",
       "   37,\n",
       "   44,\n",
       "   15,\n",
       "   45,\n",
       "   5,\n",
       "   13,\n",
       "   1,\n",
       "   25,\n",
       "   22,\n",
       "   53,\n",
       "   2,\n",
       "   27,\n",
       "   19,\n",
       "   20,\n",
       "   6,\n",
       "   23,\n",
       "   43,\n",
       "   54,\n",
       "   16,\n",
       "   28,\n",
       "   10,\n",
       "   29,\n",
       "   24,\n",
       "   46,\n",
       "   9,\n",
       "   33,\n",
       "   34,\n",
       "   38,\n",
       "   31,\n",
       "   40,\n",
       "   12,\n",
       "   50,\n",
       "   4,\n",
       "   8,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [8,\n",
       "   18,\n",
       "   6,\n",
       "   68,\n",
       "   3,\n",
       "   67,\n",
       "   21,\n",
       "   74,\n",
       "   30,\n",
       "   57,\n",
       "   23,\n",
       "   61,\n",
       "   28,\n",
       "   79,\n",
       "   12,\n",
       "   27,\n",
       "   43,\n",
       "   58,\n",
       "   32,\n",
       "   56,\n",
       "   19,\n",
       "   25,\n",
       "   38,\n",
       "   54,\n",
       "   29,\n",
       "   47,\n",
       "   62,\n",
       "   76,\n",
       "   7,\n",
       "   13,\n",
       "   53,\n",
       "   75,\n",
       "   16,\n",
       "   77,\n",
       "   15,\n",
       "   72,\n",
       "   9,\n",
       "   48,\n",
       "   22,\n",
       "   46,\n",
       "   1,\n",
       "   31,\n",
       "   63,\n",
       "   69,\n",
       "   65,\n",
       "   78,\n",
       "   26,\n",
       "   37,\n",
       "   11,\n",
       "   51,\n",
       "   35,\n",
       "   39,\n",
       "   14,\n",
       "   17,\n",
       "   44,\n",
       "   73,\n",
       "   24,\n",
       "   60,\n",
       "   52,\n",
       "   64,\n",
       "   34,\n",
       "   45,\n",
       "   0,\n",
       "   59,\n",
       "   40,\n",
       "   66,\n",
       "   2,\n",
       "   71,\n",
       "   4,\n",
       "   41,\n",
       "   33,\n",
       "   36,\n",
       "   50,\n",
       "   70,\n",
       "   10,\n",
       "   20,\n",
       "   49,\n",
       "   55,\n",
       "   5,\n",
       "   42],\n",
       "  [10,\n",
       "   37,\n",
       "   13,\n",
       "   38,\n",
       "   27,\n",
       "   32,\n",
       "   16,\n",
       "   28,\n",
       "   6,\n",
       "   20,\n",
       "   12,\n",
       "   22,\n",
       "   1,\n",
       "   8,\n",
       "   17,\n",
       "   30,\n",
       "   19,\n",
       "   26,\n",
       "   24,\n",
       "   39,\n",
       "   5,\n",
       "   15,\n",
       "   11,\n",
       "   14,\n",
       "   9,\n",
       "   18,\n",
       "   7,\n",
       "   34,\n",
       "   0,\n",
       "   4,\n",
       "   3,\n",
       "   33,\n",
       "   29,\n",
       "   35,\n",
       "   25,\n",
       "   31,\n",
       "   21,\n",
       "   36,\n",
       "   2,\n",
       "   23,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [20,\n",
       "   73,\n",
       "   64,\n",
       "   97,\n",
       "   22,\n",
       "   69,\n",
       "   66,\n",
       "   91,\n",
       "   57,\n",
       "   92,\n",
       "   44,\n",
       "   84,\n",
       "   6,\n",
       "   90,\n",
       "   85,\n",
       "   100,\n",
       "   48,\n",
       "   71,\n",
       "   53,\n",
       "   101,\n",
       "   8,\n",
       "   17,\n",
       "   38,\n",
       "   45,\n",
       "   50,\n",
       "   72,\n",
       "   36,\n",
       "   79,\n",
       "   46,\n",
       "   98,\n",
       "   32,\n",
       "   35,\n",
       "   31,\n",
       "   37,\n",
       "   54,\n",
       "   95,\n",
       "   77,\n",
       "   80,\n",
       "   7,\n",
       "   82,\n",
       "   0,\n",
       "   14,\n",
       "   16,\n",
       "   81,\n",
       "   21,\n",
       "   87,\n",
       "   25,\n",
       "   39,\n",
       "   11,\n",
       "   94,\n",
       "   28,\n",
       "   60,\n",
       "   9,\n",
       "   78,\n",
       "   102,\n",
       "   103,\n",
       "   24,\n",
       "   47,\n",
       "   33,\n",
       "   68,\n",
       "   41,\n",
       "   67,\n",
       "   63,\n",
       "   89,\n",
       "   29,\n",
       "   70,\n",
       "   23,\n",
       "   49,\n",
       "   4,\n",
       "   51,\n",
       "   13,\n",
       "   88,\n",
       "   5,\n",
       "   27,\n",
       "   3,\n",
       "   43,\n",
       "   42,\n",
       "   55,\n",
       "   1,\n",
       "   12,\n",
       "   26,\n",
       "   86,\n",
       "   61,\n",
       "   96,\n",
       "   75,\n",
       "   99,\n",
       "   18,\n",
       "   40,\n",
       "   34,\n",
       "   93,\n",
       "   56,\n",
       "   62,\n",
       "   2,\n",
       "   58,\n",
       "   52,\n",
       "   59,\n",
       "   15,\n",
       "   19,\n",
       "   10,\n",
       "   65,\n",
       "   30,\n",
       "   83,\n",
       "   74,\n",
       "   76,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111],\n",
       "  [22,\n",
       "   27,\n",
       "   5,\n",
       "   24,\n",
       "   2,\n",
       "   15,\n",
       "   10,\n",
       "   19,\n",
       "   32,\n",
       "   40,\n",
       "   9,\n",
       "   28,\n",
       "   0,\n",
       "   39,\n",
       "   17,\n",
       "   18,\n",
       "   14,\n",
       "   25,\n",
       "   6,\n",
       "   11,\n",
       "   7,\n",
       "   42,\n",
       "   46,\n",
       "   47,\n",
       "   20,\n",
       "   43,\n",
       "   41,\n",
       "   45,\n",
       "   21,\n",
       "   33,\n",
       "   13,\n",
       "   37,\n",
       "   23,\n",
       "   29,\n",
       "   26,\n",
       "   44,\n",
       "   4,\n",
       "   12,\n",
       "   1,\n",
       "   31,\n",
       "   16,\n",
       "   34,\n",
       "   30,\n",
       "   38,\n",
       "   3,\n",
       "   35,\n",
       "   8,\n",
       "   36],\n",
       "  [11,\n",
       "   77,\n",
       "   45,\n",
       "   65,\n",
       "   19,\n",
       "   67,\n",
       "   14,\n",
       "   35,\n",
       "   6,\n",
       "   39,\n",
       "   9,\n",
       "   78,\n",
       "   34,\n",
       "   83,\n",
       "   26,\n",
       "   50,\n",
       "   25,\n",
       "   69,\n",
       "   1,\n",
       "   62,\n",
       "   32,\n",
       "   52,\n",
       "   15,\n",
       "   68,\n",
       "   10,\n",
       "   76,\n",
       "   42,\n",
       "   71,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   63,\n",
       "   73,\n",
       "   22,\n",
       "   31,\n",
       "   3,\n",
       "   64,\n",
       "   66,\n",
       "   72,\n",
       "   13,\n",
       "   54,\n",
       "   51,\n",
       "   75,\n",
       "   49,\n",
       "   70,\n",
       "   41,\n",
       "   60,\n",
       "   20,\n",
       "   24,\n",
       "   12,\n",
       "   57,\n",
       "   23,\n",
       "   56,\n",
       "   4,\n",
       "   59,\n",
       "   16,\n",
       "   30,\n",
       "   5,\n",
       "   28,\n",
       "   21,\n",
       "   43,\n",
       "   29,\n",
       "   38,\n",
       "   48,\n",
       "   53,\n",
       "   0,\n",
       "   61,\n",
       "   47,\n",
       "   81,\n",
       "   40,\n",
       "   44,\n",
       "   55,\n",
       "   80,\n",
       "   27,\n",
       "   82,\n",
       "   36,\n",
       "   37,\n",
       "   7,\n",
       "   18,\n",
       "   46,\n",
       "   58,\n",
       "   17,\n",
       "   74,\n",
       "   8,\n",
       "   79,\n",
       "   2,\n",
       "   33,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [39,\n",
       "   43,\n",
       "   30,\n",
       "   42,\n",
       "   22,\n",
       "   25,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   35,\n",
       "   10,\n",
       "   20,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   14,\n",
       "   40,\n",
       "   6,\n",
       "   33,\n",
       "   11,\n",
       "   38,\n",
       "   9,\n",
       "   12,\n",
       "   26,\n",
       "   29,\n",
       "   1,\n",
       "   37,\n",
       "   4,\n",
       "   31,\n",
       "   17,\n",
       "   32,\n",
       "   0,\n",
       "   24,\n",
       "   16,\n",
       "   23,\n",
       "   27,\n",
       "   36,\n",
       "   15,\n",
       "   34,\n",
       "   2,\n",
       "   8,\n",
       "   3,\n",
       "   28,\n",
       "   5,\n",
       "   7,\n",
       "   13,\n",
       "   41],\n",
       "  [36,\n",
       "   61,\n",
       "   15,\n",
       "   44,\n",
       "   1,\n",
       "   46,\n",
       "   22,\n",
       "   59,\n",
       "   27,\n",
       "   39,\n",
       "   26,\n",
       "   49,\n",
       "   5,\n",
       "   23,\n",
       "   29,\n",
       "   52,\n",
       "   25,\n",
       "   42,\n",
       "   45,\n",
       "   62,\n",
       "   10,\n",
       "   21,\n",
       "   54,\n",
       "   63,\n",
       "   4,\n",
       "   31,\n",
       "   7,\n",
       "   13,\n",
       "   9,\n",
       "   53,\n",
       "   30,\n",
       "   47,\n",
       "   18,\n",
       "   56,\n",
       "   14,\n",
       "   35,\n",
       "   33,\n",
       "   50,\n",
       "   38,\n",
       "   57,\n",
       "   19,\n",
       "   40,\n",
       "   11,\n",
       "   60,\n",
       "   28,\n",
       "   43,\n",
       "   6,\n",
       "   48,\n",
       "   2,\n",
       "   37,\n",
       "   32,\n",
       "   58,\n",
       "   16,\n",
       "   34,\n",
       "   8,\n",
       "   51,\n",
       "   0,\n",
       "   41,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   24,\n",
       "   3,\n",
       "   55],\n",
       "  [0,\n",
       "   15,\n",
       "   4,\n",
       "   14,\n",
       "   27,\n",
       "   29,\n",
       "   7,\n",
       "   12,\n",
       "   6,\n",
       "   8,\n",
       "   13,\n",
       "   31,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   17,\n",
       "   19,\n",
       "   18,\n",
       "   21,\n",
       "   1,\n",
       "   5,\n",
       "   24,\n",
       "   32,\n",
       "   16,\n",
       "   25,\n",
       "   2,\n",
       "   39,\n",
       "   26,\n",
       "   33,\n",
       "   28,\n",
       "   34,\n",
       "   3,\n",
       "   23,\n",
       "   10,\n",
       "   22,\n",
       "   20,\n",
       "   30,\n",
       "   9,\n",
       "   11,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [29,\n",
       "   42,\n",
       "   26,\n",
       "   48,\n",
       "   31,\n",
       "   34,\n",
       "   23,\n",
       "   33,\n",
       "   30,\n",
       "   37,\n",
       "   41,\n",
       "   47,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   1,\n",
       "   22,\n",
       "   3,\n",
       "   21,\n",
       "   9,\n",
       "   13,\n",
       "   54,\n",
       "   55,\n",
       "   0,\n",
       "   10,\n",
       "   17,\n",
       "   39,\n",
       "   6,\n",
       "   49,\n",
       "   27,\n",
       "   40,\n",
       "   8,\n",
       "   36,\n",
       "   44,\n",
       "   45,\n",
       "   5,\n",
       "   43,\n",
       "   19,\n",
       "   25,\n",
       "   2,\n",
       "   24,\n",
       "   4,\n",
       "   7,\n",
       "   12,\n",
       "   38,\n",
       "   20,\n",
       "   32,\n",
       "   14,\n",
       "   46,\n",
       "   11,\n",
       "   28,\n",
       "   16,\n",
       "   35,\n",
       "   15,\n",
       "   18,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [9,\n",
       "   16,\n",
       "   3,\n",
       "   8,\n",
       "   29,\n",
       "   34,\n",
       "   46,\n",
       "   47,\n",
       "   28,\n",
       "   30,\n",
       "   2,\n",
       "   18,\n",
       "   33,\n",
       "   39,\n",
       "   27,\n",
       "   42,\n",
       "   36,\n",
       "   38,\n",
       "   0,\n",
       "   43,\n",
       "   6,\n",
       "   15,\n",
       "   40,\n",
       "   41,\n",
       "   31,\n",
       "   32,\n",
       "   14,\n",
       "   37,\n",
       "   23,\n",
       "   26,\n",
       "   7,\n",
       "   24,\n",
       "   19,\n",
       "   21,\n",
       "   12,\n",
       "   17,\n",
       "   22,\n",
       "   25,\n",
       "   20,\n",
       "   44,\n",
       "   10,\n",
       "   45,\n",
       "   1,\n",
       "   4,\n",
       "   5,\n",
       "   13,\n",
       "   11,\n",
       "   35],\n",
       "  [33,\n",
       "   57,\n",
       "   38,\n",
       "   71,\n",
       "   48,\n",
       "   70,\n",
       "   5,\n",
       "   17,\n",
       "   8,\n",
       "   28,\n",
       "   9,\n",
       "   69,\n",
       "   35,\n",
       "   54,\n",
       "   55,\n",
       "   59,\n",
       "   18,\n",
       "   53,\n",
       "   19,\n",
       "   62,\n",
       "   23,\n",
       "   37,\n",
       "   15,\n",
       "   79,\n",
       "   24,\n",
       "   58,\n",
       "   11,\n",
       "   31,\n",
       "   13,\n",
       "   27,\n",
       "   3,\n",
       "   6,\n",
       "   40,\n",
       "   73,\n",
       "   34,\n",
       "   74,\n",
       "   21,\n",
       "   65,\n",
       "   29,\n",
       "   42,\n",
       "   2,\n",
       "   52,\n",
       "   22,\n",
       "   32,\n",
       "   7,\n",
       "   10,\n",
       "   25,\n",
       "   56,\n",
       "   14,\n",
       "   78,\n",
       "   44,\n",
       "   76,\n",
       "   50,\n",
       "   67,\n",
       "   49,\n",
       "   75,\n",
       "   30,\n",
       "   45,\n",
       "   26,\n",
       "   66,\n",
       "   41,\n",
       "   77,\n",
       "   0,\n",
       "   20,\n",
       "   1,\n",
       "   51,\n",
       "   43,\n",
       "   63,\n",
       "   39,\n",
       "   60,\n",
       "   12,\n",
       "   72,\n",
       "   61,\n",
       "   68,\n",
       "   46,\n",
       "   47,\n",
       "   36,\n",
       "   64,\n",
       "   4,\n",
       "   16],\n",
       "  [3,\n",
       "   24,\n",
       "   0,\n",
       "   7,\n",
       "   12,\n",
       "   15,\n",
       "   25,\n",
       "   26,\n",
       "   4,\n",
       "   11,\n",
       "   18,\n",
       "   21,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   13,\n",
       "   16,\n",
       "   6,\n",
       "   19,\n",
       "   9,\n",
       "   17,\n",
       "   22,\n",
       "   23,\n",
       "   2,\n",
       "   8,\n",
       "   10,\n",
       "   31,\n",
       "   1,\n",
       "   20,\n",
       "   5,\n",
       "   14],\n",
       "  [13,\n",
       "   14,\n",
       "   17,\n",
       "   19,\n",
       "   0,\n",
       "   5,\n",
       "   22,\n",
       "   23,\n",
       "   6,\n",
       "   10,\n",
       "   18,\n",
       "   21,\n",
       "   1,\n",
       "   16,\n",
       "   9,\n",
       "   12,\n",
       "   11,\n",
       "   20,\n",
       "   2,\n",
       "   8,\n",
       "   3,\n",
       "   4,\n",
       "   7,\n",
       "   15,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [21,\n",
       "   29,\n",
       "   2,\n",
       "   54,\n",
       "   22,\n",
       "   38,\n",
       "   13,\n",
       "   40,\n",
       "   4,\n",
       "   36,\n",
       "   11,\n",
       "   26,\n",
       "   24,\n",
       "   45,\n",
       "   20,\n",
       "   44,\n",
       "   19,\n",
       "   35,\n",
       "   25,\n",
       "   30,\n",
       "   1,\n",
       "   6,\n",
       "   3,\n",
       "   46,\n",
       "   42,\n",
       "   48,\n",
       "   27,\n",
       "   55,\n",
       "   15,\n",
       "   37,\n",
       "   10,\n",
       "   34,\n",
       "   5,\n",
       "   41,\n",
       "   43,\n",
       "   52,\n",
       "   16,\n",
       "   33,\n",
       "   28,\n",
       "   47,\n",
       "   14,\n",
       "   18,\n",
       "   9,\n",
       "   39,\n",
       "   23,\n",
       "   51,\n",
       "   31,\n",
       "   50,\n",
       "   49,\n",
       "   53,\n",
       "   12,\n",
       "   17,\n",
       "   8,\n",
       "   32,\n",
       "   0,\n",
       "   7,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [19,\n",
       "   59,\n",
       "   29,\n",
       "   77,\n",
       "   37,\n",
       "   43,\n",
       "   78,\n",
       "   79,\n",
       "   0,\n",
       "   12,\n",
       "   3,\n",
       "   57,\n",
       "   1,\n",
       "   6,\n",
       "   8,\n",
       "   36,\n",
       "   48,\n",
       "   74,\n",
       "   44,\n",
       "   58,\n",
       "   15,\n",
       "   28,\n",
       "   18,\n",
       "   75,\n",
       "   21,\n",
       "   38,\n",
       "   33,\n",
       "   76,\n",
       "   40,\n",
       "   45,\n",
       "   46,\n",
       "   56,\n",
       "   39,\n",
       "   55,\n",
       "   17,\n",
       "   60,\n",
       "   27,\n",
       "   66,\n",
       "   11,\n",
       "   65,\n",
       "   7,\n",
       "   70,\n",
       "   52,\n",
       "   61,\n",
       "   54,\n",
       "   69,\n",
       "   4,\n",
       "   34,\n",
       "   23,\n",
       "   50,\n",
       "   30,\n",
       "   49,\n",
       "   53,\n",
       "   73,\n",
       "   26,\n",
       "   72,\n",
       "   31,\n",
       "   51,\n",
       "   35,\n",
       "   62,\n",
       "   10,\n",
       "   22,\n",
       "   5,\n",
       "   24,\n",
       "   63,\n",
       "   67,\n",
       "   64,\n",
       "   68,\n",
       "   9,\n",
       "   42,\n",
       "   25,\n",
       "   32,\n",
       "   16,\n",
       "   47,\n",
       "   20,\n",
       "   71,\n",
       "   13,\n",
       "   41,\n",
       "   2,\n",
       "   14],\n",
       "  [22,\n",
       "   38,\n",
       "   10,\n",
       "   51,\n",
       "   4,\n",
       "   32,\n",
       "   54,\n",
       "   55,\n",
       "   16,\n",
       "   24,\n",
       "   15,\n",
       "   53,\n",
       "   36,\n",
       "   45,\n",
       "   3,\n",
       "   7,\n",
       "   43,\n",
       "   52,\n",
       "   17,\n",
       "   42,\n",
       "   13,\n",
       "   27,\n",
       "   29,\n",
       "   50,\n",
       "   12,\n",
       "   40,\n",
       "   0,\n",
       "   28,\n",
       "   31,\n",
       "   37,\n",
       "   25,\n",
       "   34,\n",
       "   2,\n",
       "   21,\n",
       "   8,\n",
       "   44,\n",
       "   9,\n",
       "   49,\n",
       "   11,\n",
       "   46,\n",
       "   41,\n",
       "   47,\n",
       "   14,\n",
       "   18,\n",
       "   1,\n",
       "   48,\n",
       "   5,\n",
       "   35,\n",
       "   26,\n",
       "   39,\n",
       "   23,\n",
       "   30,\n",
       "   19,\n",
       "   33,\n",
       "   6,\n",
       "   20,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [17,\n",
       "   28,\n",
       "   36,\n",
       "   58,\n",
       "   11,\n",
       "   29,\n",
       "   31,\n",
       "   40,\n",
       "   26,\n",
       "   47,\n",
       "   52,\n",
       "   53,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   15,\n",
       "   41,\n",
       "   10,\n",
       "   49,\n",
       "   23,\n",
       "   42,\n",
       "   16,\n",
       "   54,\n",
       "   19,\n",
       "   57,\n",
       "   27,\n",
       "   39,\n",
       "   13,\n",
       "   38,\n",
       "   6,\n",
       "   8,\n",
       "   51,\n",
       "   56,\n",
       "   24,\n",
       "   46,\n",
       "   9,\n",
       "   37,\n",
       "   48,\n",
       "   55,\n",
       "   3,\n",
       "   34,\n",
       "   50,\n",
       "   59,\n",
       "   25,\n",
       "   30,\n",
       "   20,\n",
       "   33,\n",
       "   21,\n",
       "   32,\n",
       "   7,\n",
       "   43,\n",
       "   0,\n",
       "   22,\n",
       "   1,\n",
       "   5,\n",
       "   12,\n",
       "   35,\n",
       "   18,\n",
       "   45,\n",
       "   2,\n",
       "   14,\n",
       "   4,\n",
       "   44],\n",
       "  [19,\n",
       "   36,\n",
       "   21,\n",
       "   40,\n",
       "   0,\n",
       "   13,\n",
       "   28,\n",
       "   30,\n",
       "   2,\n",
       "   11,\n",
       "   8,\n",
       "   46,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   7,\n",
       "   38,\n",
       "   26,\n",
       "   53,\n",
       "   10,\n",
       "   48,\n",
       "   54,\n",
       "   55,\n",
       "   1,\n",
       "   24,\n",
       "   14,\n",
       "   37,\n",
       "   5,\n",
       "   25,\n",
       "   34,\n",
       "   43,\n",
       "   17,\n",
       "   41,\n",
       "   33,\n",
       "   44,\n",
       "   12,\n",
       "   23,\n",
       "   22,\n",
       "   45,\n",
       "   6,\n",
       "   9,\n",
       "   39,\n",
       "   42,\n",
       "   16,\n",
       "   18,\n",
       "   20,\n",
       "   35,\n",
       "   27,\n",
       "   31,\n",
       "   3,\n",
       "   4,\n",
       "   15,\n",
       "   47,\n",
       "   29,\n",
       "   32,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [17,\n",
       "   37,\n",
       "   41,\n",
       "   42,\n",
       "   12,\n",
       "   16,\n",
       "   23,\n",
       "   32,\n",
       "   35,\n",
       "   52,\n",
       "   34,\n",
       "   43,\n",
       "   5,\n",
       "   24,\n",
       "   3,\n",
       "   36,\n",
       "   29,\n",
       "   53,\n",
       "   8,\n",
       "   21,\n",
       "   19,\n",
       "   44,\n",
       "   22,\n",
       "   38,\n",
       "   46,\n",
       "   51,\n",
       "   15,\n",
       "   18,\n",
       "   0,\n",
       "   6,\n",
       "   54,\n",
       "   55,\n",
       "   9,\n",
       "   45,\n",
       "   11,\n",
       "   31,\n",
       "   26,\n",
       "   47,\n",
       "   2,\n",
       "   14,\n",
       "   48,\n",
       "   50,\n",
       "   1,\n",
       "   28,\n",
       "   10,\n",
       "   27,\n",
       "   4,\n",
       "   13,\n",
       "   7,\n",
       "   30,\n",
       "   25,\n",
       "   39,\n",
       "   20,\n",
       "   40,\n",
       "   33,\n",
       "   49,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [0,\n",
       "   35,\n",
       "   5,\n",
       "   44,\n",
       "   12,\n",
       "   31,\n",
       "   37,\n",
       "   40,\n",
       "   18,\n",
       "   51,\n",
       "   33,\n",
       "   34,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   30,\n",
       "   64,\n",
       "   36,\n",
       "   69,\n",
       "   9,\n",
       "   32,\n",
       "   1,\n",
       "   6,\n",
       "   17,\n",
       "   38,\n",
       "   53,\n",
       "   56,\n",
       "   2,\n",
       "   19,\n",
       "   22,\n",
       "   59,\n",
       "   52,\n",
       "   60,\n",
       "   23,\n",
       "   54,\n",
       "   14,\n",
       "   46,\n",
       "   20,\n",
       "   25,\n",
       "   29,\n",
       "   63,\n",
       "   7,\n",
       "   13,\n",
       "   8,\n",
       "   62,\n",
       "   42,\n",
       "   43,\n",
       "   10,\n",
       "   50,\n",
       "   16,\n",
       "   24,\n",
       "   15,\n",
       "   28,\n",
       "   70,\n",
       "   71,\n",
       "   48,\n",
       "   58,\n",
       "   27,\n",
       "   49,\n",
       "   41,\n",
       "   45,\n",
       "   21,\n",
       "   39,\n",
       "   26,\n",
       "   57,\n",
       "   3,\n",
       "   4,\n",
       "   47,\n",
       "   61,\n",
       "   11,\n",
       "   55,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [5,\n",
       "   14,\n",
       "   39,\n",
       "   40,\n",
       "   2,\n",
       "   30,\n",
       "   21,\n",
       "   31,\n",
       "   12,\n",
       "   18,\n",
       "   25,\n",
       "   46,\n",
       "   33,\n",
       "   36,\n",
       "   13,\n",
       "   16,\n",
       "   29,\n",
       "   37,\n",
       "   1,\n",
       "   15,\n",
       "   10,\n",
       "   42,\n",
       "   8,\n",
       "   17,\n",
       "   27,\n",
       "   43,\n",
       "   4,\n",
       "   35,\n",
       "   22,\n",
       "   32,\n",
       "   7,\n",
       "   28,\n",
       "   19,\n",
       "   20,\n",
       "   24,\n",
       "   47,\n",
       "   23,\n",
       "   41,\n",
       "   3,\n",
       "   26,\n",
       "   0,\n",
       "   45,\n",
       "   9,\n",
       "   34,\n",
       "   11,\n",
       "   44,\n",
       "   6,\n",
       "   38],\n",
       "  [16,\n",
       "   42,\n",
       "   19,\n",
       "   43,\n",
       "   13,\n",
       "   39,\n",
       "   8,\n",
       "   10,\n",
       "   11,\n",
       "   32,\n",
       "   9,\n",
       "   37,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   26,\n",
       "   29,\n",
       "   22,\n",
       "   28,\n",
       "   7,\n",
       "   20,\n",
       "   33,\n",
       "   38,\n",
       "   0,\n",
       "   6,\n",
       "   4,\n",
       "   17,\n",
       "   2,\n",
       "   30,\n",
       "   1,\n",
       "   27,\n",
       "   12,\n",
       "   15,\n",
       "   25,\n",
       "   31,\n",
       "   3,\n",
       "   40,\n",
       "   23,\n",
       "   35,\n",
       "   14,\n",
       "   24,\n",
       "   34,\n",
       "   41,\n",
       "   21,\n",
       "   36,\n",
       "   5,\n",
       "   18],\n",
       "  [15,\n",
       "   27,\n",
       "   18,\n",
       "   19,\n",
       "   14,\n",
       "   24,\n",
       "   3,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   5,\n",
       "   16,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   23,\n",
       "   33,\n",
       "   22,\n",
       "   32,\n",
       "   0,\n",
       "   28,\n",
       "   2,\n",
       "   11,\n",
       "   7,\n",
       "   34,\n",
       "   6,\n",
       "   25,\n",
       "   29,\n",
       "   30,\n",
       "   8,\n",
       "   35,\n",
       "   9,\n",
       "   10,\n",
       "   4,\n",
       "   31,\n",
       "   1,\n",
       "   21,\n",
       "   13,\n",
       "   26,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [23,\n",
       "   40,\n",
       "   34,\n",
       "   52,\n",
       "   16,\n",
       "   24,\n",
       "   5,\n",
       "   51,\n",
       "   12,\n",
       "   48,\n",
       "   50,\n",
       "   53,\n",
       "   11,\n",
       "   25,\n",
       "   21,\n",
       "   45,\n",
       "   28,\n",
       "   49,\n",
       "   43,\n",
       "   46,\n",
       "   13,\n",
       "   18,\n",
       "   7,\n",
       "   22,\n",
       "   1,\n",
       "   2,\n",
       "   15,\n",
       "   42,\n",
       "   29,\n",
       "   41,\n",
       "   33,\n",
       "   47,\n",
       "   14,\n",
       "   30,\n",
       "   20,\n",
       "   31,\n",
       "   36,\n",
       "   37,\n",
       "   54,\n",
       "   55,\n",
       "   9,\n",
       "   39,\n",
       "   17,\n",
       "   26,\n",
       "   10,\n",
       "   19,\n",
       "   0,\n",
       "   32,\n",
       "   35,\n",
       "   38,\n",
       "   3,\n",
       "   4,\n",
       "   8,\n",
       "   27,\n",
       "   6,\n",
       "   44,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [25,\n",
       "   63,\n",
       "   16,\n",
       "   39,\n",
       "   20,\n",
       "   41,\n",
       "   55,\n",
       "   56,\n",
       "   49,\n",
       "   65,\n",
       "   14,\n",
       "   45,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   17,\n",
       "   26,\n",
       "   28,\n",
       "   60,\n",
       "   21,\n",
       "   22,\n",
       "   5,\n",
       "   50,\n",
       "   32,\n",
       "   58,\n",
       "   52,\n",
       "   66,\n",
       "   30,\n",
       "   53,\n",
       "   11,\n",
       "   35,\n",
       "   13,\n",
       "   51,\n",
       "   7,\n",
       "   12,\n",
       "   44,\n",
       "   61,\n",
       "   37,\n",
       "   46,\n",
       "   3,\n",
       "   57,\n",
       "   15,\n",
       "   62,\n",
       "   38,\n",
       "   42,\n",
       "   8,\n",
       "   47,\n",
       "   6,\n",
       "   9,\n",
       "   0,\n",
       "   67,\n",
       "   4,\n",
       "   59,\n",
       "   18,\n",
       "   27,\n",
       "   2,\n",
       "   36,\n",
       "   1,\n",
       "   24,\n",
       "   19,\n",
       "   29,\n",
       "   10,\n",
       "   64,\n",
       "   33,\n",
       "   34,\n",
       "   31,\n",
       "   40,\n",
       "   48,\n",
       "   54,\n",
       "   23,\n",
       "   43,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [40,\n",
       "   59,\n",
       "   21,\n",
       "   55,\n",
       "   15,\n",
       "   48,\n",
       "   29,\n",
       "   57,\n",
       "   32,\n",
       "   58,\n",
       "   2,\n",
       "   42,\n",
       "   39,\n",
       "   43,\n",
       "   7,\n",
       "   60,\n",
       "   12,\n",
       "   22,\n",
       "   24,\n",
       "   49,\n",
       "   25,\n",
       "   33,\n",
       "   5,\n",
       "   34,\n",
       "   27,\n",
       "   47,\n",
       "   14,\n",
       "   56,\n",
       "   19,\n",
       "   28,\n",
       "   44,\n",
       "   51,\n",
       "   0,\n",
       "   23,\n",
       "   1,\n",
       "   62,\n",
       "   16,\n",
       "   52,\n",
       "   36,\n",
       "   45,\n",
       "   3,\n",
       "   35,\n",
       "   50,\n",
       "   53,\n",
       "   17,\n",
       "   46,\n",
       "   9,\n",
       "   38,\n",
       "   11,\n",
       "   61,\n",
       "   6,\n",
       "   63,\n",
       "   4,\n",
       "   20,\n",
       "   8,\n",
       "   13,\n",
       "   26,\n",
       "   37,\n",
       "   10,\n",
       "   30,\n",
       "   18,\n",
       "   41,\n",
       "   31,\n",
       "   54],\n",
       "  [10,\n",
       "   19,\n",
       "   2,\n",
       "   18,\n",
       "   4,\n",
       "   26,\n",
       "   0,\n",
       "   7,\n",
       "   14,\n",
       "   27,\n",
       "   5,\n",
       "   11,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   21,\n",
       "   25,\n",
       "   9,\n",
       "   23,\n",
       "   12,\n",
       "   16,\n",
       "   3,\n",
       "   17,\n",
       "   1,\n",
       "   6,\n",
       "   15,\n",
       "   20,\n",
       "   8,\n",
       "   13,\n",
       "   22,\n",
       "   24],\n",
       "  [19,\n",
       "   27,\n",
       "   17,\n",
       "   33,\n",
       "   10,\n",
       "   20,\n",
       "   38,\n",
       "   39,\n",
       "   24,\n",
       "   34,\n",
       "   9,\n",
       "   36,\n",
       "   28,\n",
       "   31,\n",
       "   4,\n",
       "   5,\n",
       "   13,\n",
       "   22,\n",
       "   25,\n",
       "   35,\n",
       "   2,\n",
       "   37,\n",
       "   23,\n",
       "   32,\n",
       "   18,\n",
       "   29,\n",
       "   7,\n",
       "   16,\n",
       "   8,\n",
       "   30,\n",
       "   0,\n",
       "   6,\n",
       "   12,\n",
       "   14,\n",
       "   15,\n",
       "   21,\n",
       "   3,\n",
       "   11,\n",
       "   1,\n",
       "   26,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [30,\n",
       "   53,\n",
       "   9,\n",
       "   36,\n",
       "   27,\n",
       "   40,\n",
       "   12,\n",
       "   22,\n",
       "   2,\n",
       "   32,\n",
       "   0,\n",
       "   15,\n",
       "   10,\n",
       "   39,\n",
       "   16,\n",
       "   38,\n",
       "   26,\n",
       "   31,\n",
       "   7,\n",
       "   21,\n",
       "   50,\n",
       "   62,\n",
       "   3,\n",
       "   69,\n",
       "   24,\n",
       "   46,\n",
       "   8,\n",
       "   61,\n",
       "   37,\n",
       "   45,\n",
       "   28,\n",
       "   54,\n",
       "   20,\n",
       "   35,\n",
       "   6,\n",
       "   33,\n",
       "   25,\n",
       "   41,\n",
       "   66,\n",
       "   67,\n",
       "   59,\n",
       "   68,\n",
       "   42,\n",
       "   63,\n",
       "   17,\n",
       "   64,\n",
       "   29,\n",
       "   43,\n",
       "   18,\n",
       "   58,\n",
       "   44,\n",
       "   65,\n",
       "   4,\n",
       "   60,\n",
       "   70,\n",
       "   71,\n",
       "   19,\n",
       "   57,\n",
       "   55,\n",
       "   56,\n",
       "   13,\n",
       "   14,\n",
       "   23,\n",
       "   48,\n",
       "   1,\n",
       "   51,\n",
       "   11,\n",
       "   47,\n",
       "   5,\n",
       "   52,\n",
       "   34,\n",
       "   49,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [7,\n",
       "   8,\n",
       "   5,\n",
       "   11,\n",
       "   20,\n",
       "   30,\n",
       "   21,\n",
       "   25,\n",
       "   14,\n",
       "   23,\n",
       "   13,\n",
       "   31,\n",
       "   6,\n",
       "   26,\n",
       "   15,\n",
       "   19,\n",
       "   4,\n",
       "   24,\n",
       "   16,\n",
       "   27,\n",
       "   0,\n",
       "   18,\n",
       "   2,\n",
       "   9,\n",
       "   12,\n",
       "   29,\n",
       "   3,\n",
       "   17,\n",
       "   1,\n",
       "   10,\n",
       "   22,\n",
       "   28],\n",
       "  [3,\n",
       "   28,\n",
       "   2,\n",
       "   4,\n",
       "   0,\n",
       "   30,\n",
       "   27,\n",
       "   33,\n",
       "   17,\n",
       "   29,\n",
       "   23,\n",
       "   36,\n",
       "   12,\n",
       "   15,\n",
       "   5,\n",
       "   21,\n",
       "   6,\n",
       "   18,\n",
       "   11,\n",
       "   31,\n",
       "   10,\n",
       "   22,\n",
       "   16,\n",
       "   19,\n",
       "   13,\n",
       "   32,\n",
       "   20,\n",
       "   37,\n",
       "   9,\n",
       "   25,\n",
       "   1,\n",
       "   24,\n",
       "   8,\n",
       "   34,\n",
       "   7,\n",
       "   26,\n",
       "   14,\n",
       "   35,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [10,\n",
       "   59,\n",
       "   8,\n",
       "   49,\n",
       "   34,\n",
       "   36,\n",
       "   16,\n",
       "   56,\n",
       "   37,\n",
       "   43,\n",
       "   33,\n",
       "   52,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   11,\n",
       "   63,\n",
       "   2,\n",
       "   40,\n",
       "   6,\n",
       "   45,\n",
       "   0,\n",
       "   62,\n",
       "   1,\n",
       "   21,\n",
       "   26,\n",
       "   54,\n",
       "   39,\n",
       "   60,\n",
       "   12,\n",
       "   35,\n",
       "   32,\n",
       "   48,\n",
       "   7,\n",
       "   38,\n",
       "   31,\n",
       "   58,\n",
       "   46,\n",
       "   47,\n",
       "   22,\n",
       "   55,\n",
       "   24,\n",
       "   30,\n",
       "   23,\n",
       "   53,\n",
       "   15,\n",
       "   69,\n",
       "   9,\n",
       "   64,\n",
       "   50,\n",
       "   57,\n",
       "   14,\n",
       "   19,\n",
       "   70,\n",
       "   71,\n",
       "   18,\n",
       "   28,\n",
       "   20,\n",
       "   51,\n",
       "   42,\n",
       "   44,\n",
       "   5,\n",
       "   61,\n",
       "   25,\n",
       "   27,\n",
       "   13,\n",
       "   41,\n",
       "   4,\n",
       "   29,\n",
       "   3,\n",
       "   17,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [19,\n",
       "   23,\n",
       "   21,\n",
       "   39,\n",
       "   1,\n",
       "   26,\n",
       "   28,\n",
       "   34,\n",
       "   2,\n",
       "   16,\n",
       "   0,\n",
       "   8,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   17,\n",
       "   20,\n",
       "   10,\n",
       "   32,\n",
       "   11,\n",
       "   25,\n",
       "   14,\n",
       "   35,\n",
       "   22,\n",
       "   27,\n",
       "   7,\n",
       "   45,\n",
       "   3,\n",
       "   18,\n",
       "   33,\n",
       "   38,\n",
       "   29,\n",
       "   36,\n",
       "   15,\n",
       "   31,\n",
       "   5,\n",
       "   9,\n",
       "   46,\n",
       "   47,\n",
       "   4,\n",
       "   37,\n",
       "   6,\n",
       "   40,\n",
       "   24,\n",
       "   30,\n",
       "   12,\n",
       "   13],\n",
       "  [20,\n",
       "   33,\n",
       "   11,\n",
       "   29,\n",
       "   3,\n",
       "   39,\n",
       "   12,\n",
       "   45,\n",
       "   22,\n",
       "   44,\n",
       "   30,\n",
       "   41,\n",
       "   9,\n",
       "   37,\n",
       "   5,\n",
       "   26,\n",
       "   32,\n",
       "   40,\n",
       "   24,\n",
       "   42,\n",
       "   4,\n",
       "   43,\n",
       "   34,\n",
       "   36,\n",
       "   16,\n",
       "   21,\n",
       "   25,\n",
       "   28,\n",
       "   15,\n",
       "   31,\n",
       "   0,\n",
       "   18,\n",
       "   2,\n",
       "   7,\n",
       "   8,\n",
       "   35,\n",
       "   6,\n",
       "   19,\n",
       "   46,\n",
       "   47,\n",
       "   13,\n",
       "   17,\n",
       "   27,\n",
       "   38,\n",
       "   14,\n",
       "   23,\n",
       "   1,\n",
       "   10],\n",
       "  [18,\n",
       "   49,\n",
       "   22,\n",
       "   35,\n",
       "   5,\n",
       "   36,\n",
       "   8,\n",
       "   33,\n",
       "   11,\n",
       "   45,\n",
       "   14,\n",
       "   15,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   25,\n",
       "   40,\n",
       "   29,\n",
       "   37,\n",
       "   16,\n",
       "   44,\n",
       "   31,\n",
       "   43,\n",
       "   3,\n",
       "   28,\n",
       "   13,\n",
       "   24,\n",
       "   38,\n",
       "   42,\n",
       "   7,\n",
       "   10,\n",
       "   21,\n",
       "   41,\n",
       "   4,\n",
       "   26,\n",
       "   12,\n",
       "   51,\n",
       "   0,\n",
       "   20,\n",
       "   32,\n",
       "   48,\n",
       "   6,\n",
       "   39,\n",
       "   9,\n",
       "   34,\n",
       "   19,\n",
       "   23,\n",
       "   2,\n",
       "   46,\n",
       "   27,\n",
       "   47,\n",
       "   1,\n",
       "   50,\n",
       "   17,\n",
       "   30,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [12,\n",
       "   40,\n",
       "   18,\n",
       "   25,\n",
       "   5,\n",
       "   11,\n",
       "   50,\n",
       "   51,\n",
       "   32,\n",
       "   36,\n",
       "   1,\n",
       "   48,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   15,\n",
       "   31,\n",
       "   20,\n",
       "   43,\n",
       "   26,\n",
       "   34,\n",
       "   7,\n",
       "   13,\n",
       "   27,\n",
       "   41,\n",
       "   30,\n",
       "   42,\n",
       "   23,\n",
       "   38,\n",
       "   33,\n",
       "   45,\n",
       "   3,\n",
       "   10,\n",
       "   35,\n",
       "   47,\n",
       "   22,\n",
       "   28,\n",
       "   0,\n",
       "   29,\n",
       "   2,\n",
       "   49,\n",
       "   4,\n",
       "   39,\n",
       "   9,\n",
       "   37,\n",
       "   14,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   16,\n",
       "   46,\n",
       "   6,\n",
       "   44,\n",
       "   8,\n",
       "   17,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [15,\n",
       "   33,\n",
       "   7,\n",
       "   23,\n",
       "   11,\n",
       "   20,\n",
       "   26,\n",
       "   27,\n",
       "   14,\n",
       "   28,\n",
       "   13,\n",
       "   25,\n",
       "   1,\n",
       "   31,\n",
       "   8,\n",
       "   34,\n",
       "   12,\n",
       "   22,\n",
       "   5,\n",
       "   35,\n",
       "   4,\n",
       "   21,\n",
       "   3,\n",
       "   6,\n",
       "   30,\n",
       "   32,\n",
       "   9,\n",
       "   18,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   17,\n",
       "   29,\n",
       "   19,\n",
       "   24,\n",
       "   0,\n",
       "   2,\n",
       "   10,\n",
       "   16,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [37,\n",
       "   53,\n",
       "   26,\n",
       "   61,\n",
       "   6,\n",
       "   14,\n",
       "   50,\n",
       "   55,\n",
       "   12,\n",
       "   47,\n",
       "   3,\n",
       "   10,\n",
       "   32,\n",
       "   40,\n",
       "   4,\n",
       "   19,\n",
       "   56,\n",
       "   58,\n",
       "   18,\n",
       "   42,\n",
       "   36,\n",
       "   46,\n",
       "   1,\n",
       "   54,\n",
       "   9,\n",
       "   30,\n",
       "   34,\n",
       "   52,\n",
       "   11,\n",
       "   57,\n",
       "   43,\n",
       "   60,\n",
       "   0,\n",
       "   5,\n",
       "   21,\n",
       "   33,\n",
       "   15,\n",
       "   45,\n",
       "   8,\n",
       "   38,\n",
       "   35,\n",
       "   48,\n",
       "   20,\n",
       "   44,\n",
       "   28,\n",
       "   31,\n",
       "   24,\n",
       "   59,\n",
       "   17,\n",
       "   29,\n",
       "   39,\n",
       "   51,\n",
       "   16,\n",
       "   41,\n",
       "   2,\n",
       "   25,\n",
       "   22,\n",
       "   23,\n",
       "   7,\n",
       "   27,\n",
       "   13,\n",
       "   49,\n",
       "   62,\n",
       "   63],\n",
       "  [0,\n",
       "   19,\n",
       "   22,\n",
       "   24,\n",
       "   7,\n",
       "   8,\n",
       "   3,\n",
       "   20,\n",
       "   10,\n",
       "   14,\n",
       "   2,\n",
       "   25,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   17,\n",
       "   18,\n",
       "   9,\n",
       "   27,\n",
       "   6,\n",
       "   15,\n",
       "   4,\n",
       "   11,\n",
       "   13,\n",
       "   16,\n",
       "   1,\n",
       "   5,\n",
       "   23,\n",
       "   26,\n",
       "   12,\n",
       "   21],\n",
       "  [4,\n",
       "   16,\n",
       "   8,\n",
       "   18,\n",
       "   6,\n",
       "   9,\n",
       "   12,\n",
       "   15,\n",
       "   3,\n",
       "   7,\n",
       "   1,\n",
       "   13,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   0,\n",
       "   5,\n",
       "   14,\n",
       "   23,\n",
       "   2,\n",
       "   17,\n",
       "   10,\n",
       "   11,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [4,\n",
       "   12,\n",
       "   1,\n",
       "   5,\n",
       "   10,\n",
       "   14,\n",
       "   17,\n",
       "   18,\n",
       "   6,\n",
       "   16,\n",
       "   11,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   2,\n",
       "   13,\n",
       "   3,\n",
       "   9,\n",
       "   0,\n",
       "   8,\n",
       "   7,\n",
       "   15,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [0,\n",
       "   48,\n",
       "   34,\n",
       "   46,\n",
       "   6,\n",
       "   28,\n",
       "   8,\n",
       "   23,\n",
       "   1,\n",
       "   14,\n",
       "   7,\n",
       "   30,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   15,\n",
       "   17,\n",
       "   26,\n",
       "   38,\n",
       "   40,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   31,\n",
       "   33,\n",
       "   10,\n",
       "   19,\n",
       "   25,\n",
       "   29,\n",
       "   18,\n",
       "   21,\n",
       "   36,\n",
       "   41,\n",
       "   20,\n",
       "   43,\n",
       "   16,\n",
       "   35,\n",
       "   12,\n",
       "   55,\n",
       "   2,\n",
       "   5,\n",
       "   3,\n",
       "   39,\n",
       "   11,\n",
       "   50,\n",
       "   13,\n",
       "   27,\n",
       "   24,\n",
       "   37,\n",
       "   4,\n",
       "   9,\n",
       "   22,\n",
       "   32,\n",
       "   42,\n",
       "   44,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [11,\n",
       "   74,\n",
       "   14,\n",
       "   86,\n",
       "   34,\n",
       "   40,\n",
       "   110,\n",
       "   111,\n",
       "   50,\n",
       "   102,\n",
       "   81,\n",
       "   107,\n",
       "   10,\n",
       "   109,\n",
       "   1,\n",
       "   45,\n",
       "   49,\n",
       "   104,\n",
       "   63,\n",
       "   98,\n",
       "   57,\n",
       "   71,\n",
       "   30,\n",
       "   36,\n",
       "   20,\n",
       "   72,\n",
       "   25,\n",
       "   29,\n",
       "   41,\n",
       "   52,\n",
       "   48,\n",
       "   55,\n",
       "   23,\n",
       "   54,\n",
       "   51,\n",
       "   84,\n",
       "   80,\n",
       "   87,\n",
       "   91,\n",
       "   101,\n",
       "   88,\n",
       "   90,\n",
       "   26,\n",
       "   105,\n",
       "   19,\n",
       "   103,\n",
       "   60,\n",
       "   70,\n",
       "   6,\n",
       "   95,\n",
       "   76,\n",
       "   78,\n",
       "   17,\n",
       "   94,\n",
       "   27,\n",
       "   92,\n",
       "   66,\n",
       "   93,\n",
       "   32,\n",
       "   85,\n",
       "   16,\n",
       "   96,\n",
       "   5,\n",
       "   68,\n",
       "   59,\n",
       "   67,\n",
       "   42,\n",
       "   82,\n",
       "   77,\n",
       "   97,\n",
       "   89,\n",
       "   108,\n",
       "   2,\n",
       "   4,\n",
       "   7,\n",
       "   31,\n",
       "   39,\n",
       "   43,\n",
       "   3,\n",
       "   56,\n",
       "   62,\n",
       "   73,\n",
       "   24,\n",
       "   99,\n",
       "   9,\n",
       "   46,\n",
       "   44,\n",
       "   69,\n",
       "   8,\n",
       "   53,\n",
       "   0,\n",
       "   58,\n",
       "   38,\n",
       "   83,\n",
       "   15,\n",
       "   61,\n",
       "   22,\n",
       "   100,\n",
       "   37,\n",
       "   47,\n",
       "   28,\n",
       "   79,\n",
       "   33,\n",
       "   64,\n",
       "   65,\n",
       "   106,\n",
       "   21,\n",
       "   35,\n",
       "   13,\n",
       "   18,\n",
       "   12,\n",
       "   75],\n",
       "  [24,\n",
       "   29,\n",
       "   23,\n",
       "   57,\n",
       "   7,\n",
       "   9,\n",
       "   18,\n",
       "   49,\n",
       "   46,\n",
       "   52,\n",
       "   19,\n",
       "   38,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   27,\n",
       "   43,\n",
       "   50,\n",
       "   67,\n",
       "   11,\n",
       "   59,\n",
       "   6,\n",
       "   26,\n",
       "   37,\n",
       "   48,\n",
       "   22,\n",
       "   53,\n",
       "   30,\n",
       "   32,\n",
       "   25,\n",
       "   58,\n",
       "   54,\n",
       "   61,\n",
       "   10,\n",
       "   45,\n",
       "   13,\n",
       "   35,\n",
       "   60,\n",
       "   62,\n",
       "   0,\n",
       "   16,\n",
       "   33,\n",
       "   34,\n",
       "   12,\n",
       "   44,\n",
       "   68,\n",
       "   69,\n",
       "   2,\n",
       "   20,\n",
       "   40,\n",
       "   70,\n",
       "   31,\n",
       "   56,\n",
       "   3,\n",
       "   15,\n",
       "   17,\n",
       "   21,\n",
       "   39,\n",
       "   71,\n",
       "   4,\n",
       "   36,\n",
       "   42,\n",
       "   55,\n",
       "   1,\n",
       "   51,\n",
       "   5,\n",
       "   41,\n",
       "   8,\n",
       "   14,\n",
       "   28,\n",
       "   47,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [1,\n",
       "   26,\n",
       "   13,\n",
       "   36,\n",
       "   12,\n",
       "   27,\n",
       "   0,\n",
       "   19,\n",
       "   23,\n",
       "   34,\n",
       "   3,\n",
       "   18,\n",
       "   8,\n",
       "   11,\n",
       "   15,\n",
       "   31,\n",
       "   10,\n",
       "   32,\n",
       "   6,\n",
       "   7,\n",
       "   4,\n",
       "   28,\n",
       "   37,\n",
       "   38,\n",
       "   2,\n",
       "   24,\n",
       "   35,\n",
       "   39,\n",
       "   17,\n",
       "   20,\n",
       "   30,\n",
       "   33,\n",
       "   16,\n",
       "   25,\n",
       "   5,\n",
       "   21,\n",
       "   9,\n",
       "   14,\n",
       "   22,\n",
       "   29,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [32,\n",
       "   39,\n",
       "   26,\n",
       "   30,\n",
       "   14,\n",
       "   27,\n",
       "   35,\n",
       "   36,\n",
       "   34,\n",
       "   42,\n",
       "   11,\n",
       "   16,\n",
       "   12,\n",
       "   22,\n",
       "   29,\n",
       "   33,\n",
       "   21,\n",
       "   31,\n",
       "   20,\n",
       "   45,\n",
       "   7,\n",
       "   9,\n",
       "   0,\n",
       "   47,\n",
       "   6,\n",
       "   15,\n",
       "   1,\n",
       "   28,\n",
       "   18,\n",
       "   38,\n",
       "   10,\n",
       "   37,\n",
       "   17,\n",
       "   44,\n",
       "   23,\n",
       "   40,\n",
       "   24,\n",
       "   46,\n",
       "   2,\n",
       "   8,\n",
       "   3,\n",
       "   19,\n",
       "   5,\n",
       "   41,\n",
       "   25,\n",
       "   43,\n",
       "   4,\n",
       "   13],\n",
       "  [10,\n",
       "   11,\n",
       "   21,\n",
       "   24,\n",
       "   9,\n",
       "   18,\n",
       "   0,\n",
       "   23,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   17,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   8,\n",
       "   15,\n",
       "   13,\n",
       "   29,\n",
       "   3,\n",
       "   19,\n",
       "   30,\n",
       "   31,\n",
       "   5,\n",
       "   22,\n",
       "   16,\n",
       "   20,\n",
       "   12,\n",
       "   14,\n",
       "   1,\n",
       "   7],\n",
       "  [24,\n",
       "   30,\n",
       "   3,\n",
       "   37,\n",
       "   26,\n",
       "   34,\n",
       "   29,\n",
       "   40,\n",
       "   32,\n",
       "   35,\n",
       "   12,\n",
       "   28,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   25,\n",
       "   31,\n",
       "   16,\n",
       "   17,\n",
       "   7,\n",
       "   21,\n",
       "   33,\n",
       "   47,\n",
       "   22,\n",
       "   23,\n",
       "   9,\n",
       "   14,\n",
       "   15,\n",
       "   20,\n",
       "   0,\n",
       "   6,\n",
       "   5,\n",
       "   41,\n",
       "   27,\n",
       "   39,\n",
       "   2,\n",
       "   10,\n",
       "   13,\n",
       "   19,\n",
       "   4,\n",
       "   8,\n",
       "   11,\n",
       "   42,\n",
       "   18,\n",
       "   36,\n",
       "   1,\n",
       "   38],\n",
       "  [36,\n",
       "   42,\n",
       "   10,\n",
       "   13,\n",
       "   30,\n",
       "   38,\n",
       "   1,\n",
       "   53,\n",
       "   5,\n",
       "   64,\n",
       "   44,\n",
       "   65,\n",
       "   6,\n",
       "   25,\n",
       "   8,\n",
       "   32,\n",
       "   41,\n",
       "   47,\n",
       "   28,\n",
       "   59,\n",
       "   7,\n",
       "   12,\n",
       "   9,\n",
       "   57,\n",
       "   4,\n",
       "   24,\n",
       "   17,\n",
       "   19,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   37,\n",
       "   46,\n",
       "   51,\n",
       "   56,\n",
       "   15,\n",
       "   21,\n",
       "   70,\n",
       "   71,\n",
       "   23,\n",
       "   27,\n",
       "   29,\n",
       "   54,\n",
       "   40,\n",
       "   45,\n",
       "   60,\n",
       "   61,\n",
       "   14,\n",
       "   33,\n",
       "   39,\n",
       "   58,\n",
       "   26,\n",
       "   31,\n",
       "   35,\n",
       "   55,\n",
       "   20,\n",
       "   50,\n",
       "   0,\n",
       "   43,\n",
       "   3,\n",
       "   16,\n",
       "   2,\n",
       "   49,\n",
       "   48,\n",
       "   52,\n",
       "   34,\n",
       "   62,\n",
       "   18,\n",
       "   63,\n",
       "   11,\n",
       "   22,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [7,\n",
       "   47,\n",
       "   27,\n",
       "   48,\n",
       "   1,\n",
       "   49,\n",
       "   39,\n",
       "   46,\n",
       "   6,\n",
       "   42,\n",
       "   8,\n",
       "   25,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   26,\n",
       "   40,\n",
       "   10,\n",
       "   43,\n",
       "   35,\n",
       "   45,\n",
       "   9,\n",
       "   28,\n",
       "   0,\n",
       "   29,\n",
       "   11,\n",
       "   20,\n",
       "   22,\n",
       "   41,\n",
       "   3,\n",
       "   4,\n",
       "   21,\n",
       "   36,\n",
       "   12,\n",
       "   18,\n",
       "   2,\n",
       "   14,\n",
       "   54,\n",
       "   55,\n",
       "   31,\n",
       "   37,\n",
       "   16,\n",
       "   23,\n",
       "   5,\n",
       "   15,\n",
       "   34,\n",
       "   44,\n",
       "   13,\n",
       "   19,\n",
       "   17,\n",
       "   24,\n",
       "   32,\n",
       "   38,\n",
       "   30,\n",
       "   33,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [16,\n",
       "   26,\n",
       "   21,\n",
       "   32,\n",
       "   5,\n",
       "   29,\n",
       "   27,\n",
       "   31,\n",
       "   13,\n",
       "   37,\n",
       "   9,\n",
       "   39,\n",
       "   6,\n",
       "   34,\n",
       "   19,\n",
       "   25,\n",
       "   30,\n",
       "   35,\n",
       "   18,\n",
       "   24,\n",
       "   1,\n",
       "   17,\n",
       "   11,\n",
       "   36,\n",
       "   10,\n",
       "   38,\n",
       "   0,\n",
       "   7,\n",
       "   15,\n",
       "   33,\n",
       "   22,\n",
       "   23,\n",
       "   3,\n",
       "   12,\n",
       "   4,\n",
       "   28,\n",
       "   2,\n",
       "   14,\n",
       "   8,\n",
       "   20,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [8,\n",
       "   28,\n",
       "   35,\n",
       "   36,\n",
       "   16,\n",
       "   21,\n",
       "   6,\n",
       "   22,\n",
       "   11,\n",
       "   12,\n",
       "   9,\n",
       "   25,\n",
       "   18,\n",
       "   33,\n",
       "   14,\n",
       "   17,\n",
       "   19,\n",
       "   20,\n",
       "   15,\n",
       "   37,\n",
       "   0,\n",
       "   3,\n",
       "   5,\n",
       "   32,\n",
       "   24,\n",
       "   31,\n",
       "   1,\n",
       "   13,\n",
       "   27,\n",
       "   30,\n",
       "   10,\n",
       "   34,\n",
       "   4,\n",
       "   26,\n",
       "   2,\n",
       "   7,\n",
       "   23,\n",
       "   29,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [35,\n",
       "   44,\n",
       "   8,\n",
       "   34,\n",
       "   36,\n",
       "   49,\n",
       "   21,\n",
       "   52,\n",
       "   9,\n",
       "   22,\n",
       "   2,\n",
       "   3,\n",
       "   0,\n",
       "   19,\n",
       "   27,\n",
       "   50,\n",
       "   7,\n",
       "   11,\n",
       "   38,\n",
       "   58,\n",
       "   26,\n",
       "   57,\n",
       "   10,\n",
       "   30,\n",
       "   24,\n",
       "   55,\n",
       "   12,\n",
       "   18,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   4,\n",
       "   39,\n",
       "   41,\n",
       "   63,\n",
       "   15,\n",
       "   29,\n",
       "   16,\n",
       "   28,\n",
       "   13,\n",
       "   20,\n",
       "   5,\n",
       "   48,\n",
       "   6,\n",
       "   43,\n",
       "   53,\n",
       "   54,\n",
       "   33,\n",
       "   45,\n",
       "   23,\n",
       "   51,\n",
       "   32,\n",
       "   37,\n",
       "   1,\n",
       "   42,\n",
       "   31,\n",
       "   56,\n",
       "   25,\n",
       "   47,\n",
       "   17,\n",
       "   46,\n",
       "   14,\n",
       "   40],\n",
       "  [15,\n",
       "   20,\n",
       "   14,\n",
       "   23,\n",
       "   9,\n",
       "   22,\n",
       "   27,\n",
       "   31,\n",
       "   8,\n",
       "   18,\n",
       "   12,\n",
       "   30,\n",
       "   2,\n",
       "   16,\n",
       "   5,\n",
       "   29,\n",
       "   10,\n",
       "   26,\n",
       "   19,\n",
       "   21,\n",
       "   1,\n",
       "   3,\n",
       "   6,\n",
       "   11,\n",
       "   7,\n",
       "   13,\n",
       "   0,\n",
       "   28,\n",
       "   4,\n",
       "   25,\n",
       "   17,\n",
       "   24],\n",
       "  [44,\n",
       "   66,\n",
       "   36,\n",
       "   49,\n",
       "   2,\n",
       "   59,\n",
       "   8,\n",
       "   39,\n",
       "   28,\n",
       "   34,\n",
       "   21,\n",
       "   56,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   35,\n",
       "   45,\n",
       "   27,\n",
       "   60,\n",
       "   37,\n",
       "   55,\n",
       "   25,\n",
       "   38,\n",
       "   3,\n",
       "   65,\n",
       "   15,\n",
       "   57,\n",
       "   46,\n",
       "   61,\n",
       "   5,\n",
       "   19,\n",
       "   10,\n",
       "   40,\n",
       "   23,\n",
       "   42,\n",
       "   4,\n",
       "   32,\n",
       "   12,\n",
       "   33,\n",
       "   7,\n",
       "   41,\n",
       "   0,\n",
       "   71,\n",
       "   17,\n",
       "   63,\n",
       "   14,\n",
       "   43,\n",
       "   22,\n",
       "   58,\n",
       "   1,\n",
       "   26,\n",
       "   16,\n",
       "   47,\n",
       "   9,\n",
       "   11,\n",
       "   48,\n",
       "   53,\n",
       "   18,\n",
       "   54,\n",
       "   51,\n",
       "   62,\n",
       "   20,\n",
       "   30,\n",
       "   13,\n",
       "   50,\n",
       "   24,\n",
       "   31,\n",
       "   6,\n",
       "   64,\n",
       "   29,\n",
       "   52,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [6,\n",
       "   25,\n",
       "   22,\n",
       "   34,\n",
       "   26,\n",
       "   36,\n",
       "   21,\n",
       "   23,\n",
       "   38,\n",
       "   42,\n",
       "   33,\n",
       "   35,\n",
       "   11,\n",
       "   43,\n",
       "   12,\n",
       "   18,\n",
       "   8,\n",
       "   32,\n",
       "   17,\n",
       "   29,\n",
       "   0,\n",
       "   7,\n",
       "   1,\n",
       "   4,\n",
       "   19,\n",
       "   44,\n",
       "   20,\n",
       "   45,\n",
       "   15,\n",
       "   28,\n",
       "   16,\n",
       "   39,\n",
       "   14,\n",
       "   27,\n",
       "   3,\n",
       "   13,\n",
       "   5,\n",
       "   40,\n",
       "   46,\n",
       "   47,\n",
       "   2,\n",
       "   30,\n",
       "   31,\n",
       "   37,\n",
       "   9,\n",
       "   24,\n",
       "   10,\n",
       "   41],\n",
       "  [13,\n",
       "   24,\n",
       "   15,\n",
       "   21,\n",
       "   9,\n",
       "   12,\n",
       "   8,\n",
       "   18,\n",
       "   1,\n",
       "   7,\n",
       "   16,\n",
       "   28,\n",
       "   4,\n",
       "   6,\n",
       "   19,\n",
       "   23,\n",
       "   2,\n",
       "   17,\n",
       "   14,\n",
       "   25,\n",
       "   20,\n",
       "   26,\n",
       "   29,\n",
       "   30,\n",
       "   3,\n",
       "   10,\n",
       "   27,\n",
       "   31,\n",
       "   11,\n",
       "   22,\n",
       "   0,\n",
       "   5],\n",
       "  [13,\n",
       "   29,\n",
       "   6,\n",
       "   35,\n",
       "   9,\n",
       "   27,\n",
       "   17,\n",
       "   19,\n",
       "   24,\n",
       "   30,\n",
       "   5,\n",
       "   34,\n",
       "   0,\n",
       "   11,\n",
       "   20,\n",
       "   33,\n",
       "   4,\n",
       "   25,\n",
       "   7,\n",
       "   18,\n",
       "   14,\n",
       "   28,\n",
       "   1,\n",
       "   21,\n",
       "   8,\n",
       "   12,\n",
       "   3,\n",
       "   16,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   23,\n",
       "   26,\n",
       "   2,\n",
       "   32,\n",
       "   15,\n",
       "   31,\n",
       "   10,\n",
       "   22,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [12,\n",
       "   22,\n",
       "   0,\n",
       "   24,\n",
       "   16,\n",
       "   35,\n",
       "   26,\n",
       "   38,\n",
       "   20,\n",
       "   42,\n",
       "   5,\n",
       "   45,\n",
       "   9,\n",
       "   44,\n",
       "   36,\n",
       "   37,\n",
       "   8,\n",
       "   33,\n",
       "   7,\n",
       "   17,\n",
       "   27,\n",
       "   32,\n",
       "   15,\n",
       "   30,\n",
       "   11,\n",
       "   40,\n",
       "   6,\n",
       "   21,\n",
       "   23,\n",
       "   28,\n",
       "   3,\n",
       "   4,\n",
       "   31,\n",
       "   34,\n",
       "   25,\n",
       "   39,\n",
       "   18,\n",
       "   41,\n",
       "   19,\n",
       "   29,\n",
       "   1,\n",
       "   14,\n",
       "   13,\n",
       "   43,\n",
       "   2,\n",
       "   10,\n",
       "   46,\n",
       "   47],\n",
       "  [3,\n",
       "   34,\n",
       "   38,\n",
       "   43,\n",
       "   14,\n",
       "   48,\n",
       "   39,\n",
       "   53,\n",
       "   30,\n",
       "   36,\n",
       "   29,\n",
       "   45,\n",
       "   57,\n",
       "   68,\n",
       "   23,\n",
       "   49,\n",
       "   4,\n",
       "   7,\n",
       "   5,\n",
       "   27,\n",
       "   31,\n",
       "   56,\n",
       "   58,\n",
       "   63,\n",
       "   16,\n",
       "   72,\n",
       "   9,\n",
       "   71,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   33,\n",
       "   54,\n",
       "   21,\n",
       "   69,\n",
       "   50,\n",
       "   61,\n",
       "   28,\n",
       "   65,\n",
       "   12,\n",
       "   25,\n",
       "   42,\n",
       "   67,\n",
       "   15,\n",
       "   55,\n",
       "   47,\n",
       "   64,\n",
       "   13,\n",
       "   73,\n",
       "   20,\n",
       "   70,\n",
       "   8,\n",
       "   60,\n",
       "   11,\n",
       "   18,\n",
       "   32,\n",
       "   35,\n",
       "   26,\n",
       "   59,\n",
       "   17,\n",
       "   40,\n",
       "   1,\n",
       "   37,\n",
       "   19,\n",
       "   75,\n",
       "   0,\n",
       "   62,\n",
       "   24,\n",
       "   52,\n",
       "   10,\n",
       "   51,\n",
       "   2,\n",
       "   74,\n",
       "   22,\n",
       "   44,\n",
       "   41,\n",
       "   46,\n",
       "   6,\n",
       "   66],\n",
       "  [11,\n",
       "   24,\n",
       "   6,\n",
       "   21,\n",
       "   2,\n",
       "   16,\n",
       "   22,\n",
       "   28,\n",
       "   3,\n",
       "   8,\n",
       "   7,\n",
       "   12,\n",
       "   17,\n",
       "   19,\n",
       "   1,\n",
       "   23,\n",
       "   4,\n",
       "   20,\n",
       "   5,\n",
       "   15,\n",
       "   10,\n",
       "   27,\n",
       "   0,\n",
       "   14,\n",
       "   13,\n",
       "   25,\n",
       "   26,\n",
       "   29,\n",
       "   9,\n",
       "   18,\n",
       "   30,\n",
       "   31],\n",
       "  [21,\n",
       "   49,\n",
       "   24,\n",
       "   30,\n",
       "   14,\n",
       "   17,\n",
       "   28,\n",
       "   36,\n",
       "   42,\n",
       "   43,\n",
       "   15,\n",
       "   44,\n",
       "   3,\n",
       "   12,\n",
       "   22,\n",
       "   27,\n",
       "   29,\n",
       "   35,\n",
       "   2,\n",
       "   5,\n",
       "   23,\n",
       "   38,\n",
       "   4,\n",
       "   41,\n",
       "   26,\n",
       "   32,\n",
       "   9,\n",
       "   46,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   11,\n",
       "   33,\n",
       "   37,\n",
       "   47,\n",
       "   25,\n",
       "   34,\n",
       "   1,\n",
       "   6,\n",
       "   7,\n",
       "   39,\n",
       "   13,\n",
       "   31,\n",
       "   16,\n",
       "   40,\n",
       "   10,\n",
       "   20,\n",
       "   18,\n",
       "   51,\n",
       "   0,\n",
       "   45,\n",
       "   8,\n",
       "   19,\n",
       "   48,\n",
       "   50,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [29,\n",
       "   30,\n",
       "   9,\n",
       "   16,\n",
       "   2,\n",
       "   12,\n",
       "   10,\n",
       "   19,\n",
       "   6,\n",
       "   18,\n",
       "   14,\n",
       "   24,\n",
       "   17,\n",
       "   25,\n",
       "   8,\n",
       "   28,\n",
       "   11,\n",
       "   13,\n",
       "   15,\n",
       "   21,\n",
       "   0,\n",
       "   3,\n",
       "   23,\n",
       "   26,\n",
       "   5,\n",
       "   22,\n",
       "   7,\n",
       "   31,\n",
       "   4,\n",
       "   27,\n",
       "   1,\n",
       "   20],\n",
       "  [12,\n",
       "   15,\n",
       "   4,\n",
       "   5,\n",
       "   2,\n",
       "   8,\n",
       "   3,\n",
       "   19,\n",
       "   11,\n",
       "   17,\n",
       "   7,\n",
       "   10,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   1,\n",
       "   18,\n",
       "   14,\n",
       "   16,\n",
       "   9,\n",
       "   13,\n",
       "   0,\n",
       "   6,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [1,\n",
       "   20,\n",
       "   45,\n",
       "   47,\n",
       "   22,\n",
       "   43,\n",
       "   16,\n",
       "   33,\n",
       "   17,\n",
       "   24,\n",
       "   4,\n",
       "   14,\n",
       "   0,\n",
       "   8,\n",
       "   25,\n",
       "   52,\n",
       "   21,\n",
       "   34,\n",
       "   29,\n",
       "   30,\n",
       "   7,\n",
       "   13,\n",
       "   11,\n",
       "   46,\n",
       "   3,\n",
       "   12,\n",
       "   39,\n",
       "   50,\n",
       "   15,\n",
       "   49,\n",
       "   35,\n",
       "   38,\n",
       "   19,\n",
       "   44,\n",
       "   31,\n",
       "   41,\n",
       "   2,\n",
       "   18,\n",
       "   6,\n",
       "   10,\n",
       "   23,\n",
       "   48,\n",
       "   9,\n",
       "   53,\n",
       "   5,\n",
       "   26,\n",
       "   36,\n",
       "   42,\n",
       "   37,\n",
       "   51,\n",
       "   28,\n",
       "   40,\n",
       "   27,\n",
       "   54,\n",
       "   32,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [12,\n",
       "   16,\n",
       "   5,\n",
       "   23,\n",
       "   21,\n",
       "   26,\n",
       "   2,\n",
       "   7,\n",
       "   3,\n",
       "   44,\n",
       "   0,\n",
       "   1,\n",
       "   8,\n",
       "   13,\n",
       "   10,\n",
       "   25,\n",
       "   15,\n",
       "   39,\n",
       "   17,\n",
       "   27,\n",
       "   22,\n",
       "   30,\n",
       "   6,\n",
       "   38,\n",
       "   4,\n",
       "   33,\n",
       "   37,\n",
       "   45,\n",
       "   11,\n",
       "   42,\n",
       "   24,\n",
       "   32,\n",
       "   19,\n",
       "   43,\n",
       "   9,\n",
       "   28,\n",
       "   14,\n",
       "   34,\n",
       "   46,\n",
       "   47,\n",
       "   31,\n",
       "   36,\n",
       "   40,\n",
       "   41,\n",
       "   18,\n",
       "   35,\n",
       "   20,\n",
       "   29],\n",
       "  [2,\n",
       "   5,\n",
       "   12,\n",
       "   33,\n",
       "   40,\n",
       "   49,\n",
       "   29,\n",
       "   37,\n",
       "   21,\n",
       "   69,\n",
       "   52,\n",
       "   61,\n",
       "   45,\n",
       "   66,\n",
       "   6,\n",
       "   55,\n",
       "   62,\n",
       "   67,\n",
       "   8,\n",
       "   39,\n",
       "   38,\n",
       "   74,\n",
       "   0,\n",
       "   17,\n",
       "   7,\n",
       "   14,\n",
       "   11,\n",
       "   24,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   32,\n",
       "   56,\n",
       "   25,\n",
       "   73,\n",
       "   20,\n",
       "   64,\n",
       "   46,\n",
       "   63,\n",
       "   4,\n",
       "   54,\n",
       "   13,\n",
       "   58,\n",
       "   27,\n",
       "   34,\n",
       "   42,\n",
       "   47,\n",
       "   57,\n",
       "   71,\n",
       "   51,\n",
       "   79,\n",
       "   36,\n",
       "   70,\n",
       "   26,\n",
       "   59,\n",
       "   28,\n",
       "   31,\n",
       "   9,\n",
       "   65,\n",
       "   15,\n",
       "   44,\n",
       "   22,\n",
       "   53,\n",
       "   16,\n",
       "   23,\n",
       "   1,\n",
       "   50,\n",
       "   3,\n",
       "   68,\n",
       "   30,\n",
       "   48,\n",
       "   60,\n",
       "   72,\n",
       "   10,\n",
       "   35,\n",
       "   18,\n",
       "   43,\n",
       "   19,\n",
       "   41],\n",
       "  [59,\n",
       "   70,\n",
       "   14,\n",
       "   77,\n",
       "   7,\n",
       "   62,\n",
       "   16,\n",
       "   72,\n",
       "   31,\n",
       "   41,\n",
       "   36,\n",
       "   50,\n",
       "   13,\n",
       "   58,\n",
       "   37,\n",
       "   65,\n",
       "   29,\n",
       "   55,\n",
       "   9,\n",
       "   75,\n",
       "   21,\n",
       "   43,\n",
       "   12,\n",
       "   73,\n",
       "   24,\n",
       "   68,\n",
       "   47,\n",
       "   64,\n",
       "   28,\n",
       "   60,\n",
       "   78,\n",
       "   79,\n",
       "   19,\n",
       "   48,\n",
       "   26,\n",
       "   52,\n",
       "   30,\n",
       "   56,\n",
       "   2,\n",
       "   45,\n",
       "   25,\n",
       "   39,\n",
       "   11,\n",
       "   74,\n",
       "   8,\n",
       "   22,\n",
       "   10,\n",
       "   67,\n",
       "   53,\n",
       "   61,\n",
       "   5,\n",
       "   20,\n",
       "   34,\n",
       "   76,\n",
       "   54,\n",
       "   57,\n",
       "   0,\n",
       "   23,\n",
       "   18,\n",
       "   42,\n",
       "   17,\n",
       "   35,\n",
       "   15,\n",
       "   63,\n",
       "   46,\n",
       "   69,\n",
       "   27,\n",
       "   71,\n",
       "   6,\n",
       "   51,\n",
       "   1,\n",
       "   33,\n",
       "   40,\n",
       "   66,\n",
       "   4,\n",
       "   32,\n",
       "   38,\n",
       "   44,\n",
       "   3,\n",
       "   49],\n",
       "  [45,\n",
       "   57,\n",
       "   33,\n",
       "   60,\n",
       "   15,\n",
       "   46,\n",
       "   7,\n",
       "   59,\n",
       "   16,\n",
       "   37,\n",
       "   5,\n",
       "   35,\n",
       "   12,\n",
       "   14,\n",
       "   10,\n",
       "   36,\n",
       "   11,\n",
       "   25,\n",
       "   3,\n",
       "   49,\n",
       "   34,\n",
       "   42,\n",
       "   62,\n",
       "   63,\n",
       "   13,\n",
       "   48,\n",
       "   0,\n",
       "   18,\n",
       "   4,\n",
       "   23,\n",
       "   31,\n",
       "   55,\n",
       "   40,\n",
       "   47,\n",
       "   50,\n",
       "   52,\n",
       "   32,\n",
       "   39,\n",
       "   1,\n",
       "   30,\n",
       "   6,\n",
       "   56,\n",
       "   28,\n",
       "   41,\n",
       "   22,\n",
       "   29,\n",
       "   24,\n",
       "   26,\n",
       "   8,\n",
       "   38,\n",
       "   27,\n",
       "   58,\n",
       "   21,\n",
       "   53,\n",
       "   43,\n",
       "   51,\n",
       "   44,\n",
       "   54,\n",
       "   2,\n",
       "   19,\n",
       "   17,\n",
       "   61,\n",
       "   9,\n",
       "   20],\n",
       "  [2,\n",
       "   13,\n",
       "   19,\n",
       "   28,\n",
       "   15,\n",
       "   25,\n",
       "   22,\n",
       "   29,\n",
       "   6,\n",
       "   27,\n",
       "   8,\n",
       "   16,\n",
       "   17,\n",
       "   24,\n",
       "   30,\n",
       "   31,\n",
       "   3,\n",
       "   23,\n",
       "   9,\n",
       "   26,\n",
       "   5,\n",
       "   18,\n",
       "   0,\n",
       "   11,\n",
       "   4,\n",
       "   21,\n",
       "   14,\n",
       "   20,\n",
       "   7,\n",
       "   10,\n",
       "   1,\n",
       "   12],\n",
       "  [7,\n",
       "   15,\n",
       "   10,\n",
       "   17,\n",
       "   5,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   0,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   3,\n",
       "   13,\n",
       "   11,\n",
       "   14,\n",
       "   1,\n",
       "   12,\n",
       "   8,\n",
       "   9,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [19,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   9,\n",
       "   15,\n",
       "   2,\n",
       "   12,\n",
       "   13,\n",
       "   18,\n",
       "   3,\n",
       "   22,\n",
       "   4,\n",
       "   7,\n",
       "   10,\n",
       "   20,\n",
       "   0,\n",
       "   17,\n",
       "   1,\n",
       "   6,\n",
       "   5,\n",
       "   16,\n",
       "   26,\n",
       "   27,\n",
       "   11,\n",
       "   14,\n",
       "   8,\n",
       "   21,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [14,\n",
       "   24,\n",
       "   11,\n",
       "   30,\n",
       "   3,\n",
       "   17,\n",
       "   4,\n",
       "   32,\n",
       "   6,\n",
       "   16,\n",
       "   9,\n",
       "   10,\n",
       "   28,\n",
       "   29,\n",
       "   19,\n",
       "   26,\n",
       "   12,\n",
       "   22,\n",
       "   21,\n",
       "   35,\n",
       "   1,\n",
       "   20,\n",
       "   8,\n",
       "   23,\n",
       "   7,\n",
       "   27,\n",
       "   15,\n",
       "   18,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   2,\n",
       "   33,\n",
       "   13,\n",
       "   25,\n",
       "   31,\n",
       "   34,\n",
       "   0,\n",
       "   5,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [4,\n",
       "   8,\n",
       "   13,\n",
       "   20,\n",
       "   9,\n",
       "   34,\n",
       "   0,\n",
       "   27,\n",
       "   16,\n",
       "   24,\n",
       "   26,\n",
       "   31,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   7,\n",
       "   32,\n",
       "   14,\n",
       "   39,\n",
       "   5,\n",
       "   12,\n",
       "   3,\n",
       "   30,\n",
       "   1,\n",
       "   21,\n",
       "   6,\n",
       "   11,\n",
       "   17,\n",
       "   19,\n",
       "   2,\n",
       "   23,\n",
       "   18,\n",
       "   25,\n",
       "   22,\n",
       "   33,\n",
       "   10,\n",
       "   28,\n",
       "   15,\n",
       "   29,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [22,\n",
       "   25,\n",
       "   15,\n",
       "   27,\n",
       "   13,\n",
       "   32,\n",
       "   16,\n",
       "   39,\n",
       "   28,\n",
       "   40,\n",
       "   37,\n",
       "   38,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   0,\n",
       "   4,\n",
       "   3,\n",
       "   45,\n",
       "   12,\n",
       "   36,\n",
       "   1,\n",
       "   30,\n",
       "   21,\n",
       "   24,\n",
       "   6,\n",
       "   18,\n",
       "   8,\n",
       "   19,\n",
       "   46,\n",
       "   47,\n",
       "   7,\n",
       "   29,\n",
       "   14,\n",
       "   23,\n",
       "   5,\n",
       "   17,\n",
       "   26,\n",
       "   33,\n",
       "   20,\n",
       "   34,\n",
       "   10,\n",
       "   31,\n",
       "   2,\n",
       "   11,\n",
       "   9,\n",
       "   35],\n",
       "  [9,\n",
       "   10,\n",
       "   12,\n",
       "   16,\n",
       "   6,\n",
       "   11,\n",
       "   20,\n",
       "   21,\n",
       "   0,\n",
       "   5,\n",
       "   19,\n",
       "   22,\n",
       "   1,\n",
       "   7,\n",
       "   15,\n",
       "   23,\n",
       "   8,\n",
       "   13,\n",
       "   17,\n",
       "   18,\n",
       "   3,\n",
       "   4,\n",
       "   2,\n",
       "   14,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [31,\n",
       "   32,\n",
       "   1,\n",
       "   51,\n",
       "   4,\n",
       "   10,\n",
       "   7,\n",
       "   52,\n",
       "   5,\n",
       "   28,\n",
       "   13,\n",
       "   46,\n",
       "   14,\n",
       "   39,\n",
       "   37,\n",
       "   49,\n",
       "   8,\n",
       "   19,\n",
       "   26,\n",
       "   40,\n",
       "   2,\n",
       "   44,\n",
       "   24,\n",
       "   36,\n",
       "   29,\n",
       "   34,\n",
       "   20,\n",
       "   42,\n",
       "   27,\n",
       "   38,\n",
       "   53,\n",
       "   54,\n",
       "   23,\n",
       "   45,\n",
       "   15,\n",
       "   55,\n",
       "   22,\n",
       "   25,\n",
       "   12,\n",
       "   30,\n",
       "   0,\n",
       "   41,\n",
       "   3,\n",
       "   9,\n",
       "   6,\n",
       "   48,\n",
       "   11,\n",
       "   35,\n",
       "   33,\n",
       "   50,\n",
       "   16,\n",
       "   17,\n",
       "   21,\n",
       "   43,\n",
       "   18,\n",
       "   47,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [12,\n",
       "   16,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   31,\n",
       "   26,\n",
       "   27,\n",
       "   13,\n",
       "   25,\n",
       "   17,\n",
       "   30,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   21,\n",
       "   29,\n",
       "   11,\n",
       "   37,\n",
       "   15,\n",
       "   32,\n",
       "   3,\n",
       "   24,\n",
       "   2,\n",
       "   14,\n",
       "   6,\n",
       "   38,\n",
       "   10,\n",
       "   23,\n",
       "   18,\n",
       "   20,\n",
       "   5,\n",
       "   22,\n",
       "   28,\n",
       "   39,\n",
       "   1,\n",
       "   19,\n",
       "   0,\n",
       "   4,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [14,\n",
       "   19,\n",
       "   16,\n",
       "   25,\n",
       "   3,\n",
       "   30,\n",
       "   5,\n",
       "   31,\n",
       "   10,\n",
       "   26,\n",
       "   9,\n",
       "   29,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   13,\n",
       "   24,\n",
       "   18,\n",
       "   36,\n",
       "   8,\n",
       "   20,\n",
       "   1,\n",
       "   27,\n",
       "   21,\n",
       "   28,\n",
       "   7,\n",
       "   12,\n",
       "   0,\n",
       "   6,\n",
       "   37,\n",
       "   38,\n",
       "   17,\n",
       "   23,\n",
       "   11,\n",
       "   39,\n",
       "   4,\n",
       "   15,\n",
       "   2,\n",
       "   22,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [0,\n",
       "   24,\n",
       "   1,\n",
       "   16,\n",
       "   13,\n",
       "   18,\n",
       "   10,\n",
       "   19,\n",
       "   8,\n",
       "   21,\n",
       "   17,\n",
       "   23,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   15,\n",
       "   20,\n",
       "   4,\n",
       "   7,\n",
       "   11,\n",
       "   22,\n",
       "   30,\n",
       "   31,\n",
       "   2,\n",
       "   25,\n",
       "   9,\n",
       "   12,\n",
       "   6,\n",
       "   14,\n",
       "   3,\n",
       "   5],\n",
       "  [52,\n",
       "   81,\n",
       "   92,\n",
       "   126,\n",
       "   14,\n",
       "   78,\n",
       "   10,\n",
       "   39,\n",
       "   18,\n",
       "   82,\n",
       "   94,\n",
       "   112,\n",
       "   49,\n",
       "   115,\n",
       "   34,\n",
       "   70,\n",
       "   11,\n",
       "   73,\n",
       "   26,\n",
       "   27,\n",
       "   75,\n",
       "   84,\n",
       "   86,\n",
       "   100,\n",
       "   38,\n",
       "   98,\n",
       "   8,\n",
       "   104,\n",
       "   47,\n",
       "   62,\n",
       "   74,\n",
       "   76,\n",
       "   87,\n",
       "   123,\n",
       "   3,\n",
       "   83,\n",
       "   20,\n",
       "   99,\n",
       "   33,\n",
       "   68,\n",
       "   28,\n",
       "   96,\n",
       "   31,\n",
       "   111,\n",
       "   46,\n",
       "   66,\n",
       "   16,\n",
       "   72,\n",
       "   57,\n",
       "   85,\n",
       "   13,\n",
       "   36,\n",
       "   40,\n",
       "   118,\n",
       "   4,\n",
       "   9,\n",
       "   56,\n",
       "   93,\n",
       "   2,\n",
       "   97,\n",
       "   43,\n",
       "   51,\n",
       "   32,\n",
       "   105,\n",
       "   22,\n",
       "   63,\n",
       "   102,\n",
       "   114,\n",
       "   37,\n",
       "   55,\n",
       "   6,\n",
       "   95,\n",
       "   21,\n",
       "   71,\n",
       "   29,\n",
       "   108,\n",
       "   12,\n",
       "   24,\n",
       "   67,\n",
       "   125,\n",
       "   15,\n",
       "   53,\n",
       "   107,\n",
       "   127,\n",
       "   7,\n",
       "   35,\n",
       "   60,\n",
       "   80,\n",
       "   1,\n",
       "   121,\n",
       "   0,\n",
       "   110,\n",
       "   30,\n",
       "   116,\n",
       "   45,\n",
       "   117,\n",
       "   64,\n",
       "   89,\n",
       "   25,\n",
       "   42,\n",
       "   79,\n",
       "   109,\n",
       "   19,\n",
       "   50,\n",
       "   119,\n",
       "   124,\n",
       "   48,\n",
       "   65,\n",
       "   90,\n",
       "   113,\n",
       "   54,\n",
       "   69,\n",
       "   23,\n",
       "   122,\n",
       "   77,\n",
       "   120,\n",
       "   88,\n",
       "   103,\n",
       "   58,\n",
       "   106,\n",
       "   41,\n",
       "   59,\n",
       "   61,\n",
       "   101,\n",
       "   5,\n",
       "   17,\n",
       "   44,\n",
       "   91],\n",
       "  [24,\n",
       "   29,\n",
       "   16,\n",
       "   47,\n",
       "   25,\n",
       "   57,\n",
       "   40,\n",
       "   49,\n",
       "   23,\n",
       "   72,\n",
       "   45,\n",
       "   61,\n",
       "   34,\n",
       "   71,\n",
       "   66,\n",
       "   70,\n",
       "   30,\n",
       "   67,\n",
       "   6,\n",
       "   13,\n",
       "   9,\n",
       "   51,\n",
       "   27,\n",
       "   59,\n",
       "   31,\n",
       "   58,\n",
       "   17,\n",
       "   41,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   19,\n",
       "   26,\n",
       "   21,\n",
       "   55,\n",
       "   5,\n",
       "   64,\n",
       "   42,\n",
       "   46,\n",
       "   4,\n",
       "   12,\n",
       "   38,\n",
       "   62,\n",
       "   11,\n",
       "   43,\n",
       "   77,\n",
       "   78,\n",
       "   0,\n",
       "   14,\n",
       "   3,\n",
       "   32,\n",
       "   36,\n",
       "   50,\n",
       "   2,\n",
       "   28,\n",
       "   37,\n",
       "   69,\n",
       "   44,\n",
       "   52,\n",
       "   15,\n",
       "   18,\n",
       "   7,\n",
       "   33,\n",
       "   22,\n",
       "   65,\n",
       "   54,\n",
       "   63,\n",
       "   39,\n",
       "   53,\n",
       "   35,\n",
       "   79,\n",
       "   20,\n",
       "   48,\n",
       "   10,\n",
       "   56,\n",
       "   60,\n",
       "   68,\n",
       "   1,\n",
       "   8],\n",
       "  [34,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   8,\n",
       "   56,\n",
       "   25,\n",
       "   60,\n",
       "   55,\n",
       "   97,\n",
       "   24,\n",
       "   75,\n",
       "   61,\n",
       "   65,\n",
       "   31,\n",
       "   43,\n",
       "   6,\n",
       "   35,\n",
       "   58,\n",
       "   73,\n",
       "   11,\n",
       "   45,\n",
       "   2,\n",
       "   76,\n",
       "   39,\n",
       "   67,\n",
       "   86,\n",
       "   109,\n",
       "   47,\n",
       "   83,\n",
       "   57,\n",
       "   64,\n",
       "   12,\n",
       "   37,\n",
       "   33,\n",
       "   110,\n",
       "   79,\n",
       "   88,\n",
       "   40,\n",
       "   91,\n",
       "   9,\n",
       "   15,\n",
       "   1,\n",
       "   71,\n",
       "   21,\n",
       "   49,\n",
       "   14,\n",
       "   89,\n",
       "   3,\n",
       "   27,\n",
       "   59,\n",
       "   105,\n",
       "   13,\n",
       "   93,\n",
       "   90,\n",
       "   111,\n",
       "   42,\n",
       "   62,\n",
       "   16,\n",
       "   74,\n",
       "   66,\n",
       "   81,\n",
       "   85,\n",
       "   101,\n",
       "   4,\n",
       "   20,\n",
       "   48,\n",
       "   63,\n",
       "   7,\n",
       "   80,\n",
       "   68,\n",
       "   102,\n",
       "   52,\n",
       "   99,\n",
       "   94,\n",
       "   104,\n",
       "   41,\n",
       "   95,\n",
       "   82,\n",
       "   103,\n",
       "   22,\n",
       "   70,\n",
       "   51,\n",
       "   87,\n",
       "   38,\n",
       "   69,\n",
       "   19,\n",
       "   29,\n",
       "   0,\n",
       "   32,\n",
       "   44,\n",
       "   100,\n",
       "   18,\n",
       "   46,\n",
       "   5,\n",
       "   50,\n",
       "   26,\n",
       "   30,\n",
       "   36,\n",
       "   77,\n",
       "   53,\n",
       "   96,\n",
       "   78,\n",
       "   84,\n",
       "   10,\n",
       "   92,\n",
       "   17,\n",
       "   23,\n",
       "   28,\n",
       "   72,\n",
       "   54,\n",
       "   98],\n",
       "  [56,\n",
       "   61,\n",
       "   14,\n",
       "   74,\n",
       "   8,\n",
       "   24,\n",
       "   21,\n",
       "   57,\n",
       "   27,\n",
       "   38,\n",
       "   17,\n",
       "   67,\n",
       "   9,\n",
       "   70,\n",
       "   63,\n",
       "   66,\n",
       "   33,\n",
       "   46,\n",
       "   23,\n",
       "   48,\n",
       "   25,\n",
       "   62,\n",
       "   13,\n",
       "   41,\n",
       "   22,\n",
       "   51,\n",
       "   58,\n",
       "   68,\n",
       "   60,\n",
       "   73,\n",
       "   11,\n",
       "   65,\n",
       "   37,\n",
       "   71,\n",
       "   30,\n",
       "   43,\n",
       "   36,\n",
       "   49,\n",
       "   7,\n",
       "   12,\n",
       "   16,\n",
       "   52,\n",
       "   2,\n",
       "   34,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   4,\n",
       "   32,\n",
       "   6,\n",
       "   79,\n",
       "   3,\n",
       "   28,\n",
       "   15,\n",
       "   44,\n",
       "   39,\n",
       "   54,\n",
       "   26,\n",
       "   64,\n",
       "   10,\n",
       "   35,\n",
       "   1,\n",
       "   5,\n",
       "   18,\n",
       "   20,\n",
       "   31,\n",
       "   59,\n",
       "   45,\n",
       "   47,\n",
       "   0,\n",
       "   72,\n",
       "   19,\n",
       "   29,\n",
       "   50,\n",
       "   53,\n",
       "   42,\n",
       "   55,\n",
       "   40,\n",
       "   69],\n",
       "  [1,\n",
       "   28,\n",
       "   20,\n",
       "   39,\n",
       "   21,\n",
       "   35,\n",
       "   14,\n",
       "   24,\n",
       "   10,\n",
       "   36,\n",
       "   8,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   17,\n",
       "   18,\n",
       "   16,\n",
       "   45,\n",
       "   5,\n",
       "   30,\n",
       "   15,\n",
       "   32,\n",
       "   12,\n",
       "   31,\n",
       "   37,\n",
       "   38,\n",
       "   11,\n",
       "   22,\n",
       "   34,\n",
       "   46,\n",
       "   0,\n",
       "   27,\n",
       "   33,\n",
       "   47,\n",
       "   4,\n",
       "   7,\n",
       "   6,\n",
       "   29,\n",
       "   9,\n",
       "   19,\n",
       "   3,\n",
       "   23,\n",
       "   2,\n",
       "   25,\n",
       "   13,\n",
       "   26],\n",
       "  [8,\n",
       "   60,\n",
       "   33,\n",
       "   49,\n",
       "   26,\n",
       "   68,\n",
       "   35,\n",
       "   65,\n",
       "   7,\n",
       "   54,\n",
       "   58,\n",
       "   64,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   23,\n",
       "   30,\n",
       "   16,\n",
       "   47,\n",
       "   24,\n",
       "   59,\n",
       "   43,\n",
       "   50,\n",
       "   15,\n",
       "   29,\n",
       "   6,\n",
       "   34,\n",
       "   3,\n",
       "   53,\n",
       "   75,\n",
       "   76,\n",
       "   14,\n",
       "   17,\n",
       "   10,\n",
       "   70,\n",
       "   12,\n",
       "   46,\n",
       "   77,\n",
       "   78,\n",
       "   37,\n",
       "   69,\n",
       "   48,\n",
       "   56,\n",
       "   27,\n",
       "   31,\n",
       "   4,\n",
       "   61,\n",
       "   18,\n",
       "   67,\n",
       "   40,\n",
       "   79,\n",
       "   41,\n",
       "   44,\n",
       "   32,\n",
       "   38,\n",
       "   11,\n",
       "   36,\n",
       "   0,\n",
       "   5,\n",
       "   39,\n",
       "   51,\n",
       "   20,\n",
       "   45,\n",
       "   19,\n",
       "   57,\n",
       "   2,\n",
       "   55,\n",
       "   21,\n",
       "   66,\n",
       "   13,\n",
       "   28,\n",
       "   1,\n",
       "   25,\n",
       "   9,\n",
       "   22,\n",
       "   62,\n",
       "   63,\n",
       "   42,\n",
       "   52],\n",
       "  [73,\n",
       "   119,\n",
       "   14,\n",
       "   56,\n",
       "   20,\n",
       "   96,\n",
       "   28,\n",
       "   104,\n",
       "   114,\n",
       "   125,\n",
       "   91,\n",
       "   167,\n",
       "   103,\n",
       "   161,\n",
       "   35,\n",
       "   129,\n",
       "   3,\n",
       "   22,\n",
       "   143,\n",
       "   166,\n",
       "   2,\n",
       "   94,\n",
       "   41,\n",
       "   99,\n",
       "   39,\n",
       "   92,\n",
       "   16,\n",
       "   40,\n",
       "   121,\n",
       "   152,\n",
       "   88,\n",
       "   131,\n",
       "   81,\n",
       "   105,\n",
       "   112,\n",
       "   162,\n",
       "   15,\n",
       "   128,\n",
       "   8,\n",
       "   31,\n",
       "   1,\n",
       "   126,\n",
       "   17,\n",
       "   139,\n",
       "   23,\n",
       "   83,\n",
       "   7,\n",
       "   89,\n",
       "   49,\n",
       "   141,\n",
       "   55,\n",
       "   122,\n",
       "   68,\n",
       "   148,\n",
       "   145,\n",
       "   156,\n",
       "   101,\n",
       "   109,\n",
       "   87,\n",
       "   163,\n",
       "   29,\n",
       "   98,\n",
       "   71,\n",
       "   146,\n",
       "   32,\n",
       "   108,\n",
       "   93,\n",
       "   151,\n",
       "   36,\n",
       "   142,\n",
       "   19,\n",
       "   43,\n",
       "   64,\n",
       "   70,\n",
       "   66,\n",
       "   149,\n",
       "   48,\n",
       "   107,\n",
       "   95,\n",
       "   111,\n",
       "   54,\n",
       "   65,\n",
       "   21,\n",
       "   58,\n",
       "   60,\n",
       "   136,\n",
       "   46,\n",
       "   72,\n",
       "   117,\n",
       "   144,\n",
       "   130,\n",
       "   155,\n",
       "   37,\n",
       "   90,\n",
       "   38,\n",
       "   164,\n",
       "   51,\n",
       "   120,\n",
       "   13,\n",
       "   159,\n",
       "   53,\n",
       "   127,\n",
       "   150,\n",
       "   165,\n",
       "   59,\n",
       "   113,\n",
       "   110,\n",
       "   135,\n",
       "   0,\n",
       "   69,\n",
       "   24,\n",
       "   47,\n",
       "   134,\n",
       "   137,\n",
       "   78,\n",
       "   97,\n",
       "   26,\n",
       "   34,\n",
       "   116,\n",
       "   157,\n",
       "   6,\n",
       "   154,\n",
       "   147,\n",
       "   158,\n",
       "   25,\n",
       "   62,\n",
       "   27,\n",
       "   102,\n",
       "   76,\n",
       "   132,\n",
       "   11,\n",
       "   123,\n",
       "   4,\n",
       "   138,\n",
       "   44,\n",
       "   52,\n",
       "   61,\n",
       "   74,\n",
       "   80,\n",
       "   153,\n",
       "   12,\n",
       "   50,\n",
       "   10,\n",
       "   118,\n",
       "   33,\n",
       "   140,\n",
       "   84,\n",
       "   133,\n",
       "   100,\n",
       "   124,\n",
       "   45,\n",
       "   67,\n",
       "   42,\n",
       "   75,\n",
       "   79,\n",
       "   82,\n",
       "   5,\n",
       "   115,\n",
       "   18,\n",
       "   160,\n",
       "   30,\n",
       "   86,\n",
       "   9,\n",
       "   77,\n",
       "   57,\n",
       "   63,\n",
       "   85,\n",
       "   106,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175],\n",
       "  [5,\n",
       "   12,\n",
       "   4,\n",
       "   15,\n",
       "   2,\n",
       "   10,\n",
       "   20,\n",
       "   21,\n",
       "   3,\n",
       "   17,\n",
       "   1,\n",
       "   22,\n",
       "   9,\n",
       "   18,\n",
       "   7,\n",
       "   11,\n",
       "   14,\n",
       "   19,\n",
       "   6,\n",
       "   23,\n",
       "   0,\n",
       "   8,\n",
       "   13,\n",
       "   16,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [1,\n",
       "   5,\n",
       "   22,\n",
       "   23,\n",
       "   11,\n",
       "   24,\n",
       "   8,\n",
       "   16,\n",
       "   17,\n",
       "   25,\n",
       "   0,\n",
       "   28,\n",
       "   9,\n",
       "   12,\n",
       "   3,\n",
       "   14,\n",
       "   4,\n",
       "   26,\n",
       "   2,\n",
       "   18,\n",
       "   10,\n",
       "   27,\n",
       "   29,\n",
       "   30,\n",
       "   15,\n",
       "   19,\n",
       "   21,\n",
       "   31,\n",
       "   7,\n",
       "   13,\n",
       "   6,\n",
       "   20],\n",
       "  [6,\n",
       "   20,\n",
       "   25,\n",
       "   29,\n",
       "   3,\n",
       "   4,\n",
       "   14,\n",
       "   23,\n",
       "   5,\n",
       "   22,\n",
       "   11,\n",
       "   13,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   15,\n",
       "   16,\n",
       "   10,\n",
       "   26,\n",
       "   1,\n",
       "   18,\n",
       "   30,\n",
       "   33,\n",
       "   24,\n",
       "   31,\n",
       "   9,\n",
       "   35,\n",
       "   2,\n",
       "   12,\n",
       "   17,\n",
       "   19,\n",
       "   21,\n",
       "   28,\n",
       "   27,\n",
       "   34,\n",
       "   7,\n",
       "   8,\n",
       "   0,\n",
       "   32,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [27,\n",
       "   40,\n",
       "   3,\n",
       "   11,\n",
       "   6,\n",
       "   39,\n",
       "   30,\n",
       "   41,\n",
       "   15,\n",
       "   36,\n",
       "   21,\n",
       "   25,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   34,\n",
       "   42,\n",
       "   5,\n",
       "   12,\n",
       "   24,\n",
       "   32,\n",
       "   22,\n",
       "   37,\n",
       "   4,\n",
       "   19,\n",
       "   26,\n",
       "   43,\n",
       "   23,\n",
       "   29,\n",
       "   8,\n",
       "   9,\n",
       "   16,\n",
       "   38,\n",
       "   14,\n",
       "   31,\n",
       "   0,\n",
       "   13,\n",
       "   2,\n",
       "   33,\n",
       "   1,\n",
       "   20,\n",
       "   18,\n",
       "   35,\n",
       "   7,\n",
       "   10,\n",
       "   17,\n",
       "   28],\n",
       "  [2,\n",
       "   16,\n",
       "   14,\n",
       "   41,\n",
       "   15,\n",
       "   43,\n",
       "   7,\n",
       "   42,\n",
       "   28,\n",
       "   39,\n",
       "   20,\n",
       "   33,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   5,\n",
       "   29,\n",
       "   1,\n",
       "   4,\n",
       "   24,\n",
       "   31,\n",
       "   6,\n",
       "   9,\n",
       "   19,\n",
       "   21,\n",
       "   3,\n",
       "   22,\n",
       "   30,\n",
       "   32,\n",
       "   0,\n",
       "   40,\n",
       "   12,\n",
       "   35,\n",
       "   8,\n",
       "   23,\n",
       "   34,\n",
       "   38,\n",
       "   26,\n",
       "   36,\n",
       "   25,\n",
       "   37,\n",
       "   10,\n",
       "   27,\n",
       "   11,\n",
       "   17,\n",
       "   13,\n",
       "   18],\n",
       "  [2,\n",
       "   24,\n",
       "   0,\n",
       "   22,\n",
       "   3,\n",
       "   27,\n",
       "   25,\n",
       "   29,\n",
       "   12,\n",
       "   30,\n",
       "   17,\n",
       "   23,\n",
       "   14,\n",
       "   26,\n",
       "   16,\n",
       "   20,\n",
       "   15,\n",
       "   18,\n",
       "   10,\n",
       "   11,\n",
       "   1,\n",
       "   19,\n",
       "   13,\n",
       "   31,\n",
       "   6,\n",
       "   8,\n",
       "   4,\n",
       "   21,\n",
       "   5,\n",
       "   28,\n",
       "   7,\n",
       "   9],\n",
       "  [6,\n",
       "   33,\n",
       "   21,\n",
       "   43,\n",
       "   18,\n",
       "   39,\n",
       "   1,\n",
       "   13,\n",
       "   2,\n",
       "   40,\n",
       "   5,\n",
       "   16,\n",
       "   15,\n",
       "   17,\n",
       "   14,\n",
       "   27,\n",
       "   30,\n",
       "   34,\n",
       "   3,\n",
       "   11,\n",
       "   9,\n",
       "   28,\n",
       "   10,\n",
       "   20,\n",
       "   7,\n",
       "   29,\n",
       "   12,\n",
       "   37,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   4,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   31,\n",
       "   36,\n",
       "   38,\n",
       "   8,\n",
       "   32,\n",
       "   0,\n",
       "   19,\n",
       "   26,\n",
       "   41,\n",
       "   35,\n",
       "   42],\n",
       "  [36,\n",
       "   68,\n",
       "   46,\n",
       "   84,\n",
       "   56,\n",
       "   80,\n",
       "   77,\n",
       "   101,\n",
       "   52,\n",
       "   62,\n",
       "   37,\n",
       "   79,\n",
       "   95,\n",
       "   103,\n",
       "   44,\n",
       "   60,\n",
       "   11,\n",
       "   89,\n",
       "   107,\n",
       "   109,\n",
       "   54,\n",
       "   92,\n",
       "   8,\n",
       "   32,\n",
       "   86,\n",
       "   87,\n",
       "   43,\n",
       "   108,\n",
       "   21,\n",
       "   90,\n",
       "   13,\n",
       "   35,\n",
       "   69,\n",
       "   99,\n",
       "   61,\n",
       "   63,\n",
       "   45,\n",
       "   82,\n",
       "   72,\n",
       "   106,\n",
       "   3,\n",
       "   6,\n",
       "   55,\n",
       "   67,\n",
       "   20,\n",
       "   73,\n",
       "   31,\n",
       "   42,\n",
       "   27,\n",
       "   65,\n",
       "   12,\n",
       "   59,\n",
       "   47,\n",
       "   57,\n",
       "   0,\n",
       "   104,\n",
       "   10,\n",
       "   25,\n",
       "   71,\n",
       "   88,\n",
       "   9,\n",
       "   15,\n",
       "   17,\n",
       "   100,\n",
       "   14,\n",
       "   34,\n",
       "   83,\n",
       "   110,\n",
       "   39,\n",
       "   49,\n",
       "   28,\n",
       "   94,\n",
       "   5,\n",
       "   24,\n",
       "   41,\n",
       "   53,\n",
       "   23,\n",
       "   78,\n",
       "   18,\n",
       "   98,\n",
       "   51,\n",
       "   102,\n",
       "   93,\n",
       "   111,\n",
       "   29,\n",
       "   81,\n",
       "   48,\n",
       "   91,\n",
       "   1,\n",
       "   64,\n",
       "   22,\n",
       "   74,\n",
       "   19,\n",
       "   85,\n",
       "   4,\n",
       "   75,\n",
       "   40,\n",
       "   105,\n",
       "   30,\n",
       "   70,\n",
       "   26,\n",
       "   97,\n",
       "   7,\n",
       "   58,\n",
       "   50,\n",
       "   76,\n",
       "   33,\n",
       "   66,\n",
       "   2,\n",
       "   16,\n",
       "   38,\n",
       "   96],\n",
       "  [87,\n",
       "   100,\n",
       "   63,\n",
       "   94,\n",
       "   4,\n",
       "   85,\n",
       "   113,\n",
       "   114,\n",
       "   14,\n",
       "   84,\n",
       "   46,\n",
       "   78,\n",
       "   3,\n",
       "   41,\n",
       "   19,\n",
       "   107,\n",
       "   34,\n",
       "   106,\n",
       "   52,\n",
       "   109,\n",
       "   45,\n",
       "   59,\n",
       "   21,\n",
       "   75,\n",
       "   5,\n",
       "   48,\n",
       "   6,\n",
       "   8,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   86,\n",
       "   90,\n",
       "   10,\n",
       "   96,\n",
       "   12,\n",
       "   53,\n",
       "   20,\n",
       "   43,\n",
       "   11,\n",
       "   24,\n",
       "   91,\n",
       "   108,\n",
       "   33,\n",
       "   101,\n",
       "   47,\n",
       "   60,\n",
       "   66,\n",
       "   76,\n",
       "   68,\n",
       "   105,\n",
       "   32,\n",
       "   81,\n",
       "   31,\n",
       "   77,\n",
       "   93,\n",
       "   99,\n",
       "   22,\n",
       "   72,\n",
       "   70,\n",
       "   92,\n",
       "   38,\n",
       "   65,\n",
       "   30,\n",
       "   56,\n",
       "   17,\n",
       "   119,\n",
       "   58,\n",
       "   95,\n",
       "   64,\n",
       "   67,\n",
       "   28,\n",
       "   49,\n",
       "   44,\n",
       "   110,\n",
       "   51,\n",
       "   79,\n",
       "   89,\n",
       "   104,\n",
       "   71,\n",
       "   102,\n",
       "   7,\n",
       "   83,\n",
       "   27,\n",
       "   36,\n",
       "   29,\n",
       "   69,\n",
       "   62,\n",
       "   73,\n",
       "   37,\n",
       "   80,\n",
       "   35,\n",
       "   111,\n",
       "   1,\n",
       "   55,\n",
       "   0,\n",
       "   103,\n",
       "   23,\n",
       "   25,\n",
       "   2,\n",
       "   97,\n",
       "   18,\n",
       "   40,\n",
       "   16,\n",
       "   88,\n",
       "   39,\n",
       "   74,\n",
       "   57,\n",
       "   98,\n",
       "   15,\n",
       "   61,\n",
       "   13,\n",
       "   54,\n",
       "   9,\n",
       "   112,\n",
       "   26,\n",
       "   82,\n",
       "   42,\n",
       "   50,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127],\n",
       "  [2,\n",
       "   5,\n",
       "   28,\n",
       "   82,\n",
       "   9,\n",
       "   44,\n",
       "   107,\n",
       "   119,\n",
       "   84,\n",
       "   101,\n",
       "   16,\n",
       "   99,\n",
       "   27,\n",
       "   121,\n",
       "   21,\n",
       "   36,\n",
       "   24,\n",
       "   103,\n",
       "   73,\n",
       "   97,\n",
       "   89,\n",
       "   123,\n",
       "   58,\n",
       "   91,\n",
       "   76,\n",
       "   80,\n",
       "   59,\n",
       "   63,\n",
       "   50,\n",
       "   106,\n",
       "   62,\n",
       "   96,\n",
       "   47,\n",
       "   49,\n",
       "   14,\n",
       "   41,\n",
       "   43,\n",
       "   78,\n",
       "   1,\n",
       "   93,\n",
       "   26,\n",
       "   110,\n",
       "   32,\n",
       "   37,\n",
       "   12,\n",
       "   30,\n",
       "   71,\n",
       "   87,\n",
       "   13,\n",
       "   111,\n",
       "   117,\n",
       "   131,\n",
       "   15,\n",
       "   70,\n",
       "   60,\n",
       "   127,\n",
       "   57,\n",
       "   129,\n",
       "   113,\n",
       "   128,\n",
       "   19,\n",
       "   124,\n",
       "   35,\n",
       "   48,\n",
       "   55,\n",
       "   66,\n",
       "   10,\n",
       "   69,\n",
       "   54,\n",
       "   65,\n",
       "   34,\n",
       "   94,\n",
       "   8,\n",
       "   68,\n",
       "   25,\n",
       "   46,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   52,\n",
       "   64,\n",
       "   18,\n",
       "   125,\n",
       "   20,\n",
       "   56,\n",
       "   53,\n",
       "   115,\n",
       "   83,\n",
       "   88,\n",
       "   86,\n",
       "   116,\n",
       "   31,\n",
       "   109,\n",
       "   67,\n",
       "   100,\n",
       "   11,\n",
       "   105,\n",
       "   33,\n",
       "   39,\n",
       "   112,\n",
       "   120,\n",
       "   61,\n",
       "   81,\n",
       "   75,\n",
       "   118,\n",
       "   45,\n",
       "   126,\n",
       "   29,\n",
       "   95,\n",
       "   3,\n",
       "   6,\n",
       "   77,\n",
       "   104,\n",
       "   74,\n",
       "   114,\n",
       "   72,\n",
       "   102,\n",
       "   4,\n",
       "   42,\n",
       "   23,\n",
       "   108,\n",
       "   51,\n",
       "   92,\n",
       "   22,\n",
       "   38,\n",
       "   7,\n",
       "   90,\n",
       "   0,\n",
       "   122,\n",
       "   17,\n",
       "   130,\n",
       "   40,\n",
       "   85,\n",
       "   79,\n",
       "   98,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143],\n",
       "  [6,\n",
       "   41,\n",
       "   3,\n",
       "   45,\n",
       "   13,\n",
       "   32,\n",
       "   9,\n",
       "   39,\n",
       "   17,\n",
       "   36,\n",
       "   10,\n",
       "   20,\n",
       "   23,\n",
       "   44,\n",
       "   25,\n",
       "   30,\n",
       "   12,\n",
       "   16,\n",
       "   27,\n",
       "   31,\n",
       "   22,\n",
       "   26,\n",
       "   46,\n",
       "   47,\n",
       "   14,\n",
       "   35,\n",
       "   0,\n",
       "   2,\n",
       "   38,\n",
       "   40,\n",
       "   5,\n",
       "   18,\n",
       "   21,\n",
       "   29,\n",
       "   8,\n",
       "   37,\n",
       "   15,\n",
       "   34,\n",
       "   7,\n",
       "   42,\n",
       "   19,\n",
       "   33,\n",
       "   1,\n",
       "   24,\n",
       "   4,\n",
       "   11,\n",
       "   28,\n",
       "   43],\n",
       "  [0,\n",
       "   23,\n",
       "   28,\n",
       "   30,\n",
       "   11,\n",
       "   27,\n",
       "   13,\n",
       "   15,\n",
       "   12,\n",
       "   20,\n",
       "   14,\n",
       "   22,\n",
       "   21,\n",
       "   24,\n",
       "   29,\n",
       "   31,\n",
       "   2,\n",
       "   26,\n",
       "   1,\n",
       "   8,\n",
       "   4,\n",
       "   10,\n",
       "   9,\n",
       "   19,\n",
       "   3,\n",
       "   7,\n",
       "   5,\n",
       "   17,\n",
       "   6,\n",
       "   16,\n",
       "   18,\n",
       "   25],\n",
       "  [16,\n",
       "   17,\n",
       "   9,\n",
       "   23,\n",
       "   21,\n",
       "   45,\n",
       "   24,\n",
       "   43,\n",
       "   50,\n",
       "   63,\n",
       "   22,\n",
       "   40,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   14,\n",
       "   18,\n",
       "   37,\n",
       "   58,\n",
       "   33,\n",
       "   53,\n",
       "   19,\n",
       "   25,\n",
       "   2,\n",
       "   61,\n",
       "   47,\n",
       "   49,\n",
       "   38,\n",
       "   57,\n",
       "   32,\n",
       "   34,\n",
       "   51,\n",
       "   56,\n",
       "   6,\n",
       "   26,\n",
       "   20,\n",
       "   39,\n",
       "   70,\n",
       "   71,\n",
       "   5,\n",
       "   41,\n",
       "   1,\n",
       "   36,\n",
       "   46,\n",
       "   62,\n",
       "   7,\n",
       "   54,\n",
       "   13,\n",
       "   35,\n",
       "   0,\n",
       "   65,\n",
       "   3,\n",
       "   15,\n",
       "   42,\n",
       "   59,\n",
       "   8,\n",
       "   52,\n",
       "   30,\n",
       "   31,\n",
       "   48,\n",
       "   60,\n",
       "   11,\n",
       "   12,\n",
       "   27,\n",
       "   44,\n",
       "   29,\n",
       "   55,\n",
       "   4,\n",
       "   28,\n",
       "   10,\n",
       "   64,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [32,\n",
       "   43,\n",
       "   11,\n",
       "   33,\n",
       "   23,\n",
       "   27,\n",
       "   10,\n",
       "   37,\n",
       "   9,\n",
       "   31,\n",
       "   17,\n",
       "   19,\n",
       "   3,\n",
       "   25,\n",
       "   13,\n",
       "   36,\n",
       "   5,\n",
       "   42,\n",
       "   1,\n",
       "   40,\n",
       "   29,\n",
       "   30,\n",
       "   45,\n",
       "   46,\n",
       "   7,\n",
       "   35,\n",
       "   22,\n",
       "   44,\n",
       "   2,\n",
       "   41,\n",
       "   12,\n",
       "   15,\n",
       "   14,\n",
       "   34,\n",
       "   0,\n",
       "   4,\n",
       "   18,\n",
       "   26,\n",
       "   6,\n",
       "   28,\n",
       "   20,\n",
       "   21,\n",
       "   8,\n",
       "   24,\n",
       "   38,\n",
       "   39,\n",
       "   16,\n",
       "   47],\n",
       "  [38,\n",
       "   46,\n",
       "   25,\n",
       "   37,\n",
       "   12,\n",
       "   20,\n",
       "   58,\n",
       "   59,\n",
       "   16,\n",
       "   57,\n",
       "   2,\n",
       "   9,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   0,\n",
       "   6,\n",
       "   47,\n",
       "   53,\n",
       "   18,\n",
       "   52,\n",
       "   7,\n",
       "   54,\n",
       "   24,\n",
       "   43,\n",
       "   26,\n",
       "   40,\n",
       "   11,\n",
       "   30,\n",
       "   50,\n",
       "   51,\n",
       "   3,\n",
       "   49,\n",
       "   19,\n",
       "   29,\n",
       "   8,\n",
       "   31,\n",
       "   4,\n",
       "   41,\n",
       "   17,\n",
       "   27,\n",
       "   10,\n",
       "   44,\n",
       "   5,\n",
       "   35,\n",
       "   13,\n",
       "   33,\n",
       "   48,\n",
       "   55,\n",
       "   14,\n",
       "   22,\n",
       "   15,\n",
       "   34,\n",
       "   21,\n",
       "   28,\n",
       "   23,\n",
       "   45,\n",
       "   36,\n",
       "   42,\n",
       "   32,\n",
       "   56,\n",
       "   1,\n",
       "   39],\n",
       "  [1,\n",
       "   15,\n",
       "   8,\n",
       "   22,\n",
       "   4,\n",
       "   12,\n",
       "   7,\n",
       "   9,\n",
       "   0,\n",
       "   18,\n",
       "   2,\n",
       "   13,\n",
       "   6,\n",
       "   14,\n",
       "   19,\n",
       "   20,\n",
       "   5,\n",
       "   11,\n",
       "   10,\n",
       "   23,\n",
       "   16,\n",
       "   17,\n",
       "   3,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [4,\n",
       "   19,\n",
       "   18,\n",
       "   27,\n",
       "   8,\n",
       "   9,\n",
       "   5,\n",
       "   26,\n",
       "   20,\n",
       "   21,\n",
       "   3,\n",
       "   22,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   17,\n",
       "   24,\n",
       "   10,\n",
       "   12,\n",
       "   1,\n",
       "   7,\n",
       "   0,\n",
       "   13,\n",
       "   2,\n",
       "   23,\n",
       "   6,\n",
       "   25,\n",
       "   14,\n",
       "   16,\n",
       "   11,\n",
       "   15],\n",
       "  [28,\n",
       "   34,\n",
       "   60,\n",
       "   67,\n",
       "   7,\n",
       "   61,\n",
       "   16,\n",
       "   17,\n",
       "   13,\n",
       "   31,\n",
       "   19,\n",
       "   56,\n",
       "   36,\n",
       "   43,\n",
       "   23,\n",
       "   39,\n",
       "   5,\n",
       "   54,\n",
       "   3,\n",
       "   63,\n",
       "   0,\n",
       "   21,\n",
       "   32,\n",
       "   40,\n",
       "   18,\n",
       "   64,\n",
       "   30,\n",
       "   42,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   11,\n",
       "   49,\n",
       "   51,\n",
       "   59,\n",
       "   2,\n",
       "   46,\n",
       "   45,\n",
       "   50,\n",
       "   14,\n",
       "   53,\n",
       "   48,\n",
       "   66,\n",
       "   24,\n",
       "   37,\n",
       "   22,\n",
       "   58,\n",
       "   29,\n",
       "   65,\n",
       "   33,\n",
       "   38,\n",
       "   6,\n",
       "   12,\n",
       "   1,\n",
       "   41,\n",
       "   25,\n",
       "   44,\n",
       "   9,\n",
       "   55,\n",
       "   4,\n",
       "   26,\n",
       "   27,\n",
       "   35,\n",
       "   8,\n",
       "   15,\n",
       "   20,\n",
       "   57,\n",
       "   47,\n",
       "   62,\n",
       "   10,\n",
       "   52,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [40,\n",
       "   61,\n",
       "   34,\n",
       "   68,\n",
       "   13,\n",
       "   60,\n",
       "   43,\n",
       "   66,\n",
       "   62,\n",
       "   63,\n",
       "   18,\n",
       "   65,\n",
       "   6,\n",
       "   15,\n",
       "   29,\n",
       "   41,\n",
       "   1,\n",
       "   55,\n",
       "   23,\n",
       "   33,\n",
       "   35,\n",
       "   48,\n",
       "   11,\n",
       "   56,\n",
       "   25,\n",
       "   44,\n",
       "   3,\n",
       "   20,\n",
       "   31,\n",
       "   67,\n",
       "   14,\n",
       "   57,\n",
       "   2,\n",
       "   32,\n",
       "   4,\n",
       "   69,\n",
       "   9,\n",
       "   26,\n",
       "   27,\n",
       "   42,\n",
       "   17,\n",
       "   30,\n",
       "   19,\n",
       "   54,\n",
       "   5,\n",
       "   7,\n",
       "   22,\n",
       "   46,\n",
       "   37,\n",
       "   59,\n",
       "   8,\n",
       "   49,\n",
       "   10,\n",
       "   36,\n",
       "   70,\n",
       "   71,\n",
       "   24,\n",
       "   64,\n",
       "   12,\n",
       "   53,\n",
       "   21,\n",
       "   28,\n",
       "   38,\n",
       "   39,\n",
       "   16,\n",
       "   51,\n",
       "   0,\n",
       "   58,\n",
       "   47,\n",
       "   50,\n",
       "   45,\n",
       "   52,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [14,\n",
       "   19,\n",
       "   6,\n",
       "   12,\n",
       "   8,\n",
       "   30,\n",
       "   5,\n",
       "   23,\n",
       "   16,\n",
       "   28,\n",
       "   1,\n",
       "   36,\n",
       "   27,\n",
       "   33,\n",
       "   7,\n",
       "   10,\n",
       "   20,\n",
       "   31,\n",
       "   9,\n",
       "   37,\n",
       "   4,\n",
       "   24,\n",
       "   0,\n",
       "   3,\n",
       "   26,\n",
       "   32,\n",
       "   17,\n",
       "   22,\n",
       "   25,\n",
       "   29,\n",
       "   13,\n",
       "   18,\n",
       "   11,\n",
       "   21,\n",
       "   15,\n",
       "   35,\n",
       "   2,\n",
       "   34,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [8,\n",
       "   13,\n",
       "   88,\n",
       "   90,\n",
       "   39,\n",
       "   58,\n",
       "   44,\n",
       "   49,\n",
       "   23,\n",
       "   47,\n",
       "   4,\n",
       "   56,\n",
       "   73,\n",
       "   82,\n",
       "   11,\n",
       "   26,\n",
       "   20,\n",
       "   81,\n",
       "   12,\n",
       "   51,\n",
       "   21,\n",
       "   25,\n",
       "   46,\n",
       "   89,\n",
       "   17,\n",
       "   35,\n",
       "   36,\n",
       "   45,\n",
       "   33,\n",
       "   61,\n",
       "   16,\n",
       "   38,\n",
       "   63,\n",
       "   77,\n",
       "   37,\n",
       "   71,\n",
       "   19,\n",
       "   83,\n",
       "   29,\n",
       "   68,\n",
       "   42,\n",
       "   62,\n",
       "   1,\n",
       "   78,\n",
       "   50,\n",
       "   59,\n",
       "   5,\n",
       "   28,\n",
       "   76,\n",
       "   85,\n",
       "   67,\n",
       "   72,\n",
       "   57,\n",
       "   70,\n",
       "   30,\n",
       "   34,\n",
       "   9,\n",
       "   64,\n",
       "   14,\n",
       "   31,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   2,\n",
       "   79,\n",
       "   65,\n",
       "   74,\n",
       "   40,\n",
       "   60,\n",
       "   10,\n",
       "   43,\n",
       "   55,\n",
       "   66,\n",
       "   32,\n",
       "   84,\n",
       "   54,\n",
       "   91,\n",
       "   6,\n",
       "   41,\n",
       "   18,\n",
       "   75,\n",
       "   3,\n",
       "   7,\n",
       "   69,\n",
       "   87,\n",
       "   27,\n",
       "   48,\n",
       "   80,\n",
       "   86,\n",
       "   0,\n",
       "   24,\n",
       "   52,\n",
       "   53,\n",
       "   15,\n",
       "   22],\n",
       "  [22,\n",
       "   46,\n",
       "   59,\n",
       "   80,\n",
       "   35,\n",
       "   64,\n",
       "   30,\n",
       "   62,\n",
       "   72,\n",
       "   74,\n",
       "   8,\n",
       "   79,\n",
       "   42,\n",
       "   66,\n",
       "   33,\n",
       "   45,\n",
       "   10,\n",
       "   49,\n",
       "   29,\n",
       "   52,\n",
       "   7,\n",
       "   9,\n",
       "   81,\n",
       "   82,\n",
       "   16,\n",
       "   32,\n",
       "   27,\n",
       "   68,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   61,\n",
       "   65,\n",
       "   15,\n",
       "   78,\n",
       "   28,\n",
       "   40,\n",
       "   31,\n",
       "   60,\n",
       "   5,\n",
       "   63,\n",
       "   0,\n",
       "   87,\n",
       "   3,\n",
       "   38,\n",
       "   36,\n",
       "   76,\n",
       "   14,\n",
       "   55,\n",
       "   67,\n",
       "   73,\n",
       "   21,\n",
       "   34,\n",
       "   18,\n",
       "   44,\n",
       "   19,\n",
       "   43,\n",
       "   37,\n",
       "   50,\n",
       "   20,\n",
       "   75,\n",
       "   12,\n",
       "   48,\n",
       "   17,\n",
       "   71,\n",
       "   39,\n",
       "   70,\n",
       "   11,\n",
       "   47,\n",
       "   6,\n",
       "   77,\n",
       "   23,\n",
       "   26,\n",
       "   1,\n",
       "   13,\n",
       "   24,\n",
       "   51,\n",
       "   4,\n",
       "   25,\n",
       "   53,\n",
       "   57,\n",
       "   41,\n",
       "   58,\n",
       "   56,\n",
       "   69,\n",
       "   2,\n",
       "   54,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [1,\n",
       "   56,\n",
       "   12,\n",
       "   19,\n",
       "   8,\n",
       "   68,\n",
       "   31,\n",
       "   33,\n",
       "   7,\n",
       "   34,\n",
       "   23,\n",
       "   30,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   21,\n",
       "   27,\n",
       "   24,\n",
       "   57,\n",
       "   3,\n",
       "   74,\n",
       "   45,\n",
       "   79,\n",
       "   26,\n",
       "   55,\n",
       "   62,\n",
       "   82,\n",
       "   15,\n",
       "   78,\n",
       "   11,\n",
       "   40,\n",
       "   9,\n",
       "   38,\n",
       "   75,\n",
       "   87,\n",
       "   37,\n",
       "   66,\n",
       "   0,\n",
       "   6,\n",
       "   73,\n",
       "   80,\n",
       "   10,\n",
       "   50,\n",
       "   5,\n",
       "   36,\n",
       "   44,\n",
       "   46,\n",
       "   4,\n",
       "   18,\n",
       "   43,\n",
       "   63,\n",
       "   48,\n",
       "   64,\n",
       "   13,\n",
       "   32,\n",
       "   28,\n",
       "   77,\n",
       "   49,\n",
       "   70,\n",
       "   29,\n",
       "   67,\n",
       "   52,\n",
       "   53,\n",
       "   25,\n",
       "   58,\n",
       "   22,\n",
       "   76,\n",
       "   17,\n",
       "   47,\n",
       "   41,\n",
       "   69,\n",
       "   39,\n",
       "   54,\n",
       "   20,\n",
       "   35,\n",
       "   14,\n",
       "   60,\n",
       "   16,\n",
       "   72,\n",
       "   61,\n",
       "   65,\n",
       "   59,\n",
       "   81,\n",
       "   51,\n",
       "   71,\n",
       "   2,\n",
       "   42,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [37,\n",
       "   54,\n",
       "   2,\n",
       "   42,\n",
       "   28,\n",
       "   59,\n",
       "   36,\n",
       "   73,\n",
       "   15,\n",
       "   61,\n",
       "   5,\n",
       "   39,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   26,\n",
       "   48,\n",
       "   4,\n",
       "   56,\n",
       "   16,\n",
       "   55,\n",
       "   21,\n",
       "   49,\n",
       "   33,\n",
       "   69,\n",
       "   18,\n",
       "   60,\n",
       "   22,\n",
       "   63,\n",
       "   30,\n",
       "   58,\n",
       "   32,\n",
       "   35,\n",
       "   23,\n",
       "   41,\n",
       "   57,\n",
       "   70,\n",
       "   50,\n",
       "   79,\n",
       "   1,\n",
       "   53,\n",
       "   40,\n",
       "   51,\n",
       "   6,\n",
       "   20,\n",
       "   47,\n",
       "   71,\n",
       "   14,\n",
       "   44,\n",
       "   8,\n",
       "   52,\n",
       "   9,\n",
       "   34,\n",
       "   24,\n",
       "   46,\n",
       "   25,\n",
       "   27,\n",
       "   19,\n",
       "   64,\n",
       "   67,\n",
       "   68,\n",
       "   66,\n",
       "   72,\n",
       "   3,\n",
       "   65,\n",
       "   11,\n",
       "   17,\n",
       "   7,\n",
       "   29,\n",
       "   31,\n",
       "   43,\n",
       "   13,\n",
       "   45,\n",
       "   0,\n",
       "   10,\n",
       "   62,\n",
       "   74,\n",
       "   12,\n",
       "   38],\n",
       "  [41,\n",
       "   73,\n",
       "   44,\n",
       "   84,\n",
       "   61,\n",
       "   69,\n",
       "   49,\n",
       "   70,\n",
       "   71,\n",
       "   88,\n",
       "   64,\n",
       "   89,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   29,\n",
       "   74,\n",
       "   1,\n",
       "   76,\n",
       "   56,\n",
       "   85,\n",
       "   50,\n",
       "   107,\n",
       "   66,\n",
       "   79,\n",
       "   16,\n",
       "   101,\n",
       "   5,\n",
       "   46,\n",
       "   20,\n",
       "   100,\n",
       "   103,\n",
       "   105,\n",
       "   34,\n",
       "   112,\n",
       "   26,\n",
       "   59,\n",
       "   13,\n",
       "   86,\n",
       "   54,\n",
       "   108,\n",
       "   53,\n",
       "   98,\n",
       "   51,\n",
       "   67,\n",
       "   14,\n",
       "   39,\n",
       "   81,\n",
       "   97,\n",
       "   11,\n",
       "   31,\n",
       "   80,\n",
       "   102,\n",
       "   7,\n",
       "   92,\n",
       "   27,\n",
       "   58,\n",
       "   62,\n",
       "   68,\n",
       "   28,\n",
       "   96,\n",
       "   65,\n",
       "   106,\n",
       "   21,\n",
       "   47,\n",
       "   22,\n",
       "   48,\n",
       "   77,\n",
       "   94,\n",
       "   32,\n",
       "   72,\n",
       "   4,\n",
       "   55,\n",
       "   23,\n",
       "   57,\n",
       "   10,\n",
       "   43,\n",
       "   8,\n",
       "   78,\n",
       "   18,\n",
       "   82,\n",
       "   24,\n",
       "   60,\n",
       "   6,\n",
       "   111,\n",
       "   118,\n",
       "   119,\n",
       "   95,\n",
       "   99,\n",
       "   17,\n",
       "   63,\n",
       "   12,\n",
       "   35,\n",
       "   33,\n",
       "   36,\n",
       "   0,\n",
       "   3,\n",
       "   90,\n",
       "   109,\n",
       "   9,\n",
       "   42,\n",
       "   37,\n",
       "   40,\n",
       "   30,\n",
       "   75,\n",
       "   15,\n",
       "   104,\n",
       "   110,\n",
       "   113,\n",
       "   2,\n",
       "   87,\n",
       "   19,\n",
       "   52,\n",
       "   25,\n",
       "   83,\n",
       "   45,\n",
       "   91,\n",
       "   38,\n",
       "   93,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127],\n",
       "  [41,\n",
       "   71,\n",
       "   8,\n",
       "   31,\n",
       "   9,\n",
       "   87,\n",
       "   54,\n",
       "   66,\n",
       "   56,\n",
       "   84,\n",
       "   11,\n",
       "   36,\n",
       "   76,\n",
       "   78,\n",
       "   14,\n",
       "   82,\n",
       "   30,\n",
       "   49,\n",
       "   52,\n",
       "   74,\n",
       "   0,\n",
       "   68,\n",
       "   26,\n",
       "   45,\n",
       "   24,\n",
       "   55,\n",
       "   42,\n",
       "   70,\n",
       "   19,\n",
       "   44,\n",
       "   13,\n",
       "   20,\n",
       "   32,\n",
       "   80,\n",
       "   3,\n",
       "   58,\n",
       "   23,\n",
       "   47,\n",
       "   4,\n",
       "   72,\n",
       "   46,\n",
       "   51,\n",
       "   16,\n",
       "   50,\n",
       "   28,\n",
       "   64,\n",
       "   21,\n",
       "   39,\n",
       "   2,\n",
       "   62,\n",
       "   77,\n",
       "   85,\n",
       "   61,\n",
       "   75,\n",
       "   18,\n",
       "   53,\n",
       "   5,\n",
       "   69,\n",
       "   22,\n",
       "   57,\n",
       "   73,\n",
       "   83,\n",
       "   7,\n",
       "   34,\n",
       "   48,\n",
       "   60,\n",
       "   6,\n",
       "   17,\n",
       "   35,\n",
       "   63,\n",
       "   10,\n",
       "   29,\n",
       "   27,\n",
       "   65,\n",
       "   25,\n",
       "   37,\n",
       "   33,\n",
       "   40,\n",
       "   12,\n",
       "   15,\n",
       "   1,\n",
       "   79,\n",
       "   59,\n",
       "   67,\n",
       "   43,\n",
       "   81,\n",
       "   38,\n",
       "   86,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [116,\n",
       "   178,\n",
       "   106,\n",
       "   191,\n",
       "   91,\n",
       "   172,\n",
       "   159,\n",
       "   186,\n",
       "   119,\n",
       "   170,\n",
       "   12,\n",
       "   120,\n",
       "   41,\n",
       "   104,\n",
       "   118,\n",
       "   180,\n",
       "   96,\n",
       "   157,\n",
       "   15,\n",
       "   147,\n",
       "   63,\n",
       "   124,\n",
       "   174,\n",
       "   179,\n",
       "   37,\n",
       "   181,\n",
       "   11,\n",
       "   55,\n",
       "   61,\n",
       "   72,\n",
       "   3,\n",
       "   158,\n",
       "   0,\n",
       "   94,\n",
       "   74,\n",
       "   122,\n",
       "   10,\n",
       "   112,\n",
       "   146,\n",
       "   164,\n",
       "   141,\n",
       "   144,\n",
       "   34,\n",
       "   108,\n",
       "   22,\n",
       "   125,\n",
       "   20,\n",
       "   109,\n",
       "   48,\n",
       "   188,\n",
       "   139,\n",
       "   149,\n",
       "   36,\n",
       "   86,\n",
       "   40,\n",
       "   88,\n",
       "   29,\n",
       "   135,\n",
       "   101,\n",
       "   140,\n",
       "   43,\n",
       "   136,\n",
       "   18,\n",
       "   89,\n",
       "   99,\n",
       "   153,\n",
       "   121,\n",
       "   133,\n",
       "   113,\n",
       "   123,\n",
       "   9,\n",
       "   82,\n",
       "   51,\n",
       "   166,\n",
       "   62,\n",
       "   100,\n",
       "   31,\n",
       "   87,\n",
       "   192,\n",
       "   193,\n",
       "   2,\n",
       "   165,\n",
       "   71,\n",
       "   154,\n",
       "   33,\n",
       "   54,\n",
       "   5,\n",
       "   143,\n",
       "   4,\n",
       "   187,\n",
       "   30,\n",
       "   167,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   23,\n",
       "   175,\n",
       "   103,\n",
       "   198,\n",
       "   8,\n",
       "   56,\n",
       "   7,\n",
       "   17,\n",
       "   110,\n",
       "   177,\n",
       "   107,\n",
       "   183,\n",
       "   66,\n",
       "   117,\n",
       "   50,\n",
       "   168,\n",
       "   42,\n",
       "   93,\n",
       "   14,\n",
       "   182,\n",
       "   105,\n",
       "   156,\n",
       "   26,\n",
       "   57,\n",
       "   145,\n",
       "   176,\n",
       "   78,\n",
       "   151,\n",
       "   19,\n",
       "   77,\n",
       "   60,\n",
       "   148,\n",
       "   28,\n",
       "   114,\n",
       "   73,\n",
       "   171,\n",
       "   13,\n",
       "   84,\n",
       "   92,\n",
       "   126,\n",
       "   38,\n",
       "   190,\n",
       "   21,\n",
       "   81,\n",
       "   65,\n",
       "   137,\n",
       "   1,\n",
       "   83,\n",
       "   79,\n",
       "   163,\n",
       "   35,\n",
       "   68,\n",
       "   27,\n",
       "   32,\n",
       "   69,\n",
       "   152,\n",
       "   161,\n",
       "   173,\n",
       "   53,\n",
       "   80,\n",
       "   45,\n",
       "   111,\n",
       "   97,\n",
       "   185,\n",
       "   39,\n",
       "   184,\n",
       "   64,\n",
       "   199,\n",
       "   25,\n",
       "   127,\n",
       "   131,\n",
       "   142,\n",
       "   102,\n",
       "   155,\n",
       "   132,\n",
       "   189,\n",
       "   58,\n",
       "   134,\n",
       "   70,\n",
       "   129,\n",
       "   49,\n",
       "   138,\n",
       "   85,\n",
       "   128,\n",
       "   6,\n",
       "   162,\n",
       "   52,\n",
       "   67,\n",
       "   24,\n",
       "   95,\n",
       "   76,\n",
       "   160,\n",
       "   150,\n",
       "   169,\n",
       "   44,\n",
       "   46,\n",
       "   16,\n",
       "   59,\n",
       "   75,\n",
       "   90,\n",
       "   47,\n",
       "   130,\n",
       "   98,\n",
       "   115,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207],\n",
       "  [58,\n",
       "   111,\n",
       "   74,\n",
       "   92,\n",
       "   87,\n",
       "   123,\n",
       "   36,\n",
       "   80,\n",
       "   100,\n",
       "   104,\n",
       "   4,\n",
       "   27,\n",
       "   51,\n",
       "   94,\n",
       "   69,\n",
       "   117,\n",
       "   105,\n",
       "   108,\n",
       "   3,\n",
       "   119,\n",
       "   40,\n",
       "   78,\n",
       "   22,\n",
       "   71,\n",
       "   106,\n",
       "   122,\n",
       "   7,\n",
       "   37,\n",
       "   57,\n",
       "   115,\n",
       "   23,\n",
       "   90,\n",
       "   32,\n",
       "   91,\n",
       "   31,\n",
       "   125,\n",
       "   64,\n",
       "   103,\n",
       "   9,\n",
       "   43,\n",
       "   33,\n",
       "   107,\n",
       "   12,\n",
       "   53,\n",
       "   54,\n",
       "   88,\n",
       "   89,\n",
       "   110,\n",
       "   13,\n",
       "   34,\n",
       "   41,\n",
       "   121,\n",
       "   21,\n",
       "   46,\n",
       "   98,\n",
       "   118,\n",
       "   42,\n",
       "   60,\n",
       "   97,\n",
       "   124,\n",
       "   49,\n",
       "   95,\n",
       "   126,\n",
       "   127,\n",
       "   26,\n",
       "   77,\n",
       "   35,\n",
       "   67,\n",
       "   24,\n",
       "   109,\n",
       "   29,\n",
       "   39,\n",
       "   48,\n",
       "   52,\n",
       "   86,\n",
       "   113,\n",
       "   15,\n",
       "   30,\n",
       "   84,\n",
       "   102,\n",
       "   0,\n",
       "   44,\n",
       "   28,\n",
       "   61,\n",
       "   59,\n",
       "   112,\n",
       "   81,\n",
       "   96,\n",
       "   63,\n",
       "   75,\n",
       "   6,\n",
       "   38,\n",
       "   8,\n",
       "   68,\n",
       "   82,\n",
       "   116,\n",
       "   65,\n",
       "   85,\n",
       "   25,\n",
       "   76,\n",
       "   2,\n",
       "   20,\n",
       "   1,\n",
       "   56,\n",
       "   62,\n",
       "   70,\n",
       "   66,\n",
       "   73,\n",
       "   17,\n",
       "   114,\n",
       "   72,\n",
       "   93,\n",
       "   16,\n",
       "   45,\n",
       "   10,\n",
       "   50,\n",
       "   5,\n",
       "   47,\n",
       "   11,\n",
       "   120,\n",
       "   83,\n",
       "   99,\n",
       "   18,\n",
       "   101,\n",
       "   19,\n",
       "   79,\n",
       "   14,\n",
       "   55],\n",
       "  [16,\n",
       "   66,\n",
       "   68,\n",
       "   76,\n",
       "   14,\n",
       "   103,\n",
       "   52,\n",
       "   53,\n",
       "   39,\n",
       "   43,\n",
       "   90,\n",
       "   93,\n",
       "   36,\n",
       "   92,\n",
       "   67,\n",
       "   89,\n",
       "   17,\n",
       "   69,\n",
       "   86,\n",
       "   101,\n",
       "   19,\n",
       "   65,\n",
       "   44,\n",
       "   75,\n",
       "   3,\n",
       "   47,\n",
       "   40,\n",
       "   72,\n",
       "   20,\n",
       "   49,\n",
       "   31,\n",
       "   45,\n",
       "   6,\n",
       "   24,\n",
       "   8,\n",
       "   85,\n",
       "   23,\n",
       "   74,\n",
       "   28,\n",
       "   78,\n",
       "   22,\n",
       "   54,\n",
       "   42,\n",
       "   60,\n",
       "   48,\n",
       "   50,\n",
       "   57,\n",
       "   61,\n",
       "   9,\n",
       "   99,\n",
       "   27,\n",
       "   51,\n",
       "   1,\n",
       "   59,\n",
       "   97,\n",
       "   102,\n",
       "   37,\n",
       "   70,\n",
       "   55,\n",
       "   73,\n",
       "   4,\n",
       "   46,\n",
       "   12,\n",
       "   95,\n",
       "   18,\n",
       "   41,\n",
       "   29,\n",
       "   30,\n",
       "   62,\n",
       "   96,\n",
       "   83,\n",
       "   100,\n",
       "   79,\n",
       "   80,\n",
       "   13,\n",
       "   87,\n",
       "   15,\n",
       "   82,\n",
       "   71,\n",
       "   94,\n",
       "   77,\n",
       "   84,\n",
       "   0,\n",
       "   33,\n",
       "   2,\n",
       "   81,\n",
       "   56,\n",
       "   91,\n",
       "   63,\n",
       "   64,\n",
       "   34,\n",
       "   98,\n",
       "   7,\n",
       "   35,\n",
       "   25,\n",
       "   32,\n",
       "   38,\n",
       "   88,\n",
       "   10,\n",
       "   11,\n",
       "   26,\n",
       "   58,\n",
       "   5,\n",
       "   21,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111],\n",
       "  [7,\n",
       "   15,\n",
       "   122,\n",
       "   181,\n",
       "   115,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   180,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   110,\n",
       "   192,\n",
       "   193,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   114,\n",
       "   207,\n",
       "   208,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   212,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   221,\n",
       "   80,\n",
       "   222,\n",
       "   223,\n",
       "   224,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   106,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   179,\n",
       "   252,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   27,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   272,\n",
       "   273,\n",
       "   274,\n",
       "   275,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   280,\n",
       "   281,\n",
       "   68,\n",
       "   282,\n",
       "   283,\n",
       "   284,\n",
       "   285,\n",
       "   286,\n",
       "   287,\n",
       "   288,\n",
       "   289,\n",
       "   290,\n",
       "   291,\n",
       "   292,\n",
       "   293,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   100,\n",
       "   297,\n",
       "   298,\n",
       "   299,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   303,\n",
       "   304,\n",
       "   305,\n",
       "   306,\n",
       "   307,\n",
       "   308,\n",
       "   309,\n",
       "   310,\n",
       "   311,\n",
       "   37,\n",
       "   312,\n",
       "   313,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   130,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   331,\n",
       "   332,\n",
       "   333,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   337,\n",
       "   338,\n",
       "   339,\n",
       "   340,\n",
       "   341,\n",
       "   148,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   345,\n",
       "   346,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   350,\n",
       "   351,\n",
       "   352,\n",
       "   353,\n",
       "   354,\n",
       "   355,\n",
       "   356,\n",
       "   125,\n",
       "   357,\n",
       "   358,\n",
       "   359,\n",
       "   360,\n",
       "   361,\n",
       "   362,\n",
       "   363,\n",
       "   364,\n",
       "   365,\n",
       "   366,\n",
       "   367,\n",
       "   368,\n",
       "   369,\n",
       "   370,\n",
       "   371,\n",
       "   69,\n",
       "   372,\n",
       "   373,\n",
       "   374,\n",
       "   375,\n",
       "   376,\n",
       "   377,\n",
       "   378,\n",
       "   379,\n",
       "   380,\n",
       "   381,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   168,\n",
       "   387,\n",
       "   388,\n",
       "   389,\n",
       "   390,\n",
       "   391,\n",
       "   392,\n",
       "   393,\n",
       "   394,\n",
       "   395,\n",
       "   396,\n",
       "   397,\n",
       "   398,\n",
       "   399,\n",
       "   400,\n",
       "   401,\n",
       "   6,\n",
       "   402,\n",
       "   403,\n",
       "   404,\n",
       "   405,\n",
       "   406,\n",
       "   407,\n",
       "   408,\n",
       "   409,\n",
       "   410,\n",
       "   411,\n",
       "   412,\n",
       "   413,\n",
       "   414,\n",
       "   415,\n",
       "   416,\n",
       "   156,\n",
       "   417,\n",
       "   418,\n",
       "   419,\n",
       "   420,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   424,\n",
       "   425,\n",
       "   426,\n",
       "   427,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   431,\n",
       "   103,\n",
       "   432,\n",
       "   433,\n",
       "   434,\n",
       "   435,\n",
       "   436,\n",
       "   437,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   441,\n",
       "   442,\n",
       "   443,\n",
       "   444,\n",
       "   445,\n",
       "   446,\n",
       "   95,\n",
       "   447,\n",
       "   448,\n",
       "   449,\n",
       "   450,\n",
       "   451,\n",
       "   452,\n",
       "   453,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   457,\n",
       "   458,\n",
       "   459,\n",
       "   460,\n",
       "   461,\n",
       "   44,\n",
       "   462,\n",
       "   463,\n",
       "   464,\n",
       "   465,\n",
       "   466,\n",
       "   467,\n",
       "   468,\n",
       "   469,\n",
       "   470,\n",
       "   471,\n",
       "   472,\n",
       "   473,\n",
       "   474,\n",
       "   475,\n",
       "   476,\n",
       "   136,\n",
       "   477,\n",
       "   478,\n",
       "   479,\n",
       "   480,\n",
       "   481,\n",
       "   482,\n",
       "   483,\n",
       "   484,\n",
       "   485,\n",
       "   486,\n",
       "   487,\n",
       "   488,\n",
       "   489,\n",
       "   490,\n",
       "   491,\n",
       "   175,\n",
       "   492,\n",
       "   493,\n",
       "   494,\n",
       "   495,\n",
       "   496,\n",
       "   497,\n",
       "   498,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   502,\n",
       "   503,\n",
       "   504,\n",
       "   505,\n",
       "   506,\n",
       "   54,\n",
       "   507,\n",
       "   508,\n",
       "   509,\n",
       "   510,\n",
       "   511,\n",
       "   512,\n",
       "   513,\n",
       "   514,\n",
       "   515,\n",
       "   516,\n",
       "   517,\n",
       "   518,\n",
       "   519,\n",
       "   520,\n",
       "   521,\n",
       "   31,\n",
       "   522,\n",
       "   523,\n",
       "   524,\n",
       "   525,\n",
       "   526,\n",
       "   527,\n",
       "   528,\n",
       "   529,\n",
       "   530,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   535,\n",
       "   536,\n",
       "   1,\n",
       "   537,\n",
       "   538,\n",
       "   539,\n",
       "   540,\n",
       "   541,\n",
       "   542,\n",
       "   543,\n",
       "   544,\n",
       "   545,\n",
       "   546,\n",
       "   547,\n",
       "   548,\n",
       "   549,\n",
       "   550,\n",
       "   551,\n",
       "   25,\n",
       "   552,\n",
       "   553,\n",
       "   554,\n",
       "   555,\n",
       "   556,\n",
       "   557,\n",
       "   558,\n",
       "   559,\n",
       "   560,\n",
       "   561,\n",
       "   562,\n",
       "   563,\n",
       "   564,\n",
       "   565,\n",
       "   566,\n",
       "   101,\n",
       "   567,\n",
       "   568,\n",
       "   569,\n",
       "   570,\n",
       "   571,\n",
       "   572,\n",
       "   573,\n",
       "   574,\n",
       "   575,\n",
       "   576,\n",
       "   577,\n",
       "   578,\n",
       "   579,\n",
       "   580,\n",
       "   581,\n",
       "   91,\n",
       "   582,\n",
       "   583,\n",
       "   584,\n",
       "   585,\n",
       "   586,\n",
       "   587,\n",
       "   588,\n",
       "   589,\n",
       "   590,\n",
       "   591,\n",
       "   592,\n",
       "   593,\n",
       "   594,\n",
       "   595,\n",
       "   596,\n",
       "   78,\n",
       "   597,\n",
       "   598,\n",
       "   599,\n",
       "   600,\n",
       "   601,\n",
       "   602,\n",
       "   603,\n",
       "   604,\n",
       "   605,\n",
       "   606,\n",
       "   607,\n",
       "   608,\n",
       "   609,\n",
       "   610,\n",
       "   611,\n",
       "   161,\n",
       "   612,\n",
       "   613,\n",
       "   614,\n",
       "   615,\n",
       "   616,\n",
       "   617,\n",
       "   618,\n",
       "   619,\n",
       "   620,\n",
       "   621,\n",
       "   622,\n",
       "   623,\n",
       "   624,\n",
       "   625,\n",
       "   626,\n",
       "   118,\n",
       "   627,\n",
       "   628,\n",
       "   629,\n",
       "   630,\n",
       "   631,\n",
       "   632,\n",
       "   633,\n",
       "   634,\n",
       "   635,\n",
       "   636,\n",
       "   637,\n",
       "   638,\n",
       "   639,\n",
       "   640,\n",
       "   641,\n",
       "   84,\n",
       "   642,\n",
       "   643,\n",
       "   644,\n",
       "   645,\n",
       "   646,\n",
       "   647,\n",
       "   648,\n",
       "   649,\n",
       "   650,\n",
       "   651,\n",
       "   652,\n",
       "   653,\n",
       "   654,\n",
       "   655,\n",
       "   656,\n",
       "   152,\n",
       "   657,\n",
       "   658,\n",
       "   659,\n",
       "   660,\n",
       "   661,\n",
       "   662,\n",
       "   663,\n",
       "   664,\n",
       "   665,\n",
       "   666,\n",
       "   667,\n",
       "   668,\n",
       "   669,\n",
       "   670,\n",
       "   671,\n",
       "   56,\n",
       "   672,\n",
       "   673,\n",
       "   674,\n",
       "   675,\n",
       "   676,\n",
       "   677,\n",
       "   678,\n",
       "   679,\n",
       "   680,\n",
       "   681,\n",
       "   682,\n",
       "   683,\n",
       "   684,\n",
       "   685,\n",
       "   686,\n",
       "   90,\n",
       "   687,\n",
       "   688,\n",
       "   689,\n",
       "   690,\n",
       "   691,\n",
       "   692,\n",
       "   693,\n",
       "   694,\n",
       "   695,\n",
       "   696,\n",
       "   697,\n",
       "   698,\n",
       "   699,\n",
       "   700,\n",
       "   701,\n",
       "   42,\n",
       "   702,\n",
       "   703,\n",
       "   704,\n",
       "   705,\n",
       "   706,\n",
       "   707,\n",
       "   708,\n",
       "   709,\n",
       "   710,\n",
       "   711,\n",
       "   712,\n",
       "   713,\n",
       "   714,\n",
       "   715,\n",
       "   716,\n",
       "   83,\n",
       "   717,\n",
       "   718,\n",
       "   719,\n",
       "   720,\n",
       "   721,\n",
       "   722,\n",
       "   723,\n",
       "   724,\n",
       "   725,\n",
       "   726,\n",
       "   727,\n",
       "   728,\n",
       "   729,\n",
       "   730,\n",
       "   731,\n",
       "   75,\n",
       "   732,\n",
       "   733,\n",
       "   734,\n",
       "   735,\n",
       "   736,\n",
       "   737,\n",
       "   738,\n",
       "   739,\n",
       "   740,\n",
       "   741,\n",
       "   742,\n",
       "   743,\n",
       "   744,\n",
       "   745,\n",
       "   746,\n",
       "   0,\n",
       "   28,\n",
       "   128,\n",
       "   747,\n",
       "   166,\n",
       "   748,\n",
       "   749,\n",
       "   750,\n",
       "   116,\n",
       "   143,\n",
       "   751,\n",
       "   752,\n",
       "   753,\n",
       "   754,\n",
       "   755,\n",
       "   756,\n",
       "   97,\n",
       "   124,\n",
       "   92,\n",
       "   757,\n",
       "   153,\n",
       "   758,\n",
       "   759,\n",
       "   760,\n",
       "   9,\n",
       "   129,\n",
       "   135,\n",
       "   761,\n",
       "   762,\n",
       "   763,\n",
       "   764,\n",
       "   765,\n",
       "   23,\n",
       "   79,\n",
       "   167,\n",
       "   766,\n",
       "   133,\n",
       "   171,\n",
       "   767,\n",
       "   768,\n",
       "   33,\n",
       "   48,\n",
       "   38,\n",
       "   61,\n",
       "   140,\n",
       "   154,\n",
       "   39,\n",
       "   151,\n",
       "   2,\n",
       "   66,\n",
       "   144,\n",
       "   769,\n",
       "   94,\n",
       "   770,\n",
       "   771,\n",
       "   772,\n",
       "   24,\n",
       "   160,\n",
       "   5,\n",
       "   773,\n",
       "   142,\n",
       "   155,\n",
       "   53,\n",
       "   65,\n",
       "   52,\n",
       "   147,\n",
       "   36,\n",
       "   774,\n",
       "   55,\n",
       "   173,\n",
       "   43,\n",
       "   50,\n",
       "   105,\n",
       "   170,\n",
       "   137,\n",
       "   150,\n",
       "   88,\n",
       "   165,\n",
       "   86,\n",
       "   775,\n",
       "   30,\n",
       "   776,\n",
       "   777,\n",
       "   778,\n",
       "   72,\n",
       "   131,\n",
       "   29,\n",
       "   149,\n",
       "   32,\n",
       "   45,\n",
       "   51,\n",
       "   64,\n",
       "   41,\n",
       "   62,\n",
       "   4,\n",
       "   26,\n",
       "   58,\n",
       "   108,\n",
       "   102,\n",
       "   172,\n",
       "   57,\n",
       "   104,\n",
       "   21,\n",
       "   81,\n",
       "   19,\n",
       "   35,\n",
       "   34,\n",
       "   138,\n",
       "   109,\n",
       "   164,\n",
       "   779,\n",
       "   780,\n",
       "   8,\n",
       "   63,\n",
       "   169,\n",
       "   781,\n",
       "   67,\n",
       "   77,\n",
       "   16,\n",
       "   22,\n",
       "   134,\n",
       "   162,\n",
       "   40,\n",
       "   59,\n",
       "   146,\n",
       "   163,\n",
       "   12,\n",
       "   178,\n",
       "   96,\n",
       "   177,\n",
       "   14,\n",
       "   782,\n",
       "   76,\n",
       "   99,\n",
       "   11,\n",
       "   139,\n",
       "   10,\n",
       "   176,\n",
       "   60,\n",
       "   98,\n",
       "   117,\n",
       "   157,\n",
       "   13,\n",
       "   107,\n",
       "   46,\n",
       "   93,\n",
       "   74,\n",
       "   783,\n",
       "   112,\n",
       "   132,\n",
       "   3,\n",
       "   120,\n",
       "   113,\n",
       "   174,\n",
       "   18,\n",
       "   71,\n",
       "   82,\n",
       "   119,\n",
       "   47,\n",
       "   158,\n",
       "   85,\n",
       "   126,\n",
       "   17,\n",
       "   87,\n",
       "   20,\n",
       "   141,\n",
       "   49,\n",
       "   145,\n",
       "   73,\n",
       "   89,\n",
       "   121,\n",
       "   123,\n",
       "   111,\n",
       "   127,\n",
       "   70,\n",
       "   159],\n",
       "  [69,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   15,\n",
       "   81,\n",
       "   66,\n",
       "   101,\n",
       "   5,\n",
       "   40,\n",
       "   9,\n",
       "   108,\n",
       "   12,\n",
       "   103,\n",
       "   11,\n",
       "   75,\n",
       "   32,\n",
       "   74,\n",
       "   28,\n",
       "   36,\n",
       "   35,\n",
       "   84,\n",
       "   23,\n",
       "   104,\n",
       "   2,\n",
       "   17,\n",
       "   100,\n",
       "   111,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   107,\n",
       "   110,\n",
       "   43,\n",
       "   91,\n",
       "   0,\n",
       "   34,\n",
       "   59,\n",
       "   79,\n",
       "   29,\n",
       "   80,\n",
       "   20,\n",
       "   37,\n",
       "   54,\n",
       "   109,\n",
       "   3,\n",
       "   98,\n",
       "   33,\n",
       "   53,\n",
       "   71,\n",
       "   86,\n",
       "   76,\n",
       "   88,\n",
       "   6,\n",
       "   78,\n",
       "   57,\n",
       "   95,\n",
       "   27,\n",
       "   42,\n",
       "   1,\n",
       "   4,\n",
       "   38,\n",
       "   60,\n",
       "   8,\n",
       "   85,\n",
       "   21,\n",
       "   89,\n",
       "   41,\n",
       "   97,\n",
       "   18,\n",
       "   102,\n",
       "   13,\n",
       "   47,\n",
       "   44,\n",
       "   64,\n",
       "   19,\n",
       "   68,\n",
       "   14,\n",
       "   24,\n",
       "   30,\n",
       "   31,\n",
       "   52,\n",
       "   112,\n",
       "   56,\n",
       "   92,\n",
       "   61,\n",
       "   93,\n",
       "   50,\n",
       "   106,\n",
       "   62,\n",
       "   63,\n",
       "   70,\n",
       "   82,\n",
       "   25,\n",
       "   49,\n",
       "   73,\n",
       "   99,\n",
       "   48,\n",
       "   90,\n",
       "   58,\n",
       "   77,\n",
       "   16,\n",
       "   46,\n",
       "   7,\n",
       "   83,\n",
       "   26,\n",
       "   39,\n",
       "   45,\n",
       "   94,\n",
       "   51,\n",
       "   96,\n",
       "   22,\n",
       "   72,\n",
       "   65,\n",
       "   105,\n",
       "   10,\n",
       "   67,\n",
       "   55,\n",
       "   87,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127],\n",
       "  [89,\n",
       "   97,\n",
       "   71,\n",
       "   93,\n",
       "   15,\n",
       "   33,\n",
       "   6,\n",
       "   106,\n",
       "   64,\n",
       "   102,\n",
       "   22,\n",
       "   88,\n",
       "   37,\n",
       "   46,\n",
       "   53,\n",
       "   67,\n",
       "   35,\n",
       "   69,\n",
       "   90,\n",
       "   99,\n",
       "   2,\n",
       "   82,\n",
       "   78,\n",
       "   87,\n",
       "   72,\n",
       "   92,\n",
       "   17,\n",
       "   56,\n",
       "   81,\n",
       "   83,\n",
       "   49,\n",
       "   66,\n",
       "   23,\n",
       "   52,\n",
       "   1,\n",
       "   70,\n",
       "   27,\n",
       "   68,\n",
       "   80,\n",
       "   103,\n",
       "   65,\n",
       "   84,\n",
       "   18,\n",
       "   85,\n",
       "   54,\n",
       "   63,\n",
       "   5,\n",
       "   47,\n",
       "   24,\n",
       "   74,\n",
       "   10,\n",
       "   21,\n",
       "   77,\n",
       "   96,\n",
       "   29,\n",
       "   62,\n",
       "   86,\n",
       "   95,\n",
       "   79,\n",
       "   108,\n",
       "   32,\n",
       "   38,\n",
       "   60,\n",
       "   101,\n",
       "   25,\n",
       "   61,\n",
       "   98,\n",
       "   104,\n",
       "   28,\n",
       "   34,\n",
       "   109,\n",
       "   110,\n",
       "   3,\n",
       "   13,\n",
       "   26,\n",
       "   41,\n",
       "   20,\n",
       "   43,\n",
       "   16,\n",
       "   107,\n",
       "   48,\n",
       "   73,\n",
       "   8,\n",
       "   40,\n",
       "   12,\n",
       "   55,\n",
       "   7,\n",
       "   45,\n",
       "   75,\n",
       "   94,\n",
       "   39,\n",
       "   100,\n",
       "   11,\n",
       "   105,\n",
       "   0,\n",
       "   50,\n",
       "   14,\n",
       "   36,\n",
       "   59,\n",
       "   111,\n",
       "   76,\n",
       "   91,\n",
       "   30,\n",
       "   44,\n",
       "   19,\n",
       "   51,\n",
       "   4,\n",
       "   42,\n",
       "   9,\n",
       "   58,\n",
       "   31,\n",
       "   57],\n",
       "  [6,\n",
       "   10,\n",
       "   41,\n",
       "   56,\n",
       "   58,\n",
       "   66,\n",
       "   57,\n",
       "   77,\n",
       "   70,\n",
       "   81,\n",
       "   34,\n",
       "   80,\n",
       "   68,\n",
       "   75,\n",
       "   48,\n",
       "   53,\n",
       "   18,\n",
       "   38,\n",
       "   27,\n",
       "   49,\n",
       "   8,\n",
       "   33,\n",
       "   85,\n",
       "   86,\n",
       "   46,\n",
       "   67,\n",
       "   51,\n",
       "   84,\n",
       "   25,\n",
       "   52,\n",
       "   7,\n",
       "   28,\n",
       "   54,\n",
       "   59,\n",
       "   17,\n",
       "   87,\n",
       "   35,\n",
       "   43,\n",
       "   4,\n",
       "   61,\n",
       "   40,\n",
       "   73,\n",
       "   19,\n",
       "   36,\n",
       "   1,\n",
       "   9,\n",
       "   16,\n",
       "   37,\n",
       "   13,\n",
       "   20,\n",
       "   0,\n",
       "   69,\n",
       "   29,\n",
       "   79,\n",
       "   60,\n",
       "   83,\n",
       "   2,\n",
       "   11,\n",
       "   22,\n",
       "   72,\n",
       "   55,\n",
       "   76,\n",
       "   71,\n",
       "   78,\n",
       "   39,\n",
       "   65,\n",
       "   26,\n",
       "   45,\n",
       "   12,\n",
       "   64,\n",
       "   44,\n",
       "   62,\n",
       "   30,\n",
       "   42,\n",
       "   5,\n",
       "   32,\n",
       "   14,\n",
       "   47,\n",
       "   3,\n",
       "   31,\n",
       "   24,\n",
       "   50,\n",
       "   23,\n",
       "   82,\n",
       "   15,\n",
       "   21,\n",
       "   63,\n",
       "   74,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [151,\n",
       "   227,\n",
       "   192,\n",
       "   228,\n",
       "   98,\n",
       "   125,\n",
       "   117,\n",
       "   195,\n",
       "   11,\n",
       "   70,\n",
       "   32,\n",
       "   212,\n",
       "   201,\n",
       "   205,\n",
       "   96,\n",
       "   255,\n",
       "   150,\n",
       "   257,\n",
       "   19,\n",
       "   245,\n",
       "   106,\n",
       "   293,\n",
       "   218,\n",
       "   230,\n",
       "   42,\n",
       "   73,\n",
       "   134,\n",
       "   232,\n",
       "   120,\n",
       "   281,\n",
       "   215,\n",
       "   225,\n",
       "   25,\n",
       "   34,\n",
       "   74,\n",
       "   302,\n",
       "   188,\n",
       "   263,\n",
       "   16,\n",
       "   306,\n",
       "   204,\n",
       "   206,\n",
       "   65,\n",
       "   297,\n",
       "   101,\n",
       "   118,\n",
       "   144,\n",
       "   196,\n",
       "   95,\n",
       "   202,\n",
       "   142,\n",
       "   158,\n",
       "   76,\n",
       "   294,\n",
       "   61,\n",
       "   66,\n",
       "   38,\n",
       "   264,\n",
       "   57,\n",
       "   94,\n",
       "   4,\n",
       "   6,\n",
       "   37,\n",
       "   119,\n",
       "   197,\n",
       "   233,\n",
       "   53,\n",
       "   107,\n",
       "   5,\n",
       "   168,\n",
       "   207,\n",
       "   261,\n",
       "   127,\n",
       "   282,\n",
       "   22,\n",
       "   160,\n",
       "   28,\n",
       "   254,\n",
       "   36,\n",
       "   249,\n",
       "   165,\n",
       "   304,\n",
       "   83,\n",
       "   301,\n",
       "   250,\n",
       "   289,\n",
       "   3,\n",
       "   183,\n",
       "   121,\n",
       "   140,\n",
       "   139,\n",
       "   194,\n",
       "   174,\n",
       "   241,\n",
       "   69,\n",
       "   138,\n",
       "   17,\n",
       "   288,\n",
       "   124,\n",
       "   203,\n",
       "   229,\n",
       "   248,\n",
       "   77,\n",
       "   108,\n",
       "   189,\n",
       "   266,\n",
       "   240,\n",
       "   295,\n",
       "   307,\n",
       "   308,\n",
       "   309,\n",
       "   310,\n",
       "   104,\n",
       "   136,\n",
       "   30,\n",
       "   224,\n",
       "   103,\n",
       "   287,\n",
       "   21,\n",
       "   56,\n",
       "   219,\n",
       "   260,\n",
       "   13,\n",
       "   48,\n",
       "   102,\n",
       "   170,\n",
       "   0,\n",
       "   43,\n",
       "   135,\n",
       "   222,\n",
       "   33,\n",
       "   116,\n",
       "   24,\n",
       "   270,\n",
       "   44,\n",
       "   181,\n",
       "   14,\n",
       "   161,\n",
       "   146,\n",
       "   209,\n",
       "   29,\n",
       "   41,\n",
       "   2,\n",
       "   242,\n",
       "   9,\n",
       "   163,\n",
       "   164,\n",
       "   171,\n",
       "   80,\n",
       "   159,\n",
       "   86,\n",
       "   155,\n",
       "   238,\n",
       "   292,\n",
       "   123,\n",
       "   275,\n",
       "   211,\n",
       "   213,\n",
       "   79,\n",
       "   283,\n",
       "   109,\n",
       "   237,\n",
       "   88,\n",
       "   252,\n",
       "   133,\n",
       "   223,\n",
       "   54,\n",
       "   89,\n",
       "   128,\n",
       "   141,\n",
       "   178,\n",
       "   279,\n",
       "   114,\n",
       "   285,\n",
       "   198,\n",
       "   278,\n",
       "   143,\n",
       "   291,\n",
       "   179,\n",
       "   311,\n",
       "   51,\n",
       "   149,\n",
       "   68,\n",
       "   220,\n",
       "   27,\n",
       "   276,\n",
       "   72,\n",
       "   175,\n",
       "   113,\n",
       "   187,\n",
       "   40,\n",
       "   50,\n",
       "   64,\n",
       "   186,\n",
       "   156,\n",
       "   271,\n",
       "   185,\n",
       "   269,\n",
       "   62,\n",
       "   91,\n",
       "   59,\n",
       "   298,\n",
       "   97,\n",
       "   231,\n",
       "   122,\n",
       "   284,\n",
       "   87,\n",
       "   112,\n",
       "   75,\n",
       "   234,\n",
       "   256,\n",
       "   296,\n",
       "   173,\n",
       "   290,\n",
       "   18,\n",
       "   46,\n",
       "   23,\n",
       "   26,\n",
       "   10,\n",
       "   182,\n",
       "   180,\n",
       "   247,\n",
       "   12,\n",
       "   131,\n",
       "   81,\n",
       "   258,\n",
       "   132,\n",
       "   208,\n",
       "   84,\n",
       "   268,\n",
       "   190,\n",
       "   214,\n",
       "   15,\n",
       "   239,\n",
       "   262,\n",
       "   273,\n",
       "   8,\n",
       "   274,\n",
       "   130,\n",
       "   166,\n",
       "   169,\n",
       "   236,\n",
       "   221,\n",
       "   244,\n",
       "   217,\n",
       "   253,\n",
       "   31,\n",
       "   226,\n",
       "   100,\n",
       "   153,\n",
       "   259,\n",
       "   277,\n",
       "   71,\n",
       "   216,\n",
       "   20,\n",
       "   115,\n",
       "   147,\n",
       "   154,\n",
       "   105,\n",
       "   145,\n",
       "   47,\n",
       "   162,\n",
       "   184,\n",
       "   193,\n",
       "   99,\n",
       "   167,\n",
       "   265,\n",
       "   280,\n",
       "   49,\n",
       "   90,\n",
       "   63,\n",
       "   286,\n",
       "   35,\n",
       "   243,\n",
       "   67,\n",
       "   210,\n",
       "   172,\n",
       "   303,\n",
       "   111,\n",
       "   199,\n",
       "   58,\n",
       "   92,\n",
       "   82,\n",
       "   85,\n",
       "   45,\n",
       "   52,\n",
       "   110,\n",
       "   299,\n",
       "   39,\n",
       "   126,\n",
       "   93,\n",
       "   200,\n",
       "   55,\n",
       "   235,\n",
       "   60,\n",
       "   129,\n",
       "   246,\n",
       "   251,\n",
       "   152,\n",
       "   267,\n",
       "   137,\n",
       "   272,\n",
       "   7,\n",
       "   157,\n",
       "   191,\n",
       "   305,\n",
       "   176,\n",
       "   177,\n",
       "   148,\n",
       "   300,\n",
       "   1,\n",
       "   78,\n",
       "   312,\n",
       "   313,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   317,\n",
       "   318,\n",
       "   319],\n",
       "  [18,\n",
       "   66,\n",
       "   4,\n",
       "   109,\n",
       "   22,\n",
       "   83,\n",
       "   29,\n",
       "   45,\n",
       "   27,\n",
       "   75,\n",
       "   44,\n",
       "   60,\n",
       "   99,\n",
       "   100,\n",
       "   24,\n",
       "   67,\n",
       "   63,\n",
       "   94,\n",
       "   85,\n",
       "   86,\n",
       "   95,\n",
       "   106,\n",
       "   16,\n",
       "   23,\n",
       "   38,\n",
       "   71,\n",
       "   52,\n",
       "   72,\n",
       "   47,\n",
       "   84,\n",
       "   46,\n",
       "   69,\n",
       "   19,\n",
       "   56,\n",
       "   1,\n",
       "   13,\n",
       "   6,\n",
       "   57,\n",
       "   7,\n",
       "   39,\n",
       "   32,\n",
       "   89,\n",
       "   37,\n",
       "   88,\n",
       "   9,\n",
       "   93,\n",
       "   76,\n",
       "   78,\n",
       "   30,\n",
       "   31,\n",
       "   48,\n",
       "   110,\n",
       "   68,\n",
       "   90,\n",
       "   2,\n",
       "   3,\n",
       "   40,\n",
       "   50,\n",
       "   15,\n",
       "   105,\n",
       "   12,\n",
       "   36,\n",
       "   21,\n",
       "   34,\n",
       "   73,\n",
       "   77,\n",
       "   58,\n",
       "   101,\n",
       "   74,\n",
       "   79,\n",
       "   41,\n",
       "   82,\n",
       "   42,\n",
       "   64,\n",
       "   25,\n",
       "   97,\n",
       "   65,\n",
       "   108,\n",
       "   14,\n",
       "   51,\n",
       "   17,\n",
       "   62,\n",
       "   8,\n",
       "   35,\n",
       "   26,\n",
       "   61,\n",
       "   70,\n",
       "   92,\n",
       "   96,\n",
       "   103,\n",
       "   49,\n",
       "   87,\n",
       "   54,\n",
       "   107,\n",
       "   53,\n",
       "   91,\n",
       "   5,\n",
       "   80,\n",
       "   33,\n",
       "   102,\n",
       "   11,\n",
       "   81,\n",
       "   20,\n",
       "   28,\n",
       "   0,\n",
       "   10,\n",
       "   43,\n",
       "   98,\n",
       "   59,\n",
       "   104,\n",
       "   55,\n",
       "   111],\n",
       "  [16,\n",
       "   70,\n",
       "   34,\n",
       "   77,\n",
       "   25,\n",
       "   46,\n",
       "   3,\n",
       "   52,\n",
       "   29,\n",
       "   75,\n",
       "   45,\n",
       "   58,\n",
       "   14,\n",
       "   53,\n",
       "   86,\n",
       "   87,\n",
       "   13,\n",
       "   82,\n",
       "   65,\n",
       "   68,\n",
       "   5,\n",
       "   61,\n",
       "   28,\n",
       "   42,\n",
       "   71,\n",
       "   80,\n",
       "   26,\n",
       "   66,\n",
       "   50,\n",
       "   76,\n",
       "   18,\n",
       "   36,\n",
       "   54,\n",
       "   57,\n",
       "   0,\n",
       "   64,\n",
       "   33,\n",
       "   74,\n",
       "   44,\n",
       "   48,\n",
       "   17,\n",
       "   31,\n",
       "   10,\n",
       "   73,\n",
       "   19,\n",
       "   22,\n",
       "   20,\n",
       "   24,\n",
       "   56,\n",
       "   59,\n",
       "   43,\n",
       "   69,\n",
       "   51,\n",
       "   81,\n",
       "   47,\n",
       "   62,\n",
       "   15,\n",
       "   67,\n",
       "   21,\n",
       "   49,\n",
       "   12,\n",
       "   84,\n",
       "   38,\n",
       "   78,\n",
       "   37,\n",
       "   83,\n",
       "   60,\n",
       "   72,\n",
       "   9,\n",
       "   27,\n",
       "   4,\n",
       "   8,\n",
       "   11,\n",
       "   23,\n",
       "   1,\n",
       "   30,\n",
       "   6,\n",
       "   55,\n",
       "   32,\n",
       "   41,\n",
       "   35,\n",
       "   39,\n",
       "   40,\n",
       "   79,\n",
       "   7,\n",
       "   85,\n",
       "   2,\n",
       "   63,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [0,\n",
       "   17,\n",
       "   1,\n",
       "   3,\n",
       "   9,\n",
       "   25,\n",
       "   13,\n",
       "   21,\n",
       "   6,\n",
       "   11,\n",
       "   15,\n",
       "   19,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   5,\n",
       "   18,\n",
       "   8,\n",
       "   22,\n",
       "   12,\n",
       "   23,\n",
       "   2,\n",
       "   7,\n",
       "   14,\n",
       "   26,\n",
       "   10,\n",
       "   31,\n",
       "   20,\n",
       "   24,\n",
       "   4,\n",
       "   16],\n",
       "  [28,\n",
       "   116,\n",
       "   33,\n",
       "   97,\n",
       "   15,\n",
       "   79,\n",
       "   16,\n",
       "   53,\n",
       "   5,\n",
       "   110,\n",
       "   44,\n",
       "   96,\n",
       "   87,\n",
       "   99,\n",
       "   41,\n",
       "   49,\n",
       "   45,\n",
       "   54,\n",
       "   3,\n",
       "   20,\n",
       "   46,\n",
       "   59,\n",
       "   7,\n",
       "   77,\n",
       "   10,\n",
       "   85,\n",
       "   42,\n",
       "   112,\n",
       "   21,\n",
       "   104,\n",
       "   86,\n",
       "   108,\n",
       "   2,\n",
       "   75,\n",
       "   78,\n",
       "   119,\n",
       "   35,\n",
       "   106,\n",
       "   4,\n",
       "   61,\n",
       "   62,\n",
       "   89,\n",
       "   101,\n",
       "   117,\n",
       "   17,\n",
       "   56,\n",
       "   19,\n",
       "   22,\n",
       "   40,\n",
       "   82,\n",
       "   70,\n",
       "   95,\n",
       "   6,\n",
       "   11,\n",
       "   25,\n",
       "   43,\n",
       "   9,\n",
       "   76,\n",
       "   18,\n",
       "   58,\n",
       "   60,\n",
       "   84,\n",
       "   52,\n",
       "   92,\n",
       "   14,\n",
       "   23,\n",
       "   32,\n",
       "   81,\n",
       "   24,\n",
       "   107,\n",
       "   50,\n",
       "   93,\n",
       "   37,\n",
       "   80,\n",
       "   1,\n",
       "   12,\n",
       "   13,\n",
       "   73,\n",
       "   26,\n",
       "   48,\n",
       "   74,\n",
       "   114,\n",
       "   36,\n",
       "   94,\n",
       "   64,\n",
       "   71,\n",
       "   30,\n",
       "   102,\n",
       "   8,\n",
       "   91,\n",
       "   68,\n",
       "   111,\n",
       "   0,\n",
       "   109,\n",
       "   51,\n",
       "   118,\n",
       "   83,\n",
       "   88,\n",
       "   47,\n",
       "   55,\n",
       "   39,\n",
       "   90,\n",
       "   72,\n",
       "   98,\n",
       "   27,\n",
       "   65,\n",
       "   31,\n",
       "   67,\n",
       "   57,\n",
       "   103,\n",
       "   38,\n",
       "   100,\n",
       "   66,\n",
       "   115,\n",
       "   34,\n",
       "   113,\n",
       "   29,\n",
       "   105,\n",
       "   63,\n",
       "   69,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127],\n",
       "  [4,\n",
       "   23,\n",
       "   22,\n",
       "   48,\n",
       "   36,\n",
       "   103,\n",
       "   109,\n",
       "   110,\n",
       "   5,\n",
       "   16,\n",
       "   2,\n",
       "   104,\n",
       "   25,\n",
       "   62,\n",
       "   69,\n",
       "   90,\n",
       "   11,\n",
       "   57,\n",
       "   75,\n",
       "   102,\n",
       "   27,\n",
       "   61,\n",
       "   56,\n",
       "   87,\n",
       "   68,\n",
       "   70,\n",
       "   6,\n",
       "   32,\n",
       "   24,\n",
       "   39,\n",
       "   52,\n",
       "   80,\n",
       "   33,\n",
       "   78,\n",
       "   73,\n",
       "   74,\n",
       "   77,\n",
       "   97,\n",
       "   49,\n",
       "   76,\n",
       "   18,\n",
       "   94,\n",
       "   71,\n",
       "   84,\n",
       "   9,\n",
       "   10,\n",
       "   15,\n",
       "   43,\n",
       "   58,\n",
       "   85,\n",
       "   8,\n",
       "   111,\n",
       "   35,\n",
       "   38,\n",
       "   1,\n",
       "   88,\n",
       "   106,\n",
       "   108,\n",
       "   47,\n",
       "   96,\n",
       "   7,\n",
       "   105,\n",
       "   40,\n",
       "   54,\n",
       "   45,\n",
       "   82,\n",
       "   17,\n",
       "   81,\n",
       "   65,\n",
       "   67,\n",
       "   34,\n",
       "   59,\n",
       "   42,\n",
       "   64,\n",
       "   63,\n",
       "   89,\n",
       "   20,\n",
       "   30,\n",
       "   44,\n",
       "   95,\n",
       "   26,\n",
       "   91,\n",
       "   0,\n",
       "   93,\n",
       "   28,\n",
       "   72,\n",
       "   13,\n",
       "   46,\n",
       "   14,\n",
       "   101,\n",
       "   53,\n",
       "   79,\n",
       "   86,\n",
       "   107,\n",
       "   98,\n",
       "   100,\n",
       "   55,\n",
       "   92,\n",
       "   12,\n",
       "   41,\n",
       "   29,\n",
       "   99,\n",
       "   37,\n",
       "   60,\n",
       "   3,\n",
       "   19,\n",
       "   31,\n",
       "   83,\n",
       "   21,\n",
       "   50,\n",
       "   51,\n",
       "   66],\n",
       "  [3,\n",
       "   9,\n",
       "   4,\n",
       "   50,\n",
       "   31,\n",
       "   42,\n",
       "   24,\n",
       "   35,\n",
       "   6,\n",
       "   12,\n",
       "   25,\n",
       "   28,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   40,\n",
       "   46,\n",
       "   15,\n",
       "   55,\n",
       "   8,\n",
       "   18,\n",
       "   32,\n",
       "   36,\n",
       "   1,\n",
       "   27,\n",
       "   20,\n",
       "   21,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   48,\n",
       "   17,\n",
       "   39,\n",
       "   16,\n",
       "   22,\n",
       "   7,\n",
       "   38,\n",
       "   30,\n",
       "   47,\n",
       "   13,\n",
       "   49,\n",
       "   0,\n",
       "   11,\n",
       "   29,\n",
       "   41,\n",
       "   5,\n",
       "   10,\n",
       "   19,\n",
       "   34,\n",
       "   2,\n",
       "   33,\n",
       "   23,\n",
       "   37,\n",
       "   14,\n",
       "   26,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [14,\n",
       "   17,\n",
       "   1,\n",
       "   6,\n",
       "   11,\n",
       "   25,\n",
       "   19,\n",
       "   27,\n",
       "   4,\n",
       "   23,\n",
       "   10,\n",
       "   28,\n",
       "   16,\n",
       "   29,\n",
       "   22,\n",
       "   30,\n",
       "   12,\n",
       "   26,\n",
       "   15,\n",
       "   20,\n",
       "   18,\n",
       "   21,\n",
       "   24,\n",
       "   31,\n",
       "   0,\n",
       "   3,\n",
       "   2,\n",
       "   8,\n",
       "   9,\n",
       "   13,\n",
       "   5,\n",
       "   7],\n",
       "  [41,\n",
       "   47,\n",
       "   22,\n",
       "   42,\n",
       "   21,\n",
       "   26,\n",
       "   2,\n",
       "   6,\n",
       "   27,\n",
       "   31,\n",
       "   36,\n",
       "   43,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   11,\n",
       "   28,\n",
       "   14,\n",
       "   34,\n",
       "   19,\n",
       "   39,\n",
       "   53,\n",
       "   54,\n",
       "   1,\n",
       "   20,\n",
       "   7,\n",
       "   35,\n",
       "   17,\n",
       "   40,\n",
       "   12,\n",
       "   32,\n",
       "   18,\n",
       "   33,\n",
       "   4,\n",
       "   46,\n",
       "   8,\n",
       "   10,\n",
       "   0,\n",
       "   24,\n",
       "   16,\n",
       "   23,\n",
       "   9,\n",
       "   55,\n",
       "   25,\n",
       "   37,\n",
       "   29,\n",
       "   48,\n",
       "   13,\n",
       "   38,\n",
       "   5,\n",
       "   44,\n",
       "   3,\n",
       "   30,\n",
       "   15,\n",
       "   45,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [6,\n",
       "   23,\n",
       "   15,\n",
       "   16,\n",
       "   2,\n",
       "   20,\n",
       "   34,\n",
       "   35,\n",
       "   0,\n",
       "   18,\n",
       "   3,\n",
       "   9,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   28,\n",
       "   29,\n",
       "   5,\n",
       "   32,\n",
       "   19,\n",
       "   31,\n",
       "   26,\n",
       "   30,\n",
       "   14,\n",
       "   21,\n",
       "   4,\n",
       "   22,\n",
       "   7,\n",
       "   13,\n",
       "   11,\n",
       "   24,\n",
       "   25,\n",
       "   27,\n",
       "   1,\n",
       "   8,\n",
       "   10,\n",
       "   33,\n",
       "   12,\n",
       "   17,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [17,\n",
       "   19,\n",
       "   3,\n",
       "   4,\n",
       "   15,\n",
       "   16,\n",
       "   30,\n",
       "   35,\n",
       "   18,\n",
       "   33,\n",
       "   5,\n",
       "   45,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   6,\n",
       "   10,\n",
       "   8,\n",
       "   28,\n",
       "   20,\n",
       "   36,\n",
       "   37,\n",
       "   47,\n",
       "   22,\n",
       "   25,\n",
       "   12,\n",
       "   38,\n",
       "   27,\n",
       "   39,\n",
       "   46,\n",
       "   49,\n",
       "   26,\n",
       "   42,\n",
       "   41,\n",
       "   48,\n",
       "   1,\n",
       "   40,\n",
       "   24,\n",
       "   44,\n",
       "   34,\n",
       "   43,\n",
       "   21,\n",
       "   29,\n",
       "   7,\n",
       "   31,\n",
       "   23,\n",
       "   32,\n",
       "   0,\n",
       "   13,\n",
       "   9,\n",
       "   11,\n",
       "   2,\n",
       "   14,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [1,\n",
       "   44,\n",
       "   30,\n",
       "   42,\n",
       "   34,\n",
       "   38,\n",
       "   19,\n",
       "   54,\n",
       "   28,\n",
       "   55,\n",
       "   22,\n",
       "   52,\n",
       "   13,\n",
       "   20,\n",
       "   8,\n",
       "   16,\n",
       "   35,\n",
       "   45,\n",
       "   17,\n",
       "   41,\n",
       "   4,\n",
       "   49,\n",
       "   12,\n",
       "   31,\n",
       "   25,\n",
       "   26,\n",
       "   43,\n",
       "   53,\n",
       "   5,\n",
       "   18,\n",
       "   7,\n",
       "   37,\n",
       "   2,\n",
       "   6,\n",
       "   0,\n",
       "   27,\n",
       "   40,\n",
       "   47,\n",
       "   14,\n",
       "   15,\n",
       "   33,\n",
       "   51,\n",
       "   9,\n",
       "   23,\n",
       "   3,\n",
       "   50,\n",
       "   24,\n",
       "   29,\n",
       "   36,\n",
       "   48,\n",
       "   10,\n",
       "   11,\n",
       "   21,\n",
       "   39,\n",
       "   32,\n",
       "   46,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   4,\n",
       "   13,\n",
       "   1,\n",
       "   6,\n",
       "   2,\n",
       "   15,\n",
       "   9,\n",
       "   14,\n",
       "   5,\n",
       "   17,\n",
       "   7,\n",
       "   11,\n",
       "   10,\n",
       "   12,\n",
       "   0,\n",
       "   8,\n",
       "   3,\n",
       "   18,\n",
       "   16,\n",
       "   19,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [6,\n",
       "   25,\n",
       "   8,\n",
       "   11,\n",
       "   2,\n",
       "   26,\n",
       "   33,\n",
       "   34,\n",
       "   4,\n",
       "   5,\n",
       "   24,\n",
       "   27,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   12,\n",
       "   21,\n",
       "   31,\n",
       "   39,\n",
       "   1,\n",
       "   20,\n",
       "   19,\n",
       "   22,\n",
       "   17,\n",
       "   30,\n",
       "   15,\n",
       "   32,\n",
       "   7,\n",
       "   13,\n",
       "   3,\n",
       "   9,\n",
       "   18,\n",
       "   28,\n",
       "   16,\n",
       "   29,\n",
       "   0,\n",
       "   10,\n",
       "   14,\n",
       "   23,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [23,\n",
       "   40,\n",
       "   39,\n",
       "   45,\n",
       "   44,\n",
       "   46,\n",
       "   1,\n",
       "   27,\n",
       "   25,\n",
       "   33,\n",
       "   4,\n",
       "   14,\n",
       "   32,\n",
       "   37,\n",
       "   8,\n",
       "   28,\n",
       "   26,\n",
       "   41,\n",
       "   35,\n",
       "   47,\n",
       "   30,\n",
       "   38,\n",
       "   3,\n",
       "   6,\n",
       "   19,\n",
       "   31,\n",
       "   5,\n",
       "   22,\n",
       "   17,\n",
       "   43,\n",
       "   24,\n",
       "   36,\n",
       "   21,\n",
       "   29,\n",
       "   9,\n",
       "   16,\n",
       "   20,\n",
       "   34,\n",
       "   11,\n",
       "   18,\n",
       "   15,\n",
       "   42,\n",
       "   2,\n",
       "   7,\n",
       "   12,\n",
       "   13,\n",
       "   0,\n",
       "   10],\n",
       "  [9,\n",
       "   23,\n",
       "   21,\n",
       "   29,\n",
       "   10,\n",
       "   28,\n",
       "   30,\n",
       "   31,\n",
       "   0,\n",
       "   19,\n",
       "   4,\n",
       "   11,\n",
       "   3,\n",
       "   5,\n",
       "   7,\n",
       "   17,\n",
       "   1,\n",
       "   18,\n",
       "   12,\n",
       "   26,\n",
       "   22,\n",
       "   27,\n",
       "   2,\n",
       "   25,\n",
       "   8,\n",
       "   15,\n",
       "   13,\n",
       "   20,\n",
       "   14,\n",
       "   24,\n",
       "   6,\n",
       "   16],\n",
       "  [25,\n",
       "   42,\n",
       "   41,\n",
       "   48,\n",
       "   1,\n",
       "   28,\n",
       "   12,\n",
       "   24,\n",
       "   33,\n",
       "   43,\n",
       "   0,\n",
       "   44,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   3,\n",
       "   17,\n",
       "   2,\n",
       "   4,\n",
       "   18,\n",
       "   23,\n",
       "   54,\n",
       "   55,\n",
       "   8,\n",
       "   11,\n",
       "   22,\n",
       "   36,\n",
       "   26,\n",
       "   49,\n",
       "   9,\n",
       "   32,\n",
       "   21,\n",
       "   47,\n",
       "   5,\n",
       "   30,\n",
       "   31,\n",
       "   39,\n",
       "   10,\n",
       "   35,\n",
       "   16,\n",
       "   46,\n",
       "   27,\n",
       "   37,\n",
       "   15,\n",
       "   19,\n",
       "   6,\n",
       "   29,\n",
       "   34,\n",
       "   45,\n",
       "   14,\n",
       "   20,\n",
       "   38,\n",
       "   40,\n",
       "   7,\n",
       "   13,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [18,\n",
       "   34,\n",
       "   1,\n",
       "   35,\n",
       "   15,\n",
       "   31,\n",
       "   9,\n",
       "   26,\n",
       "   6,\n",
       "   12,\n",
       "   7,\n",
       "   23,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   22,\n",
       "   33,\n",
       "   28,\n",
       "   30,\n",
       "   27,\n",
       "   32,\n",
       "   5,\n",
       "   11,\n",
       "   14,\n",
       "   25,\n",
       "   4,\n",
       "   16,\n",
       "   3,\n",
       "   29,\n",
       "   13,\n",
       "   20,\n",
       "   2,\n",
       "   10,\n",
       "   0,\n",
       "   19,\n",
       "   17,\n",
       "   24,\n",
       "   8,\n",
       "   21,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [21,\n",
       "   23,\n",
       "   6,\n",
       "   29,\n",
       "   3,\n",
       "   64,\n",
       "   12,\n",
       "   53,\n",
       "   8,\n",
       "   27,\n",
       "   7,\n",
       "   63,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   1,\n",
       "   19,\n",
       "   18,\n",
       "   33,\n",
       "   44,\n",
       "   56,\n",
       "   31,\n",
       "   58,\n",
       "   25,\n",
       "   39,\n",
       "   4,\n",
       "   50,\n",
       "   22,\n",
       "   34,\n",
       "   0,\n",
       "   35,\n",
       "   59,\n",
       "   65,\n",
       "   32,\n",
       "   60,\n",
       "   36,\n",
       "   41,\n",
       "   10,\n",
       "   30,\n",
       "   38,\n",
       "   46,\n",
       "   37,\n",
       "   71,\n",
       "   16,\n",
       "   28,\n",
       "   5,\n",
       "   61,\n",
       "   45,\n",
       "   52,\n",
       "   17,\n",
       "   43,\n",
       "   9,\n",
       "   51,\n",
       "   24,\n",
       "   57,\n",
       "   15,\n",
       "   26,\n",
       "   13,\n",
       "   66,\n",
       "   55,\n",
       "   62,\n",
       "   11,\n",
       "   20,\n",
       "   2,\n",
       "   42,\n",
       "   48,\n",
       "   49,\n",
       "   40,\n",
       "   54,\n",
       "   14,\n",
       "   47,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79],\n",
       "  [24,\n",
       "   40,\n",
       "   39,\n",
       "   48,\n",
       "   14,\n",
       "   35,\n",
       "   17,\n",
       "   18,\n",
       "   29,\n",
       "   36,\n",
       "   16,\n",
       "   46,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   43,\n",
       "   49,\n",
       "   23,\n",
       "   44,\n",
       "   2,\n",
       "   22,\n",
       "   28,\n",
       "   45,\n",
       "   4,\n",
       "   47,\n",
       "   8,\n",
       "   9,\n",
       "   30,\n",
       "   34,\n",
       "   42,\n",
       "   55,\n",
       "   7,\n",
       "   31,\n",
       "   19,\n",
       "   26,\n",
       "   15,\n",
       "   27,\n",
       "   0,\n",
       "   13,\n",
       "   1,\n",
       "   3,\n",
       "   5,\n",
       "   37,\n",
       "   12,\n",
       "   50,\n",
       "   11,\n",
       "   38,\n",
       "   21,\n",
       "   25,\n",
       "   33,\n",
       "   41,\n",
       "   6,\n",
       "   10,\n",
       "   20,\n",
       "   32,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63],\n",
       "  [40,\n",
       "   41,\n",
       "   51,\n",
       "   55,\n",
       "   9,\n",
       "   57,\n",
       "   7,\n",
       "   66,\n",
       "   53,\n",
       "   73,\n",
       "   2,\n",
       "   44,\n",
       "   3,\n",
       "   31,\n",
       "   30,\n",
       "   58,\n",
       "   16,\n",
       "   17,\n",
       "   25,\n",
       "   79,\n",
       "   20,\n",
       "   50,\n",
       "   39,\n",
       "   75,\n",
       "   65,\n",
       "   71,\n",
       "   11,\n",
       "   67,\n",
       "   69,\n",
       "   77,\n",
       "   18,\n",
       "   36,\n",
       "   47,\n",
       "   72,\n",
       "   61,\n",
       "   70,\n",
       "   34,\n",
       "   62,\n",
       "   28,\n",
       "   74,\n",
       "   1,\n",
       "   63,\n",
       "   27,\n",
       "   48,\n",
       "   6,\n",
       "   8,\n",
       "   24,\n",
       "   32,\n",
       "   59,\n",
       "   64,\n",
       "   0,\n",
       "   43,\n",
       "   5,\n",
       "   78,\n",
       "   12,\n",
       "   49,\n",
       "   22,\n",
       "   45,\n",
       "   42,\n",
       "   46,\n",
       "   23,\n",
       "   56,\n",
       "   15,\n",
       "   38,\n",
       "   4,\n",
       "   54,\n",
       "   10,\n",
       "   60,\n",
       "   21,\n",
       "   26,\n",
       "   35,\n",
       "   76,\n",
       "   14,\n",
       "   68,\n",
       "   13,\n",
       "   52,\n",
       "   19,\n",
       "   37,\n",
       "   29,\n",
       "   33],\n",
       "  [0,\n",
       "   86,\n",
       "   45,\n",
       "   48,\n",
       "   10,\n",
       "   13,\n",
       "   21,\n",
       "   84,\n",
       "   11,\n",
       "   80,\n",
       "   62,\n",
       "   69,\n",
       "   46,\n",
       "   64,\n",
       "   33,\n",
       "   37,\n",
       "   57,\n",
       "   90,\n",
       "   50,\n",
       "   96,\n",
       "   18,\n",
       "   95,\n",
       "   6,\n",
       "   25,\n",
       "   71,\n",
       "   83,\n",
       "   17,\n",
       "   63,\n",
       "   31,\n",
       "   82,\n",
       "   52,\n",
       "   89,\n",
       "   19,\n",
       "   100,\n",
       "   12,\n",
       "   20,\n",
       "   85,\n",
       "   87,\n",
       "   42,\n",
       "   97,\n",
       "   24,\n",
       "   72,\n",
       "   28,\n",
       "   49,\n",
       "   61,\n",
       "   98,\n",
       "   23,\n",
       "   70,\n",
       "   47,\n",
       "   60,\n",
       "   74,\n",
       "   81,\n",
       "   7,\n",
       "   99,\n",
       "   101,\n",
       "   102,\n",
       "   68,\n",
       "   73,\n",
       "   29,\n",
       "   55,\n",
       "   53,\n",
       "   59,\n",
       "   1,\n",
       "   51,\n",
       "   22,\n",
       "   66,\n",
       "   15,\n",
       "   103,\n",
       "   4,\n",
       "   77,\n",
       "   3,\n",
       "   75,\n",
       "   14,\n",
       "   36,\n",
       "   8,\n",
       "   65,\n",
       "   16,\n",
       "   88,\n",
       "   27,\n",
       "   32,\n",
       "   76,\n",
       "   93,\n",
       "   78,\n",
       "   94,\n",
       "   5,\n",
       "   56,\n",
       "   40,\n",
       "   92,\n",
       "   43,\n",
       "   79,\n",
       "   9,\n",
       "   39,\n",
       "   35,\n",
       "   91,\n",
       "   2,\n",
       "   58,\n",
       "   26,\n",
       "   38,\n",
       "   41,\n",
       "   67,\n",
       "   30,\n",
       "   54,\n",
       "   34,\n",
       "   44,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111],\n",
       "  [18,\n",
       "   26,\n",
       "   19,\n",
       "   86,\n",
       "   20,\n",
       "   57,\n",
       "   12,\n",
       "   14,\n",
       "   74,\n",
       "   83,\n",
       "   23,\n",
       "   38,\n",
       "   36,\n",
       "   70,\n",
       "   5,\n",
       "   47,\n",
       "   17,\n",
       "   73,\n",
       "   40,\n",
       "   64,\n",
       "   50,\n",
       "   52,\n",
       "   42,\n",
       "   51,\n",
       "   62,\n",
       "   85,\n",
       "   6,\n",
       "   13,\n",
       "   78,\n",
       "   81,\n",
       "   21,\n",
       "   27,\n",
       "   44,\n",
       "   45,\n",
       "   72,\n",
       "   76,\n",
       "   25,\n",
       "   49,\n",
       "   32,\n",
       "   63,\n",
       "   11,\n",
       "   82,\n",
       "   0,\n",
       "   67,\n",
       "   28,\n",
       "   41,\n",
       "   7,\n",
       "   37,\n",
       "   22,\n",
       "   79,\n",
       "   61,\n",
       "   87,\n",
       "   3,\n",
       "   35,\n",
       "   56,\n",
       "   66,\n",
       "   60,\n",
       "   84,\n",
       "   4,\n",
       "   69,\n",
       "   34,\n",
       "   59,\n",
       "   24,\n",
       "   48,\n",
       "   31,\n",
       "   55,\n",
       "   46,\n",
       "   65,\n",
       "   1,\n",
       "   8,\n",
       "   43,\n",
       "   54,\n",
       "   16,\n",
       "   68,\n",
       "   2,\n",
       "   80,\n",
       "   10,\n",
       "   75,\n",
       "   30,\n",
       "   33,\n",
       "   39,\n",
       "   53,\n",
       "   9,\n",
       "   15,\n",
       "   29,\n",
       "   58,\n",
       "   71,\n",
       "   77,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95],\n",
       "  [0,\n",
       "   4,\n",
       "   12,\n",
       "   14,\n",
       "   29,\n",
       "   36,\n",
       "   8,\n",
       "   26,\n",
       "   24,\n",
       "   38,\n",
       "   13,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   17,\n",
       "   20,\n",
       "   23,\n",
       "   33,\n",
       "   18,\n",
       "   19,\n",
       "   27,\n",
       "   34,\n",
       "   5,\n",
       "   28,\n",
       "   1,\n",
       "   39,\n",
       "   7,\n",
       "   25,\n",
       "   15,\n",
       "   35,\n",
       "   9,\n",
       "   22,\n",
       "   37,\n",
       "   40,\n",
       "   6,\n",
       "   16,\n",
       "   46,\n",
       "   47,\n",
       "   2,\n",
       "   11,\n",
       "   3,\n",
       "   10,\n",
       "   21,\n",
       "   30,\n",
       "   31,\n",
       "   32],\n",
       "  [4,\n",
       "   11,\n",
       "   10,\n",
       "   37,\n",
       "   12,\n",
       "   39,\n",
       "   17,\n",
       "   23,\n",
       "   22,\n",
       "   25,\n",
       "   18,\n",
       "   34,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   1,\n",
       "   19,\n",
       "   9,\n",
       "   14,\n",
       "   21,\n",
       "   33,\n",
       "   3,\n",
       "   36,\n",
       "   0,\n",
       "   15,\n",
       "   8,\n",
       "   43,\n",
       "   24,\n",
       "   26,\n",
       "   7,\n",
       "   40,\n",
       "   20,\n",
       "   42,\n",
       "   16,\n",
       "   27,\n",
       "   31,\n",
       "   32,\n",
       "   13,\n",
       "   35,\n",
       "   29,\n",
       "   30,\n",
       "   2,\n",
       "   38,\n",
       "   6,\n",
       "   41,\n",
       "   5,\n",
       "   28],\n",
       "  [5,\n",
       "   36,\n",
       "   9,\n",
       "   45,\n",
       "   33,\n",
       "   35,\n",
       "   46,\n",
       "   47,\n",
       "   3,\n",
       "   43,\n",
       "   21,\n",
       "   24,\n",
       "   0,\n",
       "   11,\n",
       "   6,\n",
       "   13,\n",
       "   17,\n",
       "   40,\n",
       "   32,\n",
       "   42,\n",
       "   1,\n",
       "   25,\n",
       "   8,\n",
       "   37,\n",
       "   10,\n",
       "   23,\n",
       "   15,\n",
       "   19,\n",
       "   27,\n",
       "   30,\n",
       "   14,\n",
       "   38,\n",
       "   22,\n",
       "   44,\n",
       "   12,\n",
       "   18,\n",
       "   2,\n",
       "   28,\n",
       "   7,\n",
       "   20,\n",
       "   26,\n",
       "   34,\n",
       "   16,\n",
       "   41,\n",
       "   29,\n",
       "   39,\n",
       "   4,\n",
       "   31],\n",
       "  [3,\n",
       "   8,\n",
       "   17,\n",
       "   21,\n",
       "   11,\n",
       "   12,\n",
       "   2,\n",
       "   14,\n",
       "   4,\n",
       "   20,\n",
       "   0,\n",
       "   10,\n",
       "   16,\n",
       "   19,\n",
       "   1,\n",
       "   7,\n",
       "   5,\n",
       "   13,\n",
       "   6,\n",
       "   9,\n",
       "   15,\n",
       "   18,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [3,\n",
       "   19,\n",
       "   13,\n",
       "   37,\n",
       "   29,\n",
       "   32,\n",
       "   11,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   0,\n",
       "   28,\n",
       "   9,\n",
       "   31,\n",
       "   5,\n",
       "   12,\n",
       "   25,\n",
       "   30,\n",
       "   15,\n",
       "   35,\n",
       "   1,\n",
       "   36,\n",
       "   38,\n",
       "   39,\n",
       "   26,\n",
       "   33,\n",
       "   4,\n",
       "   8,\n",
       "   22,\n",
       "   23,\n",
       "   7,\n",
       "   34,\n",
       "   6,\n",
       "   18,\n",
       "   16,\n",
       "   24,\n",
       "   10,\n",
       "   27,\n",
       "   2,\n",
       "   14,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [4,\n",
       "   10,\n",
       "   2,\n",
       "   11,\n",
       "   7,\n",
       "   20,\n",
       "   15,\n",
       "   25,\n",
       "   12,\n",
       "   19,\n",
       "   1,\n",
       "   16,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   5,\n",
       "   13,\n",
       "   6,\n",
       "   9,\n",
       "   0,\n",
       "   24,\n",
       "   30,\n",
       "   31,\n",
       "   8,\n",
       "   14,\n",
       "   17,\n",
       "   18,\n",
       "   3,\n",
       "   22,\n",
       "   21,\n",
       "   23],\n",
       "  [15,\n",
       "   33,\n",
       "   4,\n",
       "   63,\n",
       "   12,\n",
       "   52,\n",
       "   13,\n",
       "   19,\n",
       "   7,\n",
       "   11,\n",
       "   31,\n",
       "   49,\n",
       "   8,\n",
       "   55,\n",
       "   2,\n",
       "   56,\n",
       "   34,\n",
       "   47,\n",
       "   22,\n",
       "   57,\n",
       "   35,\n",
       "   54,\n",
       "   28,\n",
       "   29,\n",
       "   17,\n",
       "   61,\n",
       "   42,\n",
       "   48,\n",
       "   3,\n",
       "   46,\n",
       "   20,\n",
       "   27,\n",
       "   26,\n",
       "   41,\n",
       "   5,\n",
       "   45,\n",
       "   25,\n",
       "   30,\n",
       "   6,\n",
       "   24,\n",
       "   21,\n",
       "   44,\n",
       "   14,\n",
       "   38,\n",
       "   43,\n",
       "   60,\n",
       "   50,\n",
       "   62,\n",
       "   1,\n",
       "   53,\n",
       "   0,\n",
       "   16,\n",
       "   9,\n",
       "   37,\n",
       "   18,\n",
       "   36,\n",
       "   32,\n",
       "   40,\n",
       "   10,\n",
       "   39,\n",
       "   51,\n",
       "   58,\n",
       "   23,\n",
       "   59],\n",
       "  [6,\n",
       "   20,\n",
       "   7,\n",
       "   11,\n",
       "   0,\n",
       "   14,\n",
       "   22,\n",
       "   23,\n",
       "   2,\n",
       "   10,\n",
       "   1,\n",
       "   8,\n",
       "   5,\n",
       "   12,\n",
       "   16,\n",
       "   21,\n",
       "   4,\n",
       "   18,\n",
       "   13,\n",
       "   15,\n",
       "   3,\n",
       "   17,\n",
       "   9,\n",
       "   19,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [8,\n",
       "   21,\n",
       "   14,\n",
       "   15,\n",
       "   4,\n",
       "   10,\n",
       "   2,\n",
       "   18,\n",
       "   6,\n",
       "   11,\n",
       "   1,\n",
       "   17,\n",
       "   3,\n",
       "   19,\n",
       "   9,\n",
       "   16,\n",
       "   0,\n",
       "   13,\n",
       "   22,\n",
       "   23,\n",
       "   5,\n",
       "   12,\n",
       "   7,\n",
       "   20,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [5,\n",
       "   15,\n",
       "   12,\n",
       "   21,\n",
       "   0,\n",
       "   4,\n",
       "   7,\n",
       "   19,\n",
       "   2,\n",
       "   11,\n",
       "   8,\n",
       "   9,\n",
       "   3,\n",
       "   10,\n",
       "   16,\n",
       "   18,\n",
       "   17,\n",
       "   20,\n",
       "   13,\n",
       "   14,\n",
       "   1,\n",
       "   6,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [7,\n",
       "   21,\n",
       "   6,\n",
       "   25,\n",
       "   9,\n",
       "   24,\n",
       "   44,\n",
       "   45,\n",
       "   15,\n",
       "   26,\n",
       "   36,\n",
       "   42,\n",
       "   17,\n",
       "   34,\n",
       "   1,\n",
       "   41,\n",
       "   18,\n",
       "   35,\n",
       "   19,\n",
       "   28,\n",
       "   13,\n",
       "   40,\n",
       "   20,\n",
       "   39,\n",
       "   3,\n",
       "   22,\n",
       "   0,\n",
       "   2,\n",
       "   14,\n",
       "   43,\n",
       "   12,\n",
       "   31,\n",
       "   8,\n",
       "   37,\n",
       "   4,\n",
       "   33,\n",
       "   11,\n",
       "   29,\n",
       "   23,\n",
       "   38,\n",
       "   16,\n",
       "   27,\n",
       "   10,\n",
       "   32,\n",
       "   5,\n",
       "   30,\n",
       "   46,\n",
       "   47],\n",
       "  [7,\n",
       "   18,\n",
       "   6,\n",
       "   21,\n",
       "   27,\n",
       "   28,\n",
       "   8,\n",
       "   20,\n",
       "   25,\n",
       "   34,\n",
       "   26,\n",
       "   39,\n",
       "   4,\n",
       "   38,\n",
       "   10,\n",
       "   13,\n",
       "   0,\n",
       "   19,\n",
       "   2,\n",
       "   11,\n",
       "   15,\n",
       "   32,\n",
       "   3,\n",
       "   12,\n",
       "   14,\n",
       "   22,\n",
       "   17,\n",
       "   36,\n",
       "   24,\n",
       "   37,\n",
       "   30,\n",
       "   35,\n",
       "   9,\n",
       "   33,\n",
       "   1,\n",
       "   29,\n",
       "   5,\n",
       "   31,\n",
       "   16,\n",
       "   23,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [8,\n",
       "   11,\n",
       "   7,\n",
       "   26,\n",
       "   2,\n",
       "   9,\n",
       "   23,\n",
       "   25,\n",
       "   13,\n",
       "   29,\n",
       "   12,\n",
       "   39,\n",
       "   27,\n",
       "   33,\n",
       "   17,\n",
       "   36,\n",
       "   3,\n",
       "   24,\n",
       "   0,\n",
       "   44,\n",
       "   4,\n",
       "   43,\n",
       "   16,\n",
       "   32,\n",
       "   20,\n",
       "   38,\n",
       "   21,\n",
       "   35,\n",
       "   5,\n",
       "   10,\n",
       "   30,\n",
       "   37,\n",
       "   15,\n",
       "   22,\n",
       "   40,\n",
       "   41,\n",
       "   6,\n",
       "   14,\n",
       "   31,\n",
       "   45,\n",
       "   34,\n",
       "   42,\n",
       "   1,\n",
       "   19,\n",
       "   18,\n",
       "   28,\n",
       "   46,\n",
       "   47],\n",
       "  [36,\n",
       "   49,\n",
       "   37,\n",
       "   40,\n",
       "   22,\n",
       "   28,\n",
       "   11,\n",
       "   35,\n",
       "   19,\n",
       "   29,\n",
       "   3,\n",
       "   16,\n",
       "   30,\n",
       "   32,\n",
       "   0,\n",
       "   54,\n",
       "   4,\n",
       "   51,\n",
       "   25,\n",
       "   38,\n",
       "   12,\n",
       "   58,\n",
       "   9,\n",
       "   15,\n",
       "   7,\n",
       "   44,\n",
       "   2,\n",
       "   53,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   47,\n",
       "   52,\n",
       "   24,\n",
       "   63,\n",
       "   5,\n",
       "   56,\n",
       "   13,\n",
       "   14,\n",
       "   8,\n",
       "   57,\n",
       "   39,\n",
       "   42,\n",
       "   46,\n",
       "   55,\n",
       "   21,\n",
       "   34,\n",
       "   1,\n",
       "   18,\n",
       "   27,\n",
       "   43,\n",
       "   20,\n",
       "   50,\n",
       "   6,\n",
       "   10,\n",
       "   23,\n",
       "   26,\n",
       "   31,\n",
       "   45,\n",
       "   33,\n",
       "   48,\n",
       "   17,\n",
       "   41],\n",
       "  [3,\n",
       "   24,\n",
       "   0,\n",
       "   27,\n",
       "   1,\n",
       "   14,\n",
       "   26,\n",
       "   36,\n",
       "   21,\n",
       "   25,\n",
       "   23,\n",
       "   31,\n",
       "   6,\n",
       "   19,\n",
       "   2,\n",
       "   33,\n",
       "   10,\n",
       "   18,\n",
       "   13,\n",
       "   17,\n",
       "   4,\n",
       "   5,\n",
       "   38,\n",
       "   39,\n",
       "   22,\n",
       "   37,\n",
       "   30,\n",
       "   35,\n",
       "   12,\n",
       "   32,\n",
       "   15,\n",
       "   34,\n",
       "   20,\n",
       "   29,\n",
       "   9,\n",
       "   16,\n",
       "   11,\n",
       "   28,\n",
       "   7,\n",
       "   8,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47],\n",
       "  [3,\n",
       "   35,\n",
       "   37,\n",
       "   80,\n",
       "   31,\n",
       "   62,\n",
       "   81,\n",
       "   82,\n",
       "   33,\n",
       "   54,\n",
       "   13,\n",
       "   34,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   1,\n",
       "   55,\n",
       "   6,\n",
       "   77,\n",
       "   22,\n",
       "   39,\n",
       "   41,\n",
       "   75,\n",
       "   2,\n",
       "   64,\n",
       "   10,\n",
       "   29,\n",
       "   40,\n",
       "   63,\n",
       "   42,\n",
       "   53,\n",
       "   18,\n",
       "   65,\n",
       "   58,\n",
       "   73,\n",
       "   36,\n",
       "   44,\n",
       "   47,\n",
       "   78,\n",
       "   60,\n",
       "   66,\n",
       "   15,\n",
       "   76,\n",
       "   17,\n",
       "   70,\n",
       "   30,\n",
       "   79,\n",
       "   21,\n",
       "   57,\n",
       "   32,\n",
       "   51,\n",
       "   19,\n",
       "   20,\n",
       "   9,\n",
       "   24,\n",
       "   50,\n",
       "   68,\n",
       "   7,\n",
       "   87,\n",
       "   25,\n",
       "   43,\n",
       "   26,\n",
       "   49,\n",
       "   48,\n",
       "   52,\n",
       "   8,\n",
       "   23,\n",
       "   5,\n",
       "   46,\n",
       "   16,\n",
       "   72,\n",
       "   0,\n",
       "   71,\n",
       "   11,\n",
       "   69,\n",
       "   4,\n",
       "   45,\n",
       "   14,\n",
       "   27,\n",
       "   28,\n",
       "   61,\n",
       "   67,\n",
       "   74,\n",
       "   56,\n",
       "   59,\n",
       "   12,\n",
       "   38,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "---0_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---1_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---2_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---3_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---4_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---5_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---6_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---7_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---8_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---9_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---10_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---11_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---12_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---13_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---14_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---15_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---16_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---17_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---18_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---19_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---20_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---21_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---22_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---23_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---24_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---25_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---26_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---27_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---28_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---29_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---30_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---31_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---32_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---33_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---34_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---35_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---36_th KEGG GCN ---\n",
      "  input: M_0 = 720\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 720 * 40 / 2 = 14400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 14400 * 1 = 14400\n",
      "    biases: M_2 = 1\n",
      "---37_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---38_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---39_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---40_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---41_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---42_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---43_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---44_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---45_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---46_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---47_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---48_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---49_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---50_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---51_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---52_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---53_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---54_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---55_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---56_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---57_th KEGG GCN ---\n",
      "  input: M_0 = 160\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 160 * 40 / 2 = 3200\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3200 * 1 = 3200\n",
      "    biases: M_2 = 1\n",
      "---58_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---59_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---60_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---61_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---62_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---63_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---64_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---65_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---66_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---67_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---68_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---69_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---70_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---71_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---72_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---73_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---74_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---75_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---76_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---77_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---78_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---79_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---80_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---81_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---82_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---83_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---84_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---85_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---86_th KEGG GCN ---\n",
      "  input: M_0 = 192\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 192 * 40 / 2 = 3840\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3840 * 1 = 3840\n",
      "    biases: M_2 = 1\n",
      "---87_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---88_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---89_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---90_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---91_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---92_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---93_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---94_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---95_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---96_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---97_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---98_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---99_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---100_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---101_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---102_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---103_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---104_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---105_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---106_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---107_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---108_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---109_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---110_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---111_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---112_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---113_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---114_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---115_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---116_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---117_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---118_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---119_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---120_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---121_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---122_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---123_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---124_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---125_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---126_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---127_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---128_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---129_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---130_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---131_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---132_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---133_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---134_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---135_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---136_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---137_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---138_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---139_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---140_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---141_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---142_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---143_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---144_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---145_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---146_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---147_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---148_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---149_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---150_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---151_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---152_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---153_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---154_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---155_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---156_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---157_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---158_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---159_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---160_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---161_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---162_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---163_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---164_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---165_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---166_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---167_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---168_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---169_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---170_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---171_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---172_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---173_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---174_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---175_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---176_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---177_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---178_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---179_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---180_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---181_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---182_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---183_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---184_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---185_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---186_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---187_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---188_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---189_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---190_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---191_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---192_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---193_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---194_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---195_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---196_th KEGG GCN ---\n",
      "  input: M_0 = 176\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 176 * 40 / 2 = 3520\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3520 * 1 = 3520\n",
      "    biases: M_2 = 1\n",
      "---197_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---198_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---199_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---200_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---201_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---202_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---203_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---204_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---205_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---206_th KEGG GCN ---\n",
      "  input: M_0 = 144\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 144 * 40 / 2 = 2880\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2880 * 1 = 2880\n",
      "    biases: M_2 = 1\n",
      "---207_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---208_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---209_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---210_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---211_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---212_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---213_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---214_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---215_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---216_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---217_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---218_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---219_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---220_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---221_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---222_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---223_th KEGG GCN ---\n",
      "  input: M_0 = 208\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 208 * 40 / 2 = 4160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 4160 * 1 = 4160\n",
      "    biases: M_2 = 1\n",
      "---224_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---225_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---226_th KEGG GCN ---\n",
      "  input: M_0 = 784\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 784 * 40 / 2 = 15680\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 15680 * 1 = 15680\n",
      "    biases: M_2 = 1\n",
      "---227_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---228_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---229_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---230_th KEGG GCN ---\n",
      "  input: M_0 = 320\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 320 * 40 / 2 = 6400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 6400 * 1 = 6400\n",
      "    biases: M_2 = 1\n",
      "---231_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---232_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---233_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---234_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---235_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---236_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---237_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---238_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---239_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---240_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---241_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---242_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---243_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---244_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---245_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---246_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---247_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---248_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---249_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---250_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---251_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---252_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---253_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---254_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---255_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---256_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---257_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---258_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---259_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---260_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---261_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---262_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---263_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---264_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---265_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---266_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---267_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---268_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "--------------------------------------\n",
      "concat representation: M_0=269\n",
      "shared FCNN architecture\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 1\n",
      "    weights: M_0 * M_1 = 269 * 1 = 269\n",
      "    biases: M_1 = 1\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400 / 5430 (epoch 2.95 / 40):\n",
      "  learning_rate = 1.80e-02, loss_average = 4.55e-02\n",
      "  validation loss: 4.22e-02\n",
      "  time: 1288s (wall 445s)\n",
      "step 800 / 5430 (epoch 5.89 / 40):\n",
      "  learning_rate = 1.55e-02, loss_average = 3.12e-02\n",
      "  validation loss: 3.40e-02\n",
      "  time: 2449s (wall 745s)\n",
      "step 1200 / 5430 (epoch 8.84 / 40):\n",
      "  learning_rate = 1.33e-02, loss_average = 3.69e-02\n",
      "  validation loss: 4.05e-02\n",
      "  time: 3437s (wall 876s)\n",
      "step 1600 / 5430 (epoch 11.79 / 40):\n",
      "  learning_rate = 1.14e-02, loss_average = 3.09e-02\n",
      "  validation loss: 3.39e-02\n",
      "  time: 4429s (wall 1008s)\n",
      "step 2000 / 5430 (epoch 14.73 / 40):\n",
      "  learning_rate = 9.75e-03, loss_average = 2.98e-02\n",
      "  validation loss: 3.68e-02\n",
      "  time: 5411s (wall 1136s)\n",
      "step 2400 / 5430 (epoch 17.68 / 40):\n",
      "  learning_rate = 8.36e-03, loss_average = 3.72e-02\n",
      "  validation loss: 3.21e-02\n",
      "  time: 6401s (wall 1268s)\n",
      "step 2800 / 5430 (epoch 20.63 / 40):\n",
      "  learning_rate = 7.17e-03, loss_average = 2.67e-02\n",
      "  validation loss: 3.14e-02\n",
      "  time: 7361s (wall 1392s)\n",
      "step 3200 / 5430 (epoch 23.57 / 40):\n",
      "  learning_rate = 6.15e-03, loss_average = 3.02e-02\n",
      "  validation loss: 3.13e-02\n",
      "  time: 8246s (wall 1495s)\n",
      "step 3600 / 5430 (epoch 26.52 / 40):\n",
      "  learning_rate = 5.27e-03, loss_average = 1.76e-02\n",
      "  validation loss: 3.26e-02\n",
      "  time: 9135s (wall 1600s)\n",
      "step 4000 / 5430 (epoch 29.47 / 40):\n",
      "  learning_rate = 4.52e-03, loss_average = 2.44e-02\n",
      "  validation loss: 3.43e-02\n",
      "  time: 10020s (wall 1707s)\n",
      "step 4400 / 5430 (epoch 32.41 / 40):\n",
      "  learning_rate = 3.87e-03, loss_average = 2.86e-02\n",
      "  validation loss: 3.14e-02\n",
      "  time: 10907s (wall 1813s)\n",
      "step 4800 / 5430 (epoch 35.36 / 40):\n",
      "  learning_rate = 3.32e-03, loss_average = 2.95e-02\n",
      "  validation loss: 3.07e-02\n",
      "  time: 11791s (wall 1919s)\n",
      "step 5200 / 5430 (epoch 38.31 / 40):\n",
      "  learning_rate = 2.85e-03, loss_average = 2.89e-02\n",
      "  validation loss: 3.06e-02\n",
      "  time: 12679s (wall 2025s)\n",
      "step 5430 / 5430 (epoch 40.00 / 40):\n",
      "  learning_rate = 2.71e-03, loss_average = 3.16e-02\n",
      "  validation loss: 3.06e-02\n",
      "  time: 13224s (wall 2092s)\n",
      "INFO:tensorflow:Restoring parameters from /home/bmllab/bml_ksh/GraphCNN/GraphCNN/lib/../checkpoints/model-5430\n",
      "[0.55442691 0.49004513 0.51122332 0.60374707 0.58282965 0.57190806\n",
      " 0.57065058 0.62569231 0.48583928 0.60970896 0.56145573 0.6344496\n",
      " 0.58767384 0.62044144 0.55316818 0.58256161 0.57402182 0.58231777\n",
      " 0.61471421 0.57739854 0.58288276 0.48233509 0.57028139 0.62463522\n",
      " 0.60293818 0.61754984 0.6218707  0.57494891 0.58784914 0.61410862\n",
      " 0.62200403 0.51682144 0.57198328 0.62728226 0.54754895 0.57336932\n",
      " 0.58666563 0.56931758 0.48191491 0.51875508 0.57671028 0.58170295\n",
      " 0.56959206 0.59253401 0.58918214 0.56279939 0.54407531 0.4861517\n",
      " 0.49976182 0.57531387 0.5665319  0.5658651  0.61186695 0.55216432\n",
      " 0.58866996 0.62691122 0.63879675 0.59240752 0.54534984 0.62682182\n",
      " 0.57215494 0.57180017 0.4287647  0.51007903 0.54702306 0.61750388\n",
      " 0.52462572 0.58351111 0.62320739 0.5782147  0.49292076 0.62698424\n",
      " 0.59983552 0.56465179 0.61809438 0.54370946 0.60664523 0.53642797\n",
      " 0.62016928 0.55102307 0.62096578 0.52124083 0.46644342 0.58435869\n",
      " 0.60267121 0.56457216 0.60402226 0.64527017 0.54776466 0.53750151\n",
      " 0.60887957 0.62015122 0.59822607 0.57963306 0.62364769 0.57218361\n",
      " 0.58892769 0.59605002 0.58778232 0.56140524 0.5778203  0.58547503\n",
      " 0.51192278 0.54664183 0.56251031 0.53775221 0.61174285 0.60294366\n",
      " 0.61774468 0.50612712 0.53313553 0.53863615 0.56952077 0.63533431\n",
      " 0.56104994 0.49185812 0.57948411 0.58729744 0.45501751 0.56893861\n",
      " 0.57336271 0.57336658 0.59424323 0.58468169 0.54279059 0.62668872\n",
      " 0.53493452 0.59609038 0.46402413 0.57935834 0.60088903 0.52125627\n",
      " 0.46542314 0.53686911 0.51424056 0.54210103 0.53237468 0.4424853\n",
      " 0.61012626 0.53230721 0.54596072 0.58716005 0.6086297  0.55707484\n",
      " 0.58904147 0.52891713 0.52676171 0.45087272 0.57322639 0.5551964\n",
      " 0.57365268 0.54287493 0.59711379 0.53262717 0.59308797 0.59252858\n",
      " 0.53583968 0.48780382 0.5873816  0.59804976 0.59426242 0.60645068\n",
      " 0.60428363 0.46250492 0.53254205 0.50110662 0.58040965 0.60398549\n",
      " 0.43386421 0.6233629  0.58710182 0.55423588 0.59908956 0.56926858\n",
      " 0.60805386 0.51248401 0.57484096 0.56841958 0.60964656 0.52146393\n",
      " 0.57759589 0.53101659 0.5574578  0.59364092]\n",
      "[0.89410128 0.10358309 0.54481014 0.83132551 0.56054294 0.27281782\n",
      " 0.83074625 0.81068707 0.53214343 0.51987033 0.62364247 0.61089055\n",
      " 0.57158767 0.60923197 0.44109597 0.61386518 0.57139771 0.81876252\n",
      " 0.72246284 0.71847365 0.68231986 0.52102847 0.35211099 0.36427361\n",
      " 0.68498219 0.56742415 0.66114762 0.71357648        nan 0.34095616\n",
      " 0.70379476 0.20422113 0.66895858 0.58297025 0.53871873 0.47435604\n",
      " 0.5485992         nan 0.83351033 0.43149757 0.7570675  0.52613085\n",
      " 0.46057028 0.54290309 0.74409487 0.67881886 0.72413497 0.25704372\n",
      " 0.64476989 0.27143546 0.71932236 0.79936794 0.66449644 0.08248108\n",
      " 0.34498848 0.59968895 0.80470015 0.35693038 0.63713    0.40412618\n",
      " 0.46459061 0.53801712 0.52891513 0.45618905 0.4517075  0.7237957\n",
      " 0.47912319 0.78934089 0.58082765 0.49134444 0.5544858  0.85510927\n",
      " 0.66101929 0.4414914         nan 0.48775054 0.14208092 0.40314979\n",
      " 1.         0.52006159 0.09108355 0.43245494 0.25363367 0.7242938\n",
      " 0.80969869 0.82553625 0.50813519 0.90462864 0.60054742 0.54892271\n",
      " 0.58439066 0.87708774 0.77539592 0.7919141  0.68768712        nan\n",
      " 0.53014258 0.85596058 0.76140209 0.52029324 0.60954349 0.55715789\n",
      " 0.66356097 0.55175155 0.68838612 0.52227598 0.31485313 0.54390801\n",
      " 0.50945488 0.53379341 0.51104441 0.84431299 0.54556386 0.58418546\n",
      " 0.61130162 0.19746056 0.62136358 0.40943494 0.37461061 0.51166003\n",
      " 0.57112593 0.88652471 0.26796403 0.78504031 0.2972254  0.47324754\n",
      " 0.57720874 0.55732545 0.13574274 0.58020552 0.68590463 0.5917374\n",
      " 0.61957732 0.6306644  0.34835475 0.56663421 0.61205599 0.29003318\n",
      " 0.36682675 0.68880656 0.47912971 0.72406644 0.50082167 0.84477682\n",
      " 0.60072787 0.19055354 0.53910295 0.11009638 0.52019006 0.44494354\n",
      " 0.65646542 0.60984081 0.63977591 0.54733735 0.60097764 0.59420456\n",
      " 0.16843606 0.45329963 0.83243779 0.64401864 0.62718073 0.63730888\n",
      " 0.68389023 0.41119527 0.51601064 0.39857469 0.57997569 0.73956876\n",
      " 0.13402723 0.82415453 0.32922701 0.37102949 0.50834534 0.69964049\n",
      " 0.74001135 0.5649157  0.51509158 0.7192348  0.81371199 0.73262995\n",
      " 0.76975623 0.55283959 0.62656616 0.06852568]\n",
      "--------------------------------------\n",
      "---0_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---1_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---2_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---3_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---4_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---5_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---6_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---7_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---8_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---9_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---10_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---11_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---12_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---13_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---14_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---15_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---16_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---17_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---18_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---19_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---20_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---21_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---22_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---23_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---24_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---25_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---26_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---27_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---28_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---29_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---30_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---31_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---32_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---33_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---34_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---35_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---36_th KEGG GCN ---\n",
      "  input: M_0 = 720\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 720 * 40 / 2 = 14400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 14400 * 1 = 14400\n",
      "    biases: M_2 = 1\n",
      "---37_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---38_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---39_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---40_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---41_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---42_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---43_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---44_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---45_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---46_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---47_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---48_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---49_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---50_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---51_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---52_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---53_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---54_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---55_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---56_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---57_th KEGG GCN ---\n",
      "  input: M_0 = 160\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 160 * 40 / 2 = 3200\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3200 * 1 = 3200\n",
      "    biases: M_2 = 1\n",
      "---58_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---59_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---60_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---61_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---62_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---63_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---64_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---65_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---66_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---67_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---68_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---69_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---70_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---71_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---72_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---73_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---74_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---75_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---76_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---77_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---78_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---79_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---80_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---81_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---82_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---83_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---84_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---85_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---86_th KEGG GCN ---\n",
      "  input: M_0 = 192\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 192 * 40 / 2 = 3840\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3840 * 1 = 3840\n",
      "    biases: M_2 = 1\n",
      "---87_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---88_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---89_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---90_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---91_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---92_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---93_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---94_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---95_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---96_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---97_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---98_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---99_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---100_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---101_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---102_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---103_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---104_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---105_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---106_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---107_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---108_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---109_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---110_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---111_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---112_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---113_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---114_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---115_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---116_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---117_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---118_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---119_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---120_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---121_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---122_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---123_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---124_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---125_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---126_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---127_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---128_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---129_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---130_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---131_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---132_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---133_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---134_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---135_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---136_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---137_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---138_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---139_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---140_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---141_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---142_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---143_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---144_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---145_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---146_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---147_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---148_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---149_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---150_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---151_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---152_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---153_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---154_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---155_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---156_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---157_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---158_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---159_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---160_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---161_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---162_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---163_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---164_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---165_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---166_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---167_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---168_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---169_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---170_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---171_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---172_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---173_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---174_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---175_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---176_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---177_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---178_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---179_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---180_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---181_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---182_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---183_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---184_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---185_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---186_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---187_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---188_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---189_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---190_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---191_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---192_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---193_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---194_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---195_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---196_th KEGG GCN ---\n",
      "  input: M_0 = 176\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 176 * 40 / 2 = 3520\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3520 * 1 = 3520\n",
      "    biases: M_2 = 1\n",
      "---197_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---198_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---199_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---200_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---201_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---202_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---203_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---204_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---205_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---206_th KEGG GCN ---\n",
      "  input: M_0 = 144\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 144 * 40 / 2 = 2880\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2880 * 1 = 2880\n",
      "    biases: M_2 = 1\n",
      "---207_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---208_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---209_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---210_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---211_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---212_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---213_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---214_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---215_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---216_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---217_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---218_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---219_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---220_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---221_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---222_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---223_th KEGG GCN ---\n",
      "  input: M_0 = 208\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 208 * 40 / 2 = 4160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 4160 * 1 = 4160\n",
      "    biases: M_2 = 1\n",
      "---224_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---225_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---226_th KEGG GCN ---\n",
      "  input: M_0 = 784\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 784 * 40 / 2 = 15680\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 15680 * 1 = 15680\n",
      "    biases: M_2 = 1\n",
      "---227_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---228_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---229_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---230_th KEGG GCN ---\n",
      "  input: M_0 = 320\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 320 * 40 / 2 = 6400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 6400 * 1 = 6400\n",
      "    biases: M_2 = 1\n",
      "---231_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---232_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---233_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---234_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---235_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---236_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---237_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---238_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---239_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---240_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---241_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---242_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---243_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---244_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---245_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---246_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---247_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---248_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---249_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---250_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---251_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---252_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---253_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---254_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---255_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---256_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---257_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---258_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---259_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---260_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---261_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---262_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---263_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---264_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---265_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---266_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---267_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---268_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "--------------------------------------\n",
      "concat representation: M_0=269\n",
      "shared FCNN architecture\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 1\n",
      "    weights: M_0 * M_1 = 269 * 1 = 269\n",
      "    biases: M_1 = 1\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400 / 5440 (epoch 2.94 / 40):\n",
      "  learning_rate = 1.80e-02, loss_average = 2.38e-02\n",
      "  validation loss: 2.32e-02\n",
      "  time: 1190s (wall 417s)\n",
      "step 800 / 5440 (epoch 5.88 / 40):\n",
      "  learning_rate = 1.55e-02, loss_average = 1.70e-02\n",
      "  validation loss: 1.83e-02\n",
      "  time: 2245s (wall 698s)\n",
      "step 1200 / 5440 (epoch 8.82 / 40):\n",
      "  learning_rate = 1.33e-02, loss_average = 2.00e-02\n",
      "  validation loss: 1.75e-02\n",
      "  time: 3124s (wall 803s)\n",
      "step 1600 / 5440 (epoch 11.76 / 40):\n",
      "  learning_rate = 1.14e-02, loss_average = 2.33e-02\n",
      "  validation loss: 2.57e-02\n",
      "  time: 4003s (wall 908s)\n",
      "step 2000 / 5440 (epoch 14.71 / 40):\n",
      "  learning_rate = 9.75e-03, loss_average = 1.83e-02\n",
      "  validation loss: 1.98e-02\n",
      "  time: 4883s (wall 1013s)\n",
      "step 2400 / 5440 (epoch 17.65 / 40):\n",
      "  learning_rate = 8.36e-03, loss_average = 1.63e-02\n",
      "  validation loss: 1.68e-02\n",
      "  time: 5761s (wall 1118s)\n",
      "step 2800 / 5440 (epoch 20.59 / 40):\n",
      "  learning_rate = 7.17e-03, loss_average = 1.86e-02\n",
      "  validation loss: 2.56e-02\n",
      "  time: 6640s (wall 1223s)\n",
      "step 3200 / 5440 (epoch 23.53 / 40):\n",
      "  learning_rate = 6.15e-03, loss_average = 1.82e-02\n",
      "  validation loss: 1.82e-02\n",
      "  time: 7516s (wall 1329s)\n",
      "step 3600 / 5440 (epoch 26.47 / 40):\n",
      "  learning_rate = 5.27e-03, loss_average = 1.42e-02\n",
      "  validation loss: 1.78e-02\n",
      "  time: 8394s (wall 1433s)\n",
      "step 4000 / 5440 (epoch 29.41 / 40):\n",
      "  learning_rate = 4.52e-03, loss_average = 1.52e-02\n",
      "  validation loss: 1.62e-02\n",
      "  time: 9275s (wall 1538s)\n",
      "step 4400 / 5440 (epoch 32.35 / 40):\n",
      "  learning_rate = 3.87e-03, loss_average = 1.10e-02\n",
      "  validation loss: 2.26e-02\n",
      "  time: 10154s (wall 1643s)\n",
      "step 4800 / 5440 (epoch 35.29 / 40):\n",
      "  learning_rate = 3.32e-03, loss_average = 1.12e-02\n",
      "  validation loss: 1.58e-02\n",
      "  time: 11035s (wall 1748s)\n",
      "step 5200 / 5440 (epoch 38.24 / 40):\n",
      "  learning_rate = 2.85e-03, loss_average = 1.20e-02\n",
      "  validation loss: 1.57e-02\n",
      "  time: 11914s (wall 1854s)\n",
      "step 5440 / 5440 (epoch 40.00 / 40):\n",
      "  learning_rate = 2.71e-03, loss_average = 1.39e-02\n",
      "  validation loss: 1.80e-02\n",
      "  time: 12475s (wall 1923s)\n",
      "INFO:tensorflow:Restoring parameters from /home/bmllab/bml_ksh/GraphCNN/GraphCNN/lib/../checkpoints/model-5440\n",
      "[0.7094335  0.5686335  0.57862782 0.57499683 0.6743964  0.67652911\n",
      " 0.63894016 0.52895206 0.56317574 0.56601244 0.58521873 0.67566693\n",
      " 0.71687543 0.5714857  0.66309208 0.63697946 0.56481284 0.66006947\n",
      " 0.52950436 0.6931113  0.6409561  0.5428077  0.68364912 0.67802995\n",
      " 0.60188454 0.68617845 0.66171885 0.5837822  0.7152878  0.54124129\n",
      " 0.52317113 0.56685334 0.69006157 0.61229908 0.54737979 0.74002206\n",
      " 0.58991325 0.63422972 0.56278145 0.54947186 0.57410508 0.60654169\n",
      " 0.62027043 0.6836338  0.60608542 0.6755985  0.54046649 0.58988696\n",
      " 0.52052414 0.55977184 0.66220468 0.55938607 0.72397697 0.55398786\n",
      " 0.59458315 0.56302428 0.53416473 0.61208946 0.56321925 0.58167273\n",
      " 0.57574564 0.70572507 0.5202772  0.52434421 0.55865782 0.72162914\n",
      " 0.53963178 0.73194194 0.62518942 0.66767645 0.55618429 0.66287166\n",
      " 0.66575611 0.59130681 0.72526336 0.65829074 0.7337296  0.56742692\n",
      " 0.65897316 0.69982433 0.70019245 0.50726503 0.58140117 0.68837166\n",
      " 0.65567338 0.66414106 0.54953253 0.70348024 0.63276488 0.6049065\n",
      " 0.63107556 0.68253154 0.70920646 0.65244216 0.61046779 0.64312702\n",
      " 0.65896308 0.61121011 0.58897108 0.63381118 0.69641578 0.64596558\n",
      " 0.57847488 0.63712007 0.56148654 0.5768168  0.67326379 0.6841169\n",
      " 0.5862776  0.54373473 0.59337711 0.51226377 0.54612392 0.50929815\n",
      " 0.64398926 0.57218575 0.70081401 0.71317071 0.54003745 0.56904936\n",
      " 0.72481722 0.56839943 0.62402499 0.71839994 0.56188482 0.57591408\n",
      " 0.62234211 0.62760544 0.5871101  0.61977446 0.62343824 0.49370623\n",
      " 0.50365108 0.53336984 0.54011178 0.63164032 0.56209558 0.54373074\n",
      " 0.67693943 0.5561527  0.62745893 0.69274104 0.69382083 0.61161649\n",
      " 0.66564739 0.53452981 0.56891912 0.52189791 0.57857746 0.6230576\n",
      " 0.5836792  0.54892462 0.71302384 0.54851818 0.71333039 0.64820564\n",
      " 0.54744643 0.56936783 0.52139246 0.62905788 0.67002654 0.60641199\n",
      " 0.68625164 0.58066541 0.53729212 0.5168286  0.63600504 0.5882569\n",
      " 0.53239483 0.52269483 0.68687302 0.55638516 0.62092257 0.5800395\n",
      " 0.65257818 0.55786663 0.70025706 0.56667489 0.58989221 0.543396\n",
      " 0.70001215 0.58479488 0.58455884 0.54802918]\n",
      "[6.96887076e-01 5.82641601e-01 6.65611505e-01 5.48171575e-01\n",
      " 5.89904155e-01 3.70269647e-01 2.19355012e-01 6.06715407e-01\n",
      " 5.61928022e-01 4.13858522e-01 5.54396972e-01 6.86245635e-01\n",
      " 5.13055745e-01 5.24407413e-01 6.19979546e-01 6.34816556e-01\n",
      " 6.25531051e-01 7.96028222e-01 5.70988537e-01 7.00968923e-01\n",
      " 6.41921096e-01 6.19052183e-01 4.98902741e-01 5.65623329e-01\n",
      " 5.15659292e-01 3.19759485e-01 6.64282676e-01 4.27185022e-01\n",
      " 7.40988293e-01 3.31443287e-01 3.29264187e-01 4.08415732e-01\n",
      " 7.19246887e-01 5.16158377e-01 5.13561409e-01 7.87862268e-01\n",
      " 7.55972417e-01            nan 6.34464880e-01 5.11917435e-01\n",
      " 5.57564028e-01 5.45880323e-01 4.38171167e-01 5.40865516e-01\n",
      " 5.88681191e-01 5.75163779e-01 6.89004742e-01 5.95657866e-01\n",
      " 5.64046529e-01 5.44784734e-01 6.63580795e-01 5.76429462e-01\n",
      " 6.76457804e-01 5.91477714e-01 4.97329279e-01 6.96416569e-01\n",
      " 5.33590883e-01 1.31455052e-02 2.48987395e-01 4.51116822e-01\n",
      " 5.72809380e-01 6.84546568e-01 5.60138408e-01 4.09110247e-01\n",
      " 5.10165238e-01 7.49834522e-01 4.56431159e-01 7.88366460e-01\n",
      " 5.65858435e-01 6.34355380e-01 5.69182719e-01 5.85093813e-01\n",
      " 5.09816115e-01 7.48966575e-01            nan 3.79408727e-01\n",
      " 7.33645112e-01 6.23166242e-01 8.32059827e-01 7.09484491e-01\n",
      " 5.55093746e-01 5.16990283e-01 6.11710082e-01 7.90889576e-01\n",
      " 7.09614124e-01 6.34917905e-01 3.72120738e-01 6.11076750e-01\n",
      " 6.79875777e-01 4.10605634e-01 5.83508959e-01 5.76807556e-01\n",
      " 7.74897856e-01 7.81396070e-01 3.80349838e-01 8.24019086e-01\n",
      " 5.90705813e-01 5.37536420e-01 6.30218831e-01 5.50332409e-01\n",
      " 6.29804695e-01 5.48284021e-01 5.83490202e-01 5.48220776e-01\n",
      " 5.84584220e-01 4.78711914e-01 6.45099053e-01 6.95142244e-01\n",
      " 4.70046974e-01 6.32865393e-01 6.76077746e-01 4.48339154e-01\n",
      " 3.39951294e-01 5.86987721e-01 5.38065261e-01 3.43639334e-01\n",
      " 6.68987544e-01 5.88914333e-01 5.01498923e-01 7.20859533e-01\n",
      " 3.90813329e-01 5.58284960e-01 5.13493254e-01 7.18764105e-01\n",
      " 4.26939801e-01 6.21621948e-01 6.25012914e-01 6.38962140e-01\n",
      " 5.64091802e-01 3.31269953e-01 6.70009184e-01 2.06724599e-01\n",
      " 6.17026285e-01 6.61936527e-01 5.23306324e-01 4.43992346e-01\n",
      " 5.46881930e-01 4.99711470e-01 5.32113074e-01 4.84781261e-01\n",
      " 4.11488607e-01 6.50973657e-01 5.56524023e-01 7.32876942e-01\n",
      " 7.39032613e-01 6.39571215e-01 5.32394828e-01 2.16347346e-01\n",
      " 3.21642296e-01 5.13299984e-01 5.87879533e-01 5.99036262e-01\n",
      " 6.68863902e-01 5.05251976e-01 6.41900277e-01 6.54966824e-01\n",
      " 5.71648091e-01 5.82801677e-01 4.56798647e-01 4.23270027e-01\n",
      " 6.32477478e-01 6.57015015e-01 6.91681355e-01 6.36133110e-01\n",
      " 5.26049325e-01 2.20557058e-01 7.27900926e-01 5.06950356e-01\n",
      " 3.06586875e-01 5.28079642e-01 5.81019036e-01 6.32481505e-01\n",
      " 6.44742171e-01 4.88691935e-01 6.86005030e-01 5.73077778e-01\n",
      " 6.24176786e-01 6.20729056e-01 6.53688963e-01 5.72706951e-01\n",
      " 8.61327078e-01 4.51408691e-01 7.76445587e-01 7.08951428e-04]\n",
      "--------------------------------------\n",
      "---0_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---1_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---2_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---3_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---4_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---5_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---6_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---7_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---8_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---9_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---10_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---11_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---12_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---13_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---14_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---15_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---16_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---17_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---18_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---19_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---20_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---21_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---22_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---23_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---24_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---25_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---26_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---27_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---28_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---29_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---30_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---31_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---32_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---33_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---34_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---35_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---36_th KEGG GCN ---\n",
      "  input: M_0 = 720\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 720 * 40 / 2 = 14400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 14400 * 1 = 14400\n",
      "    biases: M_2 = 1\n",
      "---37_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---38_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---39_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---40_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---41_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---42_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---43_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---44_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---45_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---46_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---47_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---48_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---49_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---50_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---51_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---52_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---53_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---54_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---55_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---56_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---57_th KEGG GCN ---\n",
      "  input: M_0 = 160\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 160 * 40 / 2 = 3200\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3200 * 1 = 3200\n",
      "    biases: M_2 = 1\n",
      "---58_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---59_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---60_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---61_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---62_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---63_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---64_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---65_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---66_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---67_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---68_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---69_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---70_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---71_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---72_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---73_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---74_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---75_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---76_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---77_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---78_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---79_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---80_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---81_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---82_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---83_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---84_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---85_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---86_th KEGG GCN ---\n",
      "  input: M_0 = 192\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 192 * 40 / 2 = 3840\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3840 * 1 = 3840\n",
      "    biases: M_2 = 1\n",
      "---87_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---88_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---89_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---90_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---91_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---92_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---93_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---94_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---95_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---96_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---97_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---98_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---99_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---100_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---101_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---102_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---103_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---104_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---105_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---106_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---107_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---108_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---109_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---110_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---111_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---112_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---113_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---114_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---115_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---116_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---117_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---118_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---119_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---120_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---121_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---122_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---123_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---124_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---125_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---126_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---127_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---128_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---129_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---130_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---131_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---132_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---133_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---134_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---135_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---136_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---137_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---138_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---139_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---140_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---141_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---142_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---143_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---144_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---145_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---146_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---147_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---148_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---149_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---150_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---151_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---152_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---153_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---154_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---155_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---156_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---157_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---158_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---159_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---160_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---161_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---162_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---163_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---164_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---165_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---166_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---167_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---168_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---169_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---170_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---171_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---172_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---173_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---174_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---175_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---176_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---177_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---178_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---179_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---180_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---181_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---182_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---183_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---184_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---185_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---186_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---187_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---188_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---189_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---190_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---191_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---192_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---193_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---194_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---195_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---196_th KEGG GCN ---\n",
      "  input: M_0 = 176\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 176 * 40 / 2 = 3520\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3520 * 1 = 3520\n",
      "    biases: M_2 = 1\n",
      "---197_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---198_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---199_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---200_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---201_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---202_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---203_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---204_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---205_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---206_th KEGG GCN ---\n",
      "  input: M_0 = 144\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 144 * 40 / 2 = 2880\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2880 * 1 = 2880\n",
      "    biases: M_2 = 1\n",
      "---207_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---208_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---209_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---210_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---211_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---212_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---213_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---214_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---215_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---216_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---217_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---218_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---219_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---220_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---221_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---222_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---223_th KEGG GCN ---\n",
      "  input: M_0 = 208\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 208 * 40 / 2 = 4160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 4160 * 1 = 4160\n",
      "    biases: M_2 = 1\n",
      "---224_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---225_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---226_th KEGG GCN ---\n",
      "  input: M_0 = 784\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 784 * 40 / 2 = 15680\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 15680 * 1 = 15680\n",
      "    biases: M_2 = 1\n",
      "---227_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---228_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---229_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---230_th KEGG GCN ---\n",
      "  input: M_0 = 320\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 320 * 40 / 2 = 6400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 6400 * 1 = 6400\n",
      "    biases: M_2 = 1\n",
      "---231_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---232_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---233_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---234_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---235_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---236_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---237_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---238_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---239_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---240_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---241_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---242_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---243_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---244_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---245_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---246_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---247_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---248_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---249_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---250_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---251_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---252_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---253_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---254_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---255_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---256_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---257_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---258_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---259_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---260_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---261_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---262_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---263_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---264_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---265_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---266_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---267_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---268_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "--------------------------------------\n",
      "concat representation: M_0=269\n",
      "shared FCNN architecture\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 1\n",
      "    weights: M_0 * M_1 = 269 * 1 = 269\n",
      "    biases: M_1 = 1\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400 / 5270 (epoch 3.04 / 40):\n",
      "  learning_rate = 1.71e-02, loss_average = 3.17e-02\n",
      "  validation loss: 3.59e-02\n",
      "  time: 1196s (wall 437s)\n",
      "step 800 / 5270 (epoch 6.07 / 40):\n",
      "  learning_rate = 1.47e-02, loss_average = 3.12e-02\n",
      "  validation loss: 4.43e-02\n",
      "  time: 2243s (wall 724s)\n",
      "step 1200 / 5270 (epoch 9.11 / 40):\n",
      "  learning_rate = 1.26e-02, loss_average = 3.48e-02\n",
      "  validation loss: 3.39e-02\n",
      "  time: 3115s (wall 840s)\n",
      "step 1600 / 5270 (epoch 12.14 / 40):\n",
      "  learning_rate = 1.08e-02, loss_average = 4.49e-02\n",
      "  validation loss: 3.49e-02\n",
      "  time: 3989s (wall 956s)\n",
      "step 2000 / 5270 (epoch 15.18 / 40):\n",
      "  learning_rate = 9.27e-03, loss_average = 4.47e-02\n",
      "  validation loss: 3.37e-02\n",
      "  time: 4950s (wall 1093s)\n",
      "step 2400 / 5270 (epoch 18.22 / 40):\n",
      "  learning_rate = 7.94e-03, loss_average = 3.65e-02\n",
      "  validation loss: 3.21e-02\n",
      "  time: 5910s (wall 1230s)\n",
      "step 2800 / 5270 (epoch 21.25 / 40):\n",
      "  learning_rate = 6.81e-03, loss_average = 3.91e-02\n",
      "  validation loss: 3.15e-02\n",
      "  time: 6875s (wall 1368s)\n",
      "step 3200 / 5270 (epoch 24.29 / 40):\n",
      "  learning_rate = 5.84e-03, loss_average = 3.82e-02\n",
      "  validation loss: 3.43e-02\n",
      "  time: 7836s (wall 1507s)\n",
      "step 3600 / 5270 (epoch 27.32 / 40):\n",
      "  learning_rate = 5.01e-03, loss_average = 2.62e-02\n",
      "  validation loss: 3.10e-02\n",
      "  time: 8797s (wall 1645s)\n",
      "step 4000 / 5270 (epoch 30.36 / 40):\n",
      "  learning_rate = 4.29e-03, loss_average = 3.05e-02\n",
      "  validation loss: 3.57e-02\n",
      "  time: 9756s (wall 1783s)\n",
      "step 4400 / 5270 (epoch 33.40 / 40):\n",
      "  learning_rate = 3.68e-03, loss_average = 2.89e-02\n",
      "  validation loss: 3.03e-02\n",
      "  time: 10718s (wall 1921s)\n",
      "step 4800 / 5270 (epoch 36.43 / 40):\n",
      "  learning_rate = 3.16e-03, loss_average = 2.68e-02\n",
      "  validation loss: 3.14e-02\n",
      "  time: 11679s (wall 2059s)\n",
      "step 5200 / 5270 (epoch 39.47 / 40):\n",
      "  learning_rate = 2.71e-03, loss_average = 2.39e-02\n",
      "  validation loss: 2.94e-02\n",
      "  time: 12639s (wall 2197s)\n",
      "step 5270 / 5270 (epoch 40.00 / 40):\n",
      "  learning_rate = 2.71e-03, loss_average = 3.17e-02\n",
      "  validation loss: 3.64e-02\n",
      "  time: 12880s (wall 2235s)\n",
      "INFO:tensorflow:Restoring parameters from /home/bmllab/bml_ksh/GraphCNN/GraphCNN/lib/../checkpoints/model-5270\n",
      "[0.4991276  0.45450482 0.4317168  0.4617022  0.50651318 0.45668215\n",
      " 0.55236232 0.48182955 0.44121996 0.49351668 0.51491863 0.43840382\n",
      " 0.49632317 0.50114793 0.48957956 0.54675853 0.48661873 0.52510548\n",
      " 0.49671161 0.49653888 0.50765181 0.44622374 0.56330383 0.45169488\n",
      " 0.45177543 0.45891297 0.55874407 0.53209323 0.48194873 0.50566268\n",
      " 0.50585544 0.44187462 0.48716527 0.49338177 0.41274595 0.50584126\n",
      " 0.54570407 0.48194414 0.43194386 0.52833146 0.48265731 0.47430375\n",
      " 0.50897616 0.48931131 0.42965323 0.49411947 0.52961755 0.43494579\n",
      " 0.53272337 0.48425725 0.48438922 0.49680522 0.50474292 0.51085663\n",
      " 0.5051595  0.48836985 0.46272141 0.439174   0.42116117 0.48644829\n",
      " 0.412182   0.53972262 0.42357635 0.41653574 0.49810246 0.52549458\n",
      " 0.41852328 0.52603137 0.51304954 0.50021189 0.43238759 0.55844104\n",
      " 0.50612456 0.54434288 0.51415217 0.47861204 0.46808183 0.40992722\n",
      " 0.54860914 0.49460244 0.51304477 0.39998871 0.40144548 0.48483929\n",
      " 0.46487954 0.46181485 0.481861   0.54885614 0.47116593 0.48156098\n",
      " 0.55196607 0.51084375 0.46762991 0.48884577 0.45153072 0.48747373\n",
      " 0.48071161 0.52013874 0.51437223 0.47720569 0.49537113 0.49999869\n",
      " 0.42454308 0.46863994 0.42741546 0.41275337 0.51387197 0.44172633\n",
      " 0.42703703 0.39827529 0.47362605 0.41854742 0.47082046 0.48318815\n",
      " 0.49031121 0.43902954 0.47709006 0.55480444 0.43727148 0.44558787\n",
      " 0.52360052 0.54636902 0.49154115 0.47585094 0.4232325  0.51598567\n",
      " 0.47373545 0.5229705  0.41743007 0.47722459 0.58014381 0.39672071\n",
      " 0.41485336 0.49711454 0.42140907 0.48030844 0.46719316 0.44313061\n",
      " 0.44399923 0.39939201 0.47617924 0.5295096  0.45985627 0.49480316\n",
      " 0.48751205 0.46540877 0.42398429 0.4520452  0.44370282 0.45772475\n",
      " 0.52395332 0.44348368 0.46132913 0.5259676  0.49810618 0.5526439\n",
      " 0.46649539 0.44683206 0.49148703 0.52369732 0.4890914  0.54577661\n",
      " 0.47428069 0.41391516 0.4121846  0.51010925 0.48344138 0.52470696\n",
      " 0.43542776 0.48969549 0.54240108 0.41184068 0.52045727 0.55387014\n",
      " 0.50382817 0.39126208 0.5078429  0.43017179 0.49731973 0.48200774\n",
      " 0.52090245 0.41697118 0.48972094 0.47184485]\n",
      "[       nan 0.48862339 0.7156914  0.34407244 0.46785668 0.28033343\n",
      " 0.51131626 0.67992021 0.60424365 0.20340395 0.49272027 0.22107923\n",
      " 0.36379391 0.4895377  0.35698488 0.21315652 0.31056636 0.67343225\n",
      " 0.31523368        nan 0.54600805 0.50713162 0.83977165 0.17750435\n",
      " 0.51091889 0.49613569 0.49279714 0.73518047 0.21747738 0.03785188\n",
      "        nan 0.47787222 0.26111564 0.15646838 0.35478484 0.73384046\n",
      " 0.55322772 0.18389387 0.40029926 0.27178837 0.47986795 0.20351813\n",
      " 0.46209806 0.39433622 0.48867486 0.14411683 0.2978752  0.4910701\n",
      " 0.69009207 0.46137827 0.51163003 0.31797651 0.39657908 0.33195385\n",
      " 0.25805916 0.45768505 0.35389985        nan 0.19681451 0.45294963\n",
      " 0.20448032 0.3832369  0.20919764 0.16041771 0.30980005 0.25343545\n",
      " 0.60971784 0.53178146 0.30197073 0.47096812 0.42582645 0.57038335\n",
      " 0.31391671 0.54476582 0.36119919 0.39687991 0.35440758 0.38582251\n",
      " 0.59659616 0.96987315 0.34096528 0.29405443 0.12675059 0.19204897\n",
      " 0.95867517 0.39693678 0.28833705 0.23130413 0.67374353 0.49872186\n",
      " 0.58286077 0.62198905 0.19983986 0.41564763 0.20274676 0.26958327\n",
      " 0.3970223  0.72579841 0.38204063 0.50310092 0.40536081 0.51191738\n",
      " 0.50665176 0.83447051 0.586092   0.36157106 0.3103098  0.39624217\n",
      " 0.49170751 0.25602387 0.14710757 0.4048496  0.2103656  0.48579841\n",
      " 0.41666263 0.4252123  0.15161811 0.89212758 0.26061533 0.52139631\n",
      "        nan 0.49994802 0.07750126        nan 0.36295995 0.81557121\n",
      " 0.15263896 0.70371294 0.         0.25462374 0.5994181  0.24554488\n",
      " 0.42599378 0.45640764 0.43459267 0.26428203 0.55590728 0.40836334\n",
      " 0.14912207 0.40474508 0.15262604 0.34542379 0.38001298 0.17502932\n",
      " 0.26296371 0.46390196 0.16621376 0.40306052 0.2581058  0.06649735\n",
      " 0.51261289 0.36496422 0.78097236 0.47139785 0.55173982 0.53194845\n",
      " 0.29593385 0.21282298 0.40599103 0.3150069  0.3138494  0.66764788\n",
      " 0.23933472 0.43602202 0.44803149 0.41206207 0.42062174 0.29340365\n",
      " 0.37395509 0.59437218 0.80136079 0.3111949  0.50799188 0.16008136\n",
      " 0.2297551  0.79358103 0.23718491 0.60485555 0.47124322 0.6451075\n",
      " 0.55767162 0.22207502 0.53186293 0.05358435]\n",
      "--------------------------------------\n",
      "---0_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---1_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---2_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---3_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---4_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---5_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---6_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---7_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---8_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---9_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---10_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---11_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---12_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---13_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---14_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---15_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---16_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---17_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---18_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---19_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---20_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---21_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---22_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---23_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---24_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---25_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---26_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---27_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---28_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---29_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---30_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---31_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---32_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---33_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---34_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---35_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---36_th KEGG GCN ---\n",
      "  input: M_0 = 720\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 720 * 40 / 2 = 14400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 14400 * 1 = 14400\n",
      "    biases: M_2 = 1\n",
      "---37_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---38_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---39_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---40_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---41_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---42_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---43_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---44_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---45_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---46_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---47_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---48_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---49_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---50_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---51_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---52_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---53_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---54_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---55_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---56_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---57_th KEGG GCN ---\n",
      "  input: M_0 = 160\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 160 * 40 / 2 = 3200\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3200 * 1 = 3200\n",
      "    biases: M_2 = 1\n",
      "---58_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---59_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---60_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---61_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---62_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---63_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---64_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---65_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---66_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---67_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---68_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---69_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---70_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---71_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---72_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---73_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---74_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---75_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---76_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---77_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---78_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---79_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---80_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---81_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---82_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---83_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---84_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---85_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---86_th KEGG GCN ---\n",
      "  input: M_0 = 192\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 192 * 40 / 2 = 3840\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3840 * 1 = 3840\n",
      "    biases: M_2 = 1\n",
      "---87_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---88_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---89_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---90_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---91_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---92_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---93_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---94_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---95_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---96_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---97_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---98_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---99_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---100_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---101_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---102_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---103_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---104_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---105_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---106_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---107_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---108_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---109_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---110_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---111_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---112_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---113_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---114_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---115_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---116_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---117_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---118_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---119_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---120_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---121_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---122_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---123_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---124_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---125_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---126_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---127_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---128_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---129_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---130_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---131_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---132_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---133_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---134_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---135_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---136_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---137_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---138_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---139_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---140_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---141_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---142_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---143_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---144_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---145_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---146_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---147_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---148_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---149_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---150_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---151_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---152_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---153_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---154_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---155_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---156_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---157_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---158_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---159_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---160_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---161_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---162_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---163_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---164_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---165_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---166_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---167_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---168_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---169_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---170_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---171_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---172_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---173_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---174_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---175_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---176_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---177_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---178_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---179_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---180_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---181_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---182_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---183_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---184_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---185_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---186_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---187_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---188_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---189_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---190_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---191_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---192_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---193_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---194_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---195_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---196_th KEGG GCN ---\n",
      "  input: M_0 = 176\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 176 * 40 / 2 = 3520\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3520 * 1 = 3520\n",
      "    biases: M_2 = 1\n",
      "---197_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---198_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---199_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---200_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---201_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---202_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---203_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---204_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---205_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---206_th KEGG GCN ---\n",
      "  input: M_0 = 144\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 144 * 40 / 2 = 2880\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2880 * 1 = 2880\n",
      "    biases: M_2 = 1\n",
      "---207_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---208_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---209_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---210_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---211_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---212_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---213_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---214_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---215_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---216_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---217_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---218_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---219_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---220_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---221_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---222_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---223_th KEGG GCN ---\n",
      "  input: M_0 = 208\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 208 * 40 / 2 = 4160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 4160 * 1 = 4160\n",
      "    biases: M_2 = 1\n",
      "---224_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---225_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---226_th KEGG GCN ---\n",
      "  input: M_0 = 784\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 784 * 40 / 2 = 15680\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 15680 * 1 = 15680\n",
      "    biases: M_2 = 1\n",
      "---227_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---228_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---229_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---230_th KEGG GCN ---\n",
      "  input: M_0 = 320\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 320 * 40 / 2 = 6400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 6400 * 1 = 6400\n",
      "    biases: M_2 = 1\n",
      "---231_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---232_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---233_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---234_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---235_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---236_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---237_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---238_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---239_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---240_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---241_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---242_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---243_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---244_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---245_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---246_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---247_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---248_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---249_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---250_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---251_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---252_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---253_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---254_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---255_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---256_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---257_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---258_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---259_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---260_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---261_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---262_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---263_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---264_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---265_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---266_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---267_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---268_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "--------------------------------------\n",
      "concat representation: M_0=269\n",
      "shared FCNN architecture\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 1\n",
      "    weights: M_0 * M_1 = 269 * 1 = 269\n",
      "    biases: M_1 = 1\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400 / 5280 (epoch 3.03 / 40):\n",
      "  learning_rate = 1.71e-02, loss_average = 5.64e-02\n",
      "  validation loss: 4.67e-02\n",
      "  time: 1280s (wall 450s)\n",
      "step 800 / 5280 (epoch 6.06 / 40):\n",
      "  learning_rate = 1.47e-02, loss_average = 4.62e-02\n",
      "  validation loss: 4.96e-02\n",
      "  time: 2439s (wall 763s)\n",
      "step 1200 / 5280 (epoch 9.09 / 40):\n",
      "  learning_rate = 1.26e-02, loss_average = 4.32e-02\n",
      "  validation loss: 3.34e-02\n",
      "  time: 3423s (wall 898s)\n",
      "step 1600 / 5280 (epoch 12.12 / 40):\n",
      "  learning_rate = 1.08e-02, loss_average = 4.25e-02\n",
      "  validation loss: 3.14e-02\n",
      "  time: 4408s (wall 1033s)\n",
      "step 2000 / 5280 (epoch 15.15 / 40):\n",
      "  learning_rate = 9.27e-03, loss_average = 4.61e-02\n",
      "  validation loss: 3.20e-02\n",
      "  time: 5391s (wall 1167s)\n",
      "step 2400 / 5280 (epoch 18.18 / 40):\n",
      "  learning_rate = 7.94e-03, loss_average = 4.25e-02\n",
      "  validation loss: 3.12e-02\n",
      "  time: 6377s (wall 1302s)\n",
      "step 2800 / 5280 (epoch 21.21 / 40):\n",
      "  learning_rate = 6.81e-03, loss_average = 2.91e-02\n",
      "  validation loss: 2.99e-02\n",
      "  time: 7360s (wall 1436s)\n",
      "step 3200 / 5280 (epoch 24.24 / 40):\n",
      "  learning_rate = 5.84e-03, loss_average = 3.42e-02\n",
      "  validation loss: 3.25e-02\n",
      "  time: 8342s (wall 1570s)\n",
      "step 3600 / 5280 (epoch 27.27 / 40):\n",
      "  learning_rate = 5.01e-03, loss_average = 2.45e-02\n",
      "  validation loss: 2.91e-02\n",
      "  time: 9323s (wall 1703s)\n",
      "step 4000 / 5280 (epoch 30.30 / 40):\n",
      "  learning_rate = 4.29e-03, loss_average = 3.18e-02\n",
      "  validation loss: 3.07e-02\n",
      "  time: 10307s (wall 1837s)\n",
      "step 4400 / 5280 (epoch 33.33 / 40):\n",
      "  learning_rate = 3.68e-03, loss_average = 3.51e-02\n",
      "  validation loss: 3.49e-02\n",
      "  time: 11287s (wall 1970s)\n",
      "step 4800 / 5280 (epoch 36.36 / 40):\n",
      "  learning_rate = 3.16e-03, loss_average = 2.38e-02\n",
      "  validation loss: 2.82e-02\n",
      "  time: 12269s (wall 2103s)\n",
      "step 5200 / 5280 (epoch 39.39 / 40):\n",
      "  learning_rate = 2.71e-03, loss_average = 3.16e-02\n",
      "  validation loss: 2.79e-02\n",
      "  time: 13249s (wall 2236s)\n",
      "step 5280 / 5280 (epoch 40.00 / 40):\n",
      "  learning_rate = 2.71e-03, loss_average = 3.46e-02\n",
      "  validation loss: 2.84e-02\n",
      "  time: 13516s (wall 2276s)\n",
      "INFO:tensorflow:Restoring parameters from /home/bmllab/bml_ksh/GraphCNN/GraphCNN/lib/../checkpoints/model-5280\n",
      "[0.47314882 0.37035298 0.38491338 0.41033041 0.4642818  0.42963198\n",
      " 0.49512911 0.44508189 0.38605908 0.44360295 0.45051718 0.41055807\n",
      " 0.47215289 0.4748936  0.45548934 0.48853999 0.44167128 0.45492545\n",
      " 0.45244923 0.45410731 0.4506219  0.35389227 0.51425707 0.40776449\n",
      " 0.41397139 0.41213006 0.50031024 0.46818358 0.44983241 0.44677201\n",
      " 0.46013772 0.3693381  0.45879933 0.45176828 0.36616656 0.49341932\n",
      " 0.47896838 0.4544276  0.35064223 0.4474172  0.41569871 0.43391743\n",
      " 0.46859843 0.43153211 0.39421135 0.45882162 0.43555358 0.38855302\n",
      " 0.42461616 0.42652288 0.44456363 0.43346038 0.48356539 0.41049916\n",
      " 0.45897666 0.43800166 0.4380165  0.43659171 0.35577598 0.44298956\n",
      " 0.41633281 0.4801116  0.32946211 0.33597881 0.41418076 0.48336086\n",
      " 0.32560447 0.50787276 0.47225133 0.49191231 0.34222263 0.5259003\n",
      " 0.45794976 0.47957912 0.48523298 0.41913721 0.44271013 0.33936968\n",
      " 0.50354052 0.44373745 0.46757111 0.30134201 0.35578766 0.45231584\n",
      " 0.43266994 0.42110106 0.43613905 0.48777929 0.45224336 0.43816432\n",
      " 0.49882272 0.49075925 0.43687764 0.44938016 0.42886579 0.43478608\n",
      " 0.46125904 0.47845817 0.46277082 0.46246392 0.47391906 0.46978891\n",
      " 0.34055799 0.45843577 0.38146368 0.37599689 0.44307062 0.410557\n",
      " 0.39228284 0.3160013  0.4113279  0.38376912 0.42699671 0.44594997\n",
      " 0.43493748 0.38571221 0.42655221 0.51659167 0.36007467 0.40509251\n",
      " 0.47721943 0.4667505  0.42912325 0.42705983 0.37474504 0.46976012\n",
      " 0.45935929 0.43198198 0.36350071 0.42832091 0.50742829 0.30166104\n",
      " 0.33012122 0.41859823 0.34958237 0.4273417  0.36353096 0.34880418\n",
      " 0.41348624 0.34884879 0.42775857 0.47397423 0.42681029 0.45576033\n",
      " 0.45995069 0.40668017 0.34048066 0.3508437  0.39088973 0.43278715\n",
      " 0.45729408 0.33925909 0.4099786  0.44283512 0.46018028 0.50044233\n",
      " 0.35450238 0.38262272 0.44173199 0.46901023 0.46735764 0.4782151\n",
      " 0.45145723 0.37257016 0.35915101 0.43264619 0.4475854  0.45685098\n",
      " 0.34731227 0.45186704 0.49749798 0.3617537  0.47662425 0.45881066\n",
      " 0.4431853  0.30986288 0.49127743 0.38840121 0.4657205  0.43188\n",
      " 0.47418523 0.40636837 0.46862495 0.44289082]\n",
      "[       nan 0.52312537 0.78433832 0.3266197  0.69264454 0.43933311\n",
      " 0.57590639 0.63010473 0.70199029 0.07057461 0.38745127 0.37511579\n",
      " 0.41441212 0.54217481 0.39343808 0.32936773 0.38811756 0.68544033\n",
      " 0.46779231        nan 0.55121457 0.43946737 0.7042747  0.090986\n",
      " 0.56838562 0.53608154 0.56633968 0.80117157 0.40316615 0.20427274\n",
      " 0.57296137 0.4674554  0.33448775 0.31187935 0.38357723 0.65422821\n",
      " 0.49987915 0.22159448 0.4399674  0.49918439 0.45317115 0.39669848\n",
      " 0.57451089 0.51541446 0.42488965 0.23245051 0.25324509 0.44120036\n",
      " 0.49161909 0.34249838 0.90761195 0.24366556 0.52284159 0.4052931\n",
      " 0.36946129 0.55134893 0.42047066 0.68981294 0.19226957 0.47181176\n",
      " 0.34034569 0.48568287 0.22915449 0.1542375  0.57763302 0.24271129\n",
      " 0.60371309 0.54066483 0.27215255 0.92835946 0.42423061 0.73077456\n",
      " 0.26689632 0.27698869 0.65529433 0.36529789 0.55455668 0.43066999\n",
      " 0.91587181 0.82067369 0.41296026 0.27837714 0.12550813 0.28481682\n",
      " 0.87588218 0.47151145 0.26465223 0.29042122 0.45828383 0.54894759\n",
      " 0.68794579 0.65474067 0.35549203 0.58242523 0.18529756 0.4489188\n",
      " 0.51064153 0.60646406 0.43794711 0.65202717 0.37327564 0.64621728\n",
      " 0.60800184 0.82222507 0.64307157 0.34606291 0.53623488 0.46722877\n",
      " 0.52967621 0.21731708 0.26112175 0.48654213 0.17691235 0.54338184\n",
      " 0.37558265 0.57908537 0.26124808 0.80249211 0.2527566  0.86818322\n",
      "        nan 0.45544958 0.48405712        nan 0.22714701 0.59723231\n",
      " 0.14045534 0.63468058 0.15024261 0.33739402 0.54934556 0.22465819\n",
      " 0.44696446 0.37855986 0.55844658 0.25700767 0.54403776 0.51181862\n",
      " 0.3249005  0.40929015 0.31099817 0.57448936 0.37181908 0.51717525\n",
      " 0.04470424 0.52685713 0.21257507 0.42069074 0.24338051 0.06756472\n",
      " 0.75668818 0.30152091 0.58098012 0.26163637 0.61020982 0.53866587\n",
      " 0.22564466 0.20884615 0.71130659 0.30190616 0.319483   0.66782444\n",
      " 0.28366058 0.39159862 0.58130666 0.52276252 0.67507318 0.37788721\n",
      " 0.19629939 0.79964035 0.8040521  0.26454019 0.52541124 0.16510596\n",
      " 0.29141953 0.81979535 0.40977903 0.57068705 0.40476489 0.6548488\n",
      " 0.5216279  0.22564573 0.46932881 0.25076899]\n",
      "--------------------------------------\n",
      "---0_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---1_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---2_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---3_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---4_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---5_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---6_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---7_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---8_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---9_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---10_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---11_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---12_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---13_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---14_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---15_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---16_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---17_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---18_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---19_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---20_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---21_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---22_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---23_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---24_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---25_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---26_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---27_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---28_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---29_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---30_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---31_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---32_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---33_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---34_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---35_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---36_th KEGG GCN ---\n",
      "  input: M_0 = 720\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 720 * 40 / 2 = 14400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 14400 * 1 = 14400\n",
      "    biases: M_2 = 1\n",
      "---37_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---38_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---39_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---40_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---41_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---42_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---43_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---44_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---45_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---46_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---47_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---48_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---49_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---50_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---51_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---52_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---53_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---54_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---55_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---56_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---57_th KEGG GCN ---\n",
      "  input: M_0 = 160\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 160 * 40 / 2 = 3200\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3200 * 1 = 3200\n",
      "    biases: M_2 = 1\n",
      "---58_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---59_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---60_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---61_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---62_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---63_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---64_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---65_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---66_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---67_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---68_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---69_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---70_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---71_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---72_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---73_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---74_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---75_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---76_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---77_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---78_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---79_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---80_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---81_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---82_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---83_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---84_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---85_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---86_th KEGG GCN ---\n",
      "  input: M_0 = 192\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 192 * 40 / 2 = 3840\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3840 * 1 = 3840\n",
      "    biases: M_2 = 1\n",
      "---87_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---88_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---89_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---90_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---91_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---92_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---93_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---94_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---95_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---96_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---97_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---98_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---99_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---100_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---101_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---102_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---103_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---104_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---105_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---106_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---107_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---108_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---109_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---110_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---111_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---112_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---113_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---114_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---115_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---116_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---117_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---118_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---119_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---120_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---121_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---122_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---123_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---124_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---125_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---126_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---127_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---128_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---129_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---130_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---131_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---132_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---133_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---134_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---135_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---136_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---137_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---138_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---139_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---140_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---141_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---142_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---143_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---144_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---145_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---146_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---147_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---148_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---149_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---150_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---151_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---152_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---153_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---154_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---155_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---156_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---157_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---158_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---159_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---160_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---161_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---162_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---163_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---164_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---165_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---166_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---167_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---168_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---169_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---170_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---171_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---172_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---173_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---174_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---175_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---176_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---177_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---178_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---179_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---180_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---181_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---182_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---183_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---184_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---185_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---186_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---187_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---188_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---189_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---190_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---191_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---192_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---193_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---194_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---195_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---196_th KEGG GCN ---\n",
      "  input: M_0 = 176\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 176 * 40 / 2 = 3520\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 3520 * 1 = 3520\n",
      "    biases: M_2 = 1\n",
      "---197_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---198_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---199_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---200_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---201_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---202_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---203_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---204_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---205_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---206_th KEGG GCN ---\n",
      "  input: M_0 = 144\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 144 * 40 / 2 = 2880\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2880 * 1 = 2880\n",
      "    biases: M_2 = 1\n",
      "---207_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---208_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---209_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---210_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---211_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---212_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---213_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---214_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---215_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---216_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---217_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---218_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---219_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---220_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---221_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---222_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---223_th KEGG GCN ---\n",
      "  input: M_0 = 208\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 208 * 40 / 2 = 4160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 4160 * 1 = 4160\n",
      "    biases: M_2 = 1\n",
      "---224_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---225_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---226_th KEGG GCN ---\n",
      "  input: M_0 = 784\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 784 * 40 / 2 = 15680\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 15680 * 1 = 15680\n",
      "    biases: M_2 = 1\n",
      "---227_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---228_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---229_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---230_th KEGG GCN ---\n",
      "  input: M_0 = 320\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 320 * 40 / 2 = 6400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 6400 * 1 = 6400\n",
      "    biases: M_2 = 1\n",
      "---231_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---232_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---233_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---234_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 40 / 2 = 2560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2560 * 1 = 2560\n",
      "    biases: M_2 = 1\n",
      "---235_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---236_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---237_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---238_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---239_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---240_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---241_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---242_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---243_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---244_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---245_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---246_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---247_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---248_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---249_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---250_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 40 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1600 * 1 = 1600\n",
      "    biases: M_2 = 1\n",
      "---251_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 40 / 2 = 2240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 2240 * 1 = 2240\n",
      "    biases: M_2 = 1\n",
      "---252_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "---253_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---254_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---255_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---256_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---257_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---258_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---259_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---260_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---261_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---262_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 40 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 640 * 1 = 640\n",
      "    biases: M_2 = 1\n",
      "---263_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---264_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---265_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---266_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 40 / 2 = 1280\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1280 * 1 = 1280\n",
      "    biases: M_2 = 1\n",
      "---267_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 40 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 960 * 1 = 960\n",
      "    biases: M_2 = 1\n",
      "---268_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 40 / 2 = 1920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 40 * 40 = 1600\n",
      "    biases: F_1 = 40\n",
      "  layer 2: logits (softmax)\n",
      "    representation: M_2 = 1\n",
      "    weights: M_1 * M_2 = 1920 * 1 = 1920\n",
      "    biases: M_2 = 1\n",
      "--------------------------------------\n",
      "concat representation: M_0=269\n",
      "shared FCNN architecture\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 1\n",
      "    weights: M_0 * M_1 = 269 * 1 = 269\n",
      "    biases: M_1 = 1\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400 / 5210 (epoch 3.07 / 40):\n",
      "  learning_rate = 1.71e-02, loss_average = 5.08e-02\n",
      "  validation loss: 4.97e-02\n",
      "  time: 1281s (wall 463s)\n",
      "step 800 / 5210 (epoch 6.14 / 40):\n",
      "  learning_rate = 1.47e-02, loss_average = 5.42e-02\n",
      "  validation loss: 4.63e-02\n",
      "  time: 2429s (wall 776s)\n",
      "step 1200 / 5210 (epoch 9.21 / 40):\n",
      "  learning_rate = 1.26e-02, loss_average = 5.29e-02\n",
      "  validation loss: 4.48e-02\n",
      "  time: 3400s (wall 916s)\n",
      "step 1600 / 5210 (epoch 12.28 / 40):\n",
      "  learning_rate = 1.08e-02, loss_average = 4.56e-02\n",
      "  validation loss: 6.42e-02\n",
      "  time: 4374s (wall 1056s)\n",
      "step 2000 / 5210 (epoch 15.36 / 40):\n",
      "  learning_rate = 9.27e-03, loss_average = 5.79e-02\n",
      "  validation loss: 4.36e-02\n",
      "  time: 5345s (wall 1196s)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "n_fold = 3\n",
    "Y_train, Y_test = train_test_split(data_IC50,  test_size=0.25, shuffle=True, random_state=20)\n",
    "\n",
    "\n",
    "for cv in range(n_fold):\n",
    "    Y_pred = np.zeros([Y_test.shape[0], Y_test.shape[1]])\n",
    "    Y_test = np.zeros([Y_test.shape[0], Y_test.shape[1]])\n",
    "#    print('n_fold:{}'.format(cv))\n",
    "    j = 0\n",
    "    for i in range(201):\n",
    "        label = data_IC50.iloc[:,i]\n",
    "        label = np.array(label)\n",
    "        data_minmax = label[~np.isnan(label)]\n",
    "        min = data_minmax.min()\n",
    "        max = data_minmax.max()\n",
    "        label = (label - min) / (max - min)\n",
    "#         label_indexes = np.argwhere([~np.isnan(label)])[:, 1]\n",
    "        \n",
    "        train_idx, test_idx = train_test_split(range(label.shape[0]), test_size=0.25, shuffle=True, random_state=20)\n",
    "        train_idx = np.array(train_idx)\n",
    "        \n",
    "        train_idx = train_idx[~np.isnan(label[train_idx])]        \n",
    "        list_train, list_val = Validation(n_fold,train_idx)\n",
    "\n",
    "        val_idx = train_idx[list_val[cv]]\n",
    "        train_idx = train_idx[list_train[cv]]\n",
    "        test_idx = np.array(test_idx)\n",
    "        \n",
    "        train_labels = label[train_idx]\n",
    "        val_labels = label[val_idx]\n",
    "        test_labels = label[test_idx]\n",
    "        \n",
    "        \n",
    "        common = {}\n",
    "        common['num_epochs']     = 40\n",
    "        common['batch_size']     = 4\n",
    "        common['decay_steps']    = train_idx.shape[0] / common['batch_size']\n",
    "        common['eval_frequency'] = 10 * common['num_epochs']\n",
    "        common['brelu']          = 'b1relu'\n",
    "        common['pool']           = 'mpool1'\n",
    "\n",
    "        common['regularization'] = 0\n",
    "        common['dropout']        = 1\n",
    "        common['learning_rate']  = 0.02\n",
    "        common['decay_rate']     = 0.95\n",
    "        common['momentum']       = 0.9\n",
    "        common['F']              = [40]\n",
    "        common['K']              = [40]\n",
    "        common['p']              = [2]\n",
    "        common['M']              = [1]\n",
    "        # concatnate fully connected NN\n",
    "        common['M1']             = [1]\n",
    "\n",
    "        train_dataset = []\n",
    "        val_dataset = []\n",
    "        test_dataset = []\n",
    "        for kegg_id in range(len(dataset['L'])):\n",
    "            exp = dataset['exp'][kegg_id]\n",
    "            perm = dataset['perm'][kegg_id]\n",
    "#            print('kegg_id: {}, #nodes: {}'.format(kegg_id, len(perm)))\n",
    "#            print(exp.shape)\n",
    "#            print(train_idx.shape)\n",
    "            train_data = exp[train_idx, :]\n",
    "            val_data = exp[val_idx, :]\n",
    "            test_data = exp[test_idx, :]\n",
    "            train_data = coarsening.perm_data(train_data, perm)\n",
    "            val_data = coarsening.perm_data(val_data, perm)\n",
    "            test_data = coarsening.perm_data(test_data, perm)\n",
    "            train_dataset.append(train_data)\n",
    "            val_dataset.append(val_data)\n",
    "            test_dataset.append(test_data)\n",
    "#            print(train_data.shape, val_data.shape, test_data.shape)\n",
    "        if True:\n",
    "            name = 'cgconv_softmax'\n",
    "            params = common.copy()\n",
    "       \n",
    "        model = models_update.cgcnn(dataset['L'], **params)\n",
    "        loss, t_step = model.fit(train_dataset, train_labels, val_dataset, val_labels)\n",
    "\n",
    "\n",
    "        Y_pred[:, j] = model.predict(test_dataset)\n",
    "        Y_test[:, j] = test_labels        \n",
    "#        print(Y_pred[:, j])\n",
    "#        print(Y_test[:, j])\n",
    "        j = j+1\n",
    "#        np.savez(('4040.cv_{}'.format(cv)), Y_true=Y_test, Y_pred=Y_pred)\n",
    "\n",
    "    np.savez(('4040.cv_{}'.format(cv)), Y_true=Y_test, Y_pred=Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : [0.03449682940019994, 0.016506825518722328, 0.03304470752500458, 0.0359634220979898]\n",
      "PCC : [0.3829299218333685, 0.41952147256776295, 0.23990148477324408, 0.27363753511941785]\n",
      "SCC : [0.3694496743726658, 0.4427821974239871, 0.23844460592236225, 0.2894369167772668]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "RMSE = []\n",
    "PCC = []\n",
    "SCC = []\n",
    "\n",
    "for i in range(4):\n",
    "    y_test = Y_test[:, i]\n",
    "    y_pred = Y_pred[:, i]\n",
    "    y_pred = y_pred[~np.isnan(y_test)]\n",
    "    y_test = y_test[~np.isnan(y_test)]\n",
    "\n",
    "    RMSE.append(mean_squared_error(y_test,y_pred))\n",
    "    PCC.append(stats.pearsonr(y_test,y_pred)[0])\n",
    "    SCC.append(stats.spearmanr(y_test,y_pred)[0])\n",
    "print(\"RMSE :\",RMSE)\n",
    "print(\"PCC :\",PCC)\n",
    "print(\"SCC :\",SCC)\n",
    "# 50 40 2 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : [0.0, 0.0, 0.0, 0.0]\n",
      "PCC : [nan, nan, nan, nan]\n",
      "SCC : [nan, nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/bmlserver/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats \n",
    "RMSE = []\n",
    "PCC = []\n",
    "SCC = []\n",
    "\n",
    "for i in range(4):\n",
    "    y_test = Y_test[:, i]\n",
    "    y_pred = Y_pred[:, i]\n",
    "    y_pred = y_pred[~np.isnan(y_test)]\n",
    "    y_test = y_test[~np.isnan(y_test)]\n",
    "\n",
    "    RMSE.append(mean_squared_error(y_test,y_pred))\n",
    "    PCC.append(stats.pearsonr(y_test,y_pred)[0])\n",
    "    SCC.append(stats.spearmanr(y_test,y_pred)[0])\n",
    "print(\"RMSE :\",RMSE)\n",
    "print(\"PCC :\",PCC)\n",
    "print(\"SCC :\",SCC)\n",
    "\n",
    "## 40 40 2 1 1 ( 1, 2) #따봉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "---0_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---1_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---2_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---3_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---4_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---5_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---6_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---7_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---8_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---9_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---10_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---11_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---12_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---13_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---14_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---15_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---16_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---17_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---18_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---19_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---20_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---21_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---22_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---23_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---24_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---25_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---26_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---27_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---28_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---29_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---30_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---31_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---32_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---33_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---34_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---35_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---36_th KEGG GCN ---\n",
      "  input: M_0 = 720\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 720 * 10 / 2 = 3600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 360 * 10 / 2 = 1800\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 180 * 10 / 2 = 900\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 900 * 1 = 900\n",
      "    biases: M_4 = 1\n",
      "---37_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---38_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---39_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---40_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---41_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---42_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---43_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---44_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---45_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---46_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---47_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---48_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---49_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---50_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---51_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---52_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---53_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---54_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---55_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---56_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---57_th KEGG GCN ---\n",
      "  input: M_0 = 160\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 160 * 10 / 2 = 800\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 80 * 10 / 2 = 400\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 40 * 10 / 2 = 200\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 200 * 1 = 200\n",
      "    biases: M_4 = 1\n",
      "---58_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---59_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---60_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---61_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---62_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---63_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---64_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---65_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---66_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---67_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---68_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---69_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---70_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---71_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---72_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---73_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---74_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---75_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---76_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---77_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---78_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---79_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---80_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---81_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---82_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---83_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---84_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---85_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---86_th KEGG GCN ---\n",
      "  input: M_0 = 192\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 192 * 10 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 96 * 10 / 2 = 480\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 48 * 10 / 2 = 240\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 240 * 1 = 240\n",
      "    biases: M_4 = 1\n",
      "---87_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---88_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---89_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---90_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---91_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---92_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---93_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---94_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---95_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---96_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---97_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---98_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---99_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---100_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---101_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---102_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---103_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---104_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---105_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---106_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---107_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---108_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---109_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---110_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---111_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---112_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---113_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---114_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---115_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---116_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---117_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---118_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---119_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---120_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---121_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---122_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---123_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---124_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---125_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---126_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---127_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---128_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---129_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---130_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---131_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---132_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---133_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---134_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---135_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---136_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---137_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---138_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---139_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---140_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---141_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---142_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---143_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---144_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---145_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---146_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---147_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---148_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---149_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---150_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---151_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---152_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---153_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---154_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---155_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---156_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---157_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---158_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---159_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---160_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---161_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---162_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---163_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---164_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---165_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---166_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---167_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---168_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---169_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---170_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---171_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---172_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---173_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---174_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---175_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---176_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---177_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---178_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---179_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---180_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---181_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---182_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---183_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---184_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---185_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---186_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---187_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---188_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---189_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---190_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---191_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---192_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---193_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---194_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---195_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---196_th KEGG GCN ---\n",
      "  input: M_0 = 176\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 176 * 10 / 2 = 880\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 88 * 10 / 2 = 440\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 44 * 10 / 2 = 220\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 220 * 1 = 220\n",
      "    biases: M_4 = 1\n",
      "---197_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---198_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---199_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---200_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---201_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---202_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---203_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---204_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---205_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---206_th KEGG GCN ---\n",
      "  input: M_0 = 144\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 144 * 10 / 2 = 720\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 72 * 10 / 2 = 360\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 36 * 10 / 2 = 180\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 180 * 1 = 180\n",
      "    biases: M_4 = 1\n",
      "---207_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---208_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---209_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---210_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---211_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---212_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---213_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---214_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---215_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---216_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---217_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---218_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---219_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---220_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---221_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---222_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---223_th KEGG GCN ---\n",
      "  input: M_0 = 192\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 192 * 10 / 2 = 960\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 96 * 10 / 2 = 480\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 48 * 10 / 2 = 240\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 240 * 1 = 240\n",
      "    biases: M_4 = 1\n",
      "---224_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---225_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---226_th KEGG GCN ---\n",
      "  input: M_0 = 784\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 784 * 10 / 2 = 3920\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 392 * 10 / 2 = 1960\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 196 * 10 / 2 = 980\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 980 * 1 = 980\n",
      "    biases: M_4 = 1\n",
      "---227_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---228_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---229_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---230_th KEGG GCN ---\n",
      "  input: M_0 = 320\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 320 * 10 / 2 = 1600\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 160 * 10 / 2 = 800\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 80 * 10 / 2 = 400\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 400 * 1 = 400\n",
      "    biases: M_4 = 1\n",
      "---231_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---232_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---233_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---234_th KEGG GCN ---\n",
      "  input: M_0 = 128\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 128 * 10 / 2 = 640\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 64 * 10 / 2 = 320\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 32 * 10 / 2 = 160\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 160 * 1 = 160\n",
      "    biases: M_4 = 1\n",
      "---235_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---236_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---237_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---238_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---239_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---240_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---241_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---242_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---243_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---244_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---245_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---246_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---247_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---248_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---249_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---250_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "---251_th KEGG GCN ---\n",
      "  input: M_0 = 112\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 112 * 10 / 2 = 560\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 56 * 10 / 2 = 280\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 28 * 10 / 2 = 140\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 140 * 1 = 140\n",
      "    biases: M_4 = 1\n",
      "---252_th KEGG GCN ---\n",
      "  input: M_0 = 96\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 96 * 10 / 2 = 480\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 48 * 10 / 2 = 240\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 24 * 10 / 2 = 120\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 120 * 1 = 120\n",
      "    biases: M_4 = 1\n",
      "---253_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---254_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---255_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---256_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---257_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---258_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---259_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---260_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---261_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---262_th KEGG GCN ---\n",
      "  input: M_0 = 32\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 32 * 10 / 2 = 160\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 16 * 10 / 2 = 80\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 8 * 10 / 2 = 40\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 40 * 1 = 40\n",
      "    biases: M_4 = 1\n",
      "---263_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---264_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---265_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---266_th KEGG GCN ---\n",
      "  input: M_0 = 64\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 64 * 10 / 2 = 320\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 10 / 2 = 160\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 16 * 10 / 2 = 80\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 80 * 1 = 80\n",
      "    biases: M_4 = 1\n",
      "---267_th KEGG GCN ---\n",
      "  input: M_0 = 48\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 48 * 10 / 2 = 240\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 24 * 10 / 2 = 120\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 12 * 10 / 2 = 60\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 60 * 1 = 60\n",
      "    biases: M_4 = 1\n",
      "---268_th KEGG GCN ---\n",
      "  input: M_0 = 80\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 80 * 10 / 2 = 400\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 10 * 5 = 50\n",
      "    biases: F_1 = 10\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 40 * 10 / 2 = 200\n",
      "    weights: F_1 * F_2 * K_2 = 10 * 10 * 5 = 500\n",
      "    biases: F_2 = 10\n",
      "  layer 3: cgconv3\n",
      "    representation: M_2 * F_3 / p_3 = 20 * 10 / 2 = 100\n",
      "    weights: F_2 * F_3 * K_3 = 10 * 10 * 5 = 500\n",
      "    biases: F_3 = 10\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 1\n",
      "    weights: M_3 * M_4 = 100 * 1 = 100\n",
      "    biases: M_4 = 1\n",
      "--------------------------------------\n",
      "concat representation: M_0=269\n",
      "shared FCNN architecture\n",
      "  layer 1: logits (softmax)\n",
      "    representation: M_1 = 1\n",
      "    weights: M_0 * M_1 = 269 * 1 = 269\n",
      "    biases: M_1 = 1\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400 / 5430 (epoch 2.95 / 40):\n",
      "  learning_rate = 1.80e-02, loss_average = 3.09e-02\n",
      "  validation loss: 3.34e-02\n",
      "  time: 1008s (wall 273s)\n",
      "step 800 / 5430 (epoch 5.89 / 40):\n",
      "  learning_rate = 1.55e-02, loss_average = 3.46e-02\n",
      "  validation loss: 3.51e-02\n",
      "  time: 1965s (wall 490s)\n",
      "step 1200 / 5430 (epoch 8.84 / 40):\n",
      "  learning_rate = 1.33e-02, loss_average = 3.44e-02\n",
      "  validation loss: 4.35e-02\n",
      "  time: 2809s (wall 598s)\n",
      "step 1600 / 5430 (epoch 11.79 / 40):\n",
      "  learning_rate = 1.14e-02, loss_average = 4.54e-02\n",
      "  validation loss: 3.78e-02\n",
      "  time: 3653s (wall 705s)\n",
      "step 2000 / 5430 (epoch 14.73 / 40):\n",
      "  learning_rate = 9.75e-03, loss_average = 2.97e-02\n",
      "  validation loss: 3.35e-02\n",
      "  time: 4497s (wall 813s)\n",
      "step 2400 / 5430 (epoch 17.68 / 40):\n",
      "  learning_rate = 8.36e-03, loss_average = 2.25e-02\n",
      "  validation loss: 3.46e-02\n",
      "  time: 5343s (wall 921s)\n",
      "step 2800 / 5430 (epoch 20.63 / 40):\n",
      "  learning_rate = 7.17e-03, loss_average = 3.10e-02\n",
      "  validation loss: 3.31e-02\n",
      "  time: 6187s (wall 1029s)\n",
      "step 3200 / 5430 (epoch 23.57 / 40):\n",
      "  learning_rate = 6.15e-03, loss_average = 3.62e-02\n",
      "  validation loss: 3.74e-02\n",
      "  time: 7030s (wall 1137s)\n",
      "step 3600 / 5430 (epoch 26.52 / 40):\n",
      "  learning_rate = 5.27e-03, loss_average = 3.59e-02\n",
      "  validation loss: 4.13e-02\n",
      "  time: 7869s (wall 1243s)\n",
      "step 4000 / 5430 (epoch 29.47 / 40):\n",
      "  learning_rate = 4.52e-03, loss_average = 2.88e-02\n",
      "  validation loss: 3.56e-02\n",
      "  time: 8708s (wall 1349s)\n",
      "step 4400 / 5430 (epoch 32.41 / 40):\n",
      "  learning_rate = 3.87e-03, loss_average = 3.41e-02\n",
      "  validation loss: 3.40e-02\n",
      "  time: 9548s (wall 1455s)\n",
      "step 4800 / 5430 (epoch 35.36 / 40):\n",
      "  learning_rate = 3.32e-03, loss_average = 2.94e-02\n",
      "  validation loss: 3.68e-02\n",
      "  time: 10387s (wall 1561s)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "U = [137,195,172,55]\n",
    "n_fold = 3\n",
    "C = [0,1,2,3]\n",
    "Y_train, Y_test = train_test_split(data_IC50,  test_size=0.25, shuffle=True, random_state=20)\n",
    "#print(Y_test.shape)\n",
    "\n",
    "Kernel_Num =  [[10, 10, 10], [20, 20, 20]]\n",
    "Kernel_Size = [[5, 5, 5], [10, 10, 10], [30, 30, 30]]\n",
    "for ker_num in range(len(Kernel_Num)):\n",
    "    for ker_size in range(len(Kernel_Size)):\n",
    "        for cv in range(1):\n",
    "            Y_pred = np.zeros([Y_test.shape[0], Y_test.shape[1]])\n",
    "            Y_test = np.zeros([Y_test.shape[0], Y_test.shape[1]])\n",
    "        #    print('n_fold:{}'.format(cv))\n",
    "            j = 0\n",
    "            for i in C:\n",
    "                label = data_IC50.iloc[:,i]\n",
    "                label = np.array(label)\n",
    "                data_minmax = label[~np.isnan(label)]\n",
    "                min = data_minmax.min()\n",
    "                max = data_minmax.max()\n",
    "                label = (label - min) / (max - min)\n",
    "        #         label_indexes = np.argwhere([~np.isnan(label)])[:, 1]\n",
    "\n",
    "                train_idx, test_idx = train_test_split(range(label.shape[0]), test_size=0.25, shuffle=True, random_state=20)\n",
    "                train_idx = np.array(train_idx)\n",
    "\n",
    "                train_idx = train_idx[~np.isnan(label[train_idx])]        \n",
    "                list_train, list_val = Validation(n_fold,train_idx)\n",
    "\n",
    "                val_idx = train_idx[list_val[cv]]\n",
    "                train_idx = train_idx\n",
    "                test_idx = np.array(test_idx)\n",
    "\n",
    "                train_labels = label[train_idx]\n",
    "                val_labels = label[val_idx]\n",
    "                test_labels = label[test_idx]\n",
    "\n",
    "\n",
    "                common = {}\n",
    "                common['num_epochs']     = 40\n",
    "                common['batch_size']     = 4\n",
    "                common['decay_steps']    = train_idx.shape[0] / common['batch_size']\n",
    "                common['eval_frequency'] = 10 * common['num_epochs']\n",
    "                common['brelu']          = 'b1relu'\n",
    "                common['pool']           = 'mpool1'\n",
    "\n",
    "                common['regularization'] = 0\n",
    "                common['dropout']        = 1\n",
    "                common['learning_rate']  = 0.02\n",
    "                common['decay_rate']     = 0.95\n",
    "                common['momentum']       = 0.9\n",
    "                common['F']              = Kernel_Num[ker_num]\n",
    "                common['K']              = Kernel_Size[ker_size]\n",
    "                common['p']              = [2,2,2]\n",
    "                common['M']              = [1]\n",
    "                # concatnate fully connected NN\n",
    "                common['M1']             = [1]\n",
    "\n",
    "                train_dataset = []\n",
    "                val_dataset = []\n",
    "                test_dataset = []\n",
    "                for kegg_id in range(len(dataset['L'])):\n",
    "                    exp = dataset['exp'][kegg_id]\n",
    "                    perm = dataset['perm'][kegg_id]\n",
    "        #            print('kegg_id: {}, #nodes: {}'.format(kegg_id, len(perm)))\n",
    "        #            print(exp.shape)\n",
    "        #            print(train_idx.shape)\n",
    "                    train_data = exp[train_idx, :]\n",
    "                    val_data = exp[val_idx, :]\n",
    "                    test_data = exp[test_idx, :]\n",
    "                    train_data = coarsening.perm_data(train_data, perm)\n",
    "                    val_data = coarsening.perm_data(val_data, perm)\n",
    "                    test_data = coarsening.perm_data(test_data, perm)\n",
    "                    train_dataset.append(train_data)\n",
    "                    val_dataset.append(val_data)\n",
    "                    test_dataset.append(test_data)\n",
    "        #            print(train_data.shape, val_data.shape, test_data.shape)\n",
    "                if True:\n",
    "                    name = 'cgconv_softmax'\n",
    "                    params = common.copy()\n",
    "\n",
    "                model = models_update.cgcnn(dataset['L'], **params)\n",
    "                loss, t_step = model.fit(train_dataset, train_labels, val_dataset, val_labels)\n",
    "\n",
    "\n",
    "                Y_pred[:, j] = model.predict(test_dataset)\n",
    "                Y_test[:, j] = test_labels        \n",
    "                print(Y_pred[:, j])\n",
    "                print(Y_test[:, j])\n",
    "                j = j+1\n",
    "                np.savez(('Kernel_Test_{}_{}'.format(Kernel_Num[ker_num],Kernel_Size[ker_size])), Y_true=Y_test, Y_pred=Y_pred)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_[1]_[1]\n",
      "Test_[1]_[16, 1]\n",
      "Test_[1]_[128, 1]\n",
      "Test_[16, 1]_[1]\n",
      "Test_[16, 1]_[16, 1]\n",
      "Test_[16, 1]_[128, 1]\n",
      "Test_[128, 1]_[1]\n",
      "Test_[128, 1]_[16, 1]\n",
      "Test_[128, 1]_[128, 1]\n"
     ]
    }
   ],
   "source": [
    "FCa = [[1], [16,1], [128,1]]\n",
    "FCb = [[1], [16,1], [128,1]]\n",
    "for FC1 in range(len(FCa)):\n",
    "    for FC2 in range(len(FCb)):\n",
    "        print(\"Test_{}_{}\".format(FCa[FC1],FCb[FC2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 10, 10] [5, 5, 5]\n",
      "[10, 10, 10] [10, 10, 10]\n",
      "[10, 10, 10] [30, 30, 30]\n",
      "[20, 20, 20] [5, 5, 5]\n",
      "[20, 20, 20] [10, 10, 10]\n",
      "[20, 20, 20] [30, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "Kernel_Num =  [[10, 10, 10], [20, 20, 20]]\n",
    "Kernel_Size = [[5, 5, 5], [10, 10, 10], [30, 30, 30]]\n",
    "for ker_num in range(len(Kernel_Num)):\n",
    "    for ker_size in range(len(Kernel_Size)):\n",
    "        print(Kernel_Num[ker_num], Kernel_Size[ker_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484738\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
