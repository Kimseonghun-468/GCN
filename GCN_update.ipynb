{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(n_fold,X):\n",
    "    list_train_fold = []\n",
    "    list_val_fold = []\n",
    "    list_train = []\n",
    "    Number = X.shape[0]//n_fold\n",
    "    for i in range(X.shape[0]):\n",
    "        list_train.append(i)\n",
    "    \n",
    "    for i in range(n_fold)[::-1]:\n",
    "        list_val = []\n",
    "        if i==n_fold-1:\n",
    "            for j in range(Number*i,X.shape[0]):\n",
    "                list_val.append(j)\n",
    "                \n",
    "            list_train_fold.append(np.setdiff1d(list_train,list_val))\n",
    "            list_val_fold.append(list_val)\n",
    "            \n",
    "        \n",
    "        if i != n_fold-1: \n",
    "            for j in range(Number*i,Number*(i+1)):\n",
    "                list_val.append(j)\n",
    "            list_train_fold.append(np.setdiff1d(list_train,list_val))\n",
    "            list_val_fold.append(list_val)\n",
    "        \n",
    "    return list_train_fold, list_val_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "from lib import models_update, graph, coarsening, utils\n",
    "# modified\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import random\n",
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_simulation(exp, ppi,p):\n",
    "    dataset = {}\n",
    "    all_nodes = exp.shape[1]\n",
    "    dataset['exp'] = []\n",
    "    dataset['L'] = []\n",
    "    dataset['perm'] = []\n",
    "    Pathway_Name = list(p)\n",
    "    \n",
    "        \n",
    "    for i in range(len(Pathway_Name)):\n",
    "        \n",
    "    \n",
    "        Node_Number = []\n",
    "        Sub_Pathway = p[Pathway_Name[i]]['gene'].values() #i번째 Pathway의 GeneExpression 저장\n",
    "        Numbering = list(Sub_Pathway) # Gene 갯수 체크\n",
    "        for a in range(len(Numbering)): \n",
    "            for b in range(exp.shape[1]): \n",
    "                if Numbering[a] == exp.columns[b]: # 해당하는 Gene이름의 index 번호 체크\n",
    "                    Node_Number.append(b)\n",
    "\n",
    "        sample_ppi = ppi.iloc[Node_Number, Node_Number] # ppi를 해당하는 Gene과 맞춤\n",
    "        sample_ppi = np.array(sample_ppi) \n",
    "        print(\"PPI Shape :\",len(Node_Number), sample_ppi.shape[0])\n",
    "        A = coo_matrix(sample_ppi,dtype=np.float32) \n",
    "        print(\"Number of Edge :\",A.nnz//2, \"//\",i)\n",
    "        \n",
    "        graphs, perm = coarsening.coarsen(A, levels=4, self_connections=False)\n",
    "        print(type(A))\n",
    "        L = [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "        dataset['exp'].append(exp.iloc[:, Node_Number].values) \n",
    "        dataset['L'].append(L)\n",
    "        dataset['perm'].append(perm)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734, 7545)\n",
      "(7545, 7545)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "ppi = pd.read_csv(\"Table/PPI.csv\")\n",
    "ppi.drop(['string'], axis='columns', inplace=True)\n",
    "exp = pd.read_csv(\"Table/EXP.csv\")\n",
    "exp.drop(['Unnamed: 0'], axis='columns', inplace=True)\n",
    "print(exp.shape)\n",
    "print(ppi.shape)\n",
    "with open('Table/Pathway.pkl','rb')as f: #Pathway 받아오기\n",
    "    p = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_simulation(exp, ppi,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734, 201)\n"
     ]
    }
   ],
   "source": [
    "data_IC50 = pd.read_csv(\"Data/Table_S6_GDSC_Drug_response_IC50.csv\")\n",
    "data_IC50.drop(['Unnamed: 0'], axis='columns', inplace=True)\n",
    "print(data_IC50.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734\n",
      "(550,)\n",
      "(543,)\n",
      "(362,)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "n_fold = 3\n",
    "Y_train, Y_test = train_test_split(data_IC50,  test_size=0.25, shuffle=True, random_state=20)\n",
    "\n",
    "\n",
    "for cv in range(1):\n",
    "    Y_pred = np.zeros([Y_test.shape[0], Y_test.shape[1]])\n",
    "    Y_test = np.zeros([Y_test.shape[0], Y_test.shape[1]])\n",
    "#    print('n_fold:{}'.format(cv))\n",
    "    j = 0\n",
    "    for i in range(1):\n",
    "        label = data_IC50.iloc[:,i]\n",
    "        label = np.array(label)\n",
    "        data_minmax = label[~np.isnan(label)]\n",
    "        min = data_minmax.min()\n",
    "        max = data_minmax.max()\n",
    "        label = (label - min) / (max - min)\n",
    "#         label_indexes = np.argwhere([~np.isnan(label)])[:, 1]\n",
    "        \n",
    "        print(label.shape[0])\n",
    "        train_idx, test_idx = train_test_split(range(label.shape[0]), test_size=0.25, shuffle=True, random_state=20)\n",
    "        train_idx = np.array(train_idx)\n",
    "        print(train_idx.shape)\n",
    "        train_idx = train_idx[~np.isnan(label[train_idx])]        \n",
    "        list_train, list_val = Validation(n_fold,train_idx)\n",
    "\n",
    "        val_idx = train_idx[list_val[cv]]\n",
    "        print(train_idx.shape)\n",
    "        train_idx = train_idx[list_train[cv]]\n",
    "        print(train_idx.shape)\n",
    "        test_idx = np.array(test_idx)\n",
    "        \n",
    "        train_labels = label[train_idx]\n",
    "        val_labels = label[val_idx]\n",
    "        test_labels = label[test_idx]\n",
    "        \n",
    "        \n",
    "        common = {}\n",
    "        common['num_epochs']     = 40\n",
    "        common['batch_size']     = 4\n",
    "        common['decay_steps']    = train_idx.shape[0] / common['batch_size']\n",
    "        common['eval_frequency'] = 10 * common['num_epochs']\n",
    "        common['brelu']          = 'b1relu'\n",
    "        common['pool']           = 'mpool1'\n",
    "\n",
    "        common['regularization'] = 0\n",
    "        common['dropout']        = 1\n",
    "        common['learning_rate']  = 0.02\n",
    "        common['decay_rate']     = 0.95\n",
    "        common['momentum']       = 0.9\n",
    "        common['F']              = [1]\n",
    "        common['K']              = [1]\n",
    "        common['p']              = [2]\n",
    "        common['M']              = [1]\n",
    "        # concatnate fully connected NN\n",
    "        common['M1']             = [1]\n",
    "\n",
    "        train_dataset = []\n",
    "        val_dataset = []\n",
    "        test_dataset = []\n",
    "        # 3개의 dataset을 각 pathway에 맞게 연결\n",
    "        for kegg_id in range(len(dataset['L'])):\n",
    "            exp = dataset['exp'][kegg_id]\n",
    "            perm = dataset['perm'][kegg_id]\n",
    "            train_data = exp[train_idx, :]\n",
    "            val_data = exp[val_idx, :]\n",
    "            test_data = exp[test_idx, :]\n",
    "            train_data = coarsening.perm_data(train_data, perm)\n",
    "            val_data = coarsening.perm_data(val_data, perm)\n",
    "            test_data = coarsening.perm_data(test_data, perm)\n",
    "            train_dataset.append(train_data)\n",
    "            val_dataset.append(val_data)\n",
    "            test_dataset.append(test_data)\n",
    "        if True:\n",
    "            name = 'cgconv_softmax'\n",
    "            params = common.copy()\n",
    "        \n",
    "        \n",
    "        model = models_update.cgcnn(dataset['L'], **params)\n",
    "        loss, t_step = model.fit(train_dataset, train_labels, val_dataset, val_labels)\n",
    "\n",
    "\n",
    "        Y_pred[:, j] = model.predict(test_dataset)\n",
    "        Y_test[:, j] = test_labels        \n",
    "        print(Y_pred[:, j])\n",
    "        print(Y_test[:, j])\n",
    "        j = j+1\n",
    "#       np.savez(('parameter.cv_{}'.format(cv)), Y_true=Y_test, Y_pred=Y_pred)\n",
    "\n",
    "    np.savez(('parameter.cv_{}'.format(cv)), Y_true=Y_test, Y_pred=Y_pred)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : [0.03449682940019994, 0.016506825518722328, 0.03304470752500458, 0.0359634220979898]\n",
      "PCC : [0.3829299218333685, 0.41952147256776295, 0.23990148477324408, 0.27363753511941785]\n",
      "SCC : [0.3694496743726658, 0.4427821974239871, 0.23844460592236225, 0.2894369167772668]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "RMSE = []\n",
    "PCC = []\n",
    "SCC = []\n",
    "\n",
    "for i in range(4):\n",
    "    y_test = Y_test[:, i]\n",
    "    y_pred = Y_pred[:, i]\n",
    "    y_pred = y_pred[~np.isnan(y_test)]\n",
    "    y_test = y_test[~np.isnan(y_test)]\n",
    "\n",
    "    RMSE.append(mean_squared_error(y_test,y_pred))\n",
    "    PCC.append(stats.pearsonr(y_test,y_pred)[0])\n",
    "    SCC.append(stats.spearmanr(y_test,y_pred)[0])\n",
    "print(\"RMSE :\",RMSE)\n",
    "print(\"PCC :\",PCC)\n",
    "print(\"SCC :\",SCC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
